# 3x RTX 4090 è®­ç»ƒä¼˜åŒ–æŒ‡å—

## ğŸ“Š æ˜¾å­˜åˆ†æ

### å½“å‰é…ç½®
- **GPU**: 3x RTX 4090 (24GB x 3 = 72GB æ€»æ˜¾å­˜)
- **æ¨¡å‹**: Kolors (SDXL-based, ~2.6B UNet + 4B Text Encoder)
- **ç²¾åº¦**: BF16 æ··åˆç²¾åº¦

### æ˜¾å­˜å ç”¨ä¼°ç®—

| ç»„ä»¶ | å‚æ•°é‡ | BF16 æ˜¾å­˜ | å¤‡æ³¨ |
|------|--------|-----------|------|
| Kolors UNet | 2.6B | ~5.2GB | å†»ç»“ï¼Œä»…æ¨ç† |
| Text Encoder (ChatGLM) | 4B | ~8GB | å†»ç»“ |
| Spatial Adapter | ~50M | ~100MB | å¯è®­ç»ƒ |
| ä¼˜åŒ–å™¨çŠ¶æ€ (AdamW) | 50M Ã— 2 | ~200MB | 2 ä»½åŠ¨é‡ |
| æ¢¯åº¦ | 50M | ~100MB | |
| æ¿€æ´»å€¼ (batch=1) | - | ~8GB | æœ€å¤§å¤´ |
| **å•å¡æ€»è®¡** | - | **~21.6GB** | æ¥è¿‘æé™ |

---

## âš ï¸ å…³é”®é—®é¢˜ï¼šä½ çš„ä»£ç ä¼š OOM

### é—®é¢˜ä»£ç 
```python
# âŒ ä½ çš„ main.py (ä¼šçˆ†æ˜¾å­˜)
for mode in ['captioning', 'retrieval', 'generation']:
    loss = forward(mode)
    total_loss += loss

total_loss.backward()  # ä¿å­˜ 3 ä»½å®Œæ•´è®¡ç®—å›¾ï¼
```

**ä¸ºä»€ä¹ˆä¼š OOMï¼Ÿ**
1. PyTorch çš„ `backward()` éœ€è¦ä¿å­˜æ•´ä¸ªè®¡ç®—å›¾
2. ä½ ç´¯åŠ äº† 3 ä¸ª mode çš„ lossï¼Œè®¡ç®—å›¾åŒ…å« 3 æ¬¡ UNet forward
3. æ¿€æ´»å€¼æ˜¾å­˜ = 8GB Ã— 3 = **24GB**ï¼ˆè¶…å‡ºå•å¡å®¹é‡ï¼‰

---

## âœ… è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ Aï¼šåˆ†æ­¥ Backwardï¼ˆæ¨èï¼‰

```python
# âœ… ä¿®æ”¹ main.py çš„ train() å‡½æ•°
optimizer.zero_grad()

for mode in ['captioning', 'retrieval', 'generation']:
    # Forward
    loss = forward(mode)
    
    # ç«‹å³ Backwardï¼ˆé‡Šæ”¾è®¡ç®—å›¾ï¼‰
    scaled_loss = loss / 3.0  # å¹³å‡ 3 ä¸ª mode çš„æ¢¯åº¦
    scaler.scale(scaled_loss).backward()
    
    # æ¸…ç†ä¸­é—´å˜é‡
    del loss
    torch.cuda.empty_cache()

# æ¢¯åº¦è£å‰ª + ä¼˜åŒ–å™¨æ­¥è¿›
scaler.unscale_(optimizer)
torch.nn.utils.clip_grad_norm_(trainable_params, args.grad_clip)
scaler.step(optimizer)
scaler.update()
```

**ä¼˜ç‚¹**ï¼š
- æ¯æ¬¡åªä¿å­˜ 1 ä»½è®¡ç®—å›¾ï¼Œæ˜¾å­˜å ç”¨ ~8GB
- æ¢¯åº¦è‡ªåŠ¨ç´¯åŠ ï¼ˆPyTorch é»˜è®¤è¡Œä¸ºï¼‰
- æ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„

---

### æ–¹æ¡ˆ Bï¼šGradient Checkpointing

```python
# âœ… åœ¨åŠ è½½ UNet æ—¶å¯ç”¨
from diffusers import UNet2DConditionModel

unet = UNet2DConditionModel.from_pretrained(
    "Kwai-Kolors/Kolors",
    subfolder="unet",
    torch_dtype=torch.bfloat16
)

# å¯ç”¨ Gradient Checkpointing
unet.enable_gradient_checkpointing()
```

**åŸç†**ï¼š
- å‰å‘ä¼ æ’­æ—¶ä¸ä¿å­˜ä¸­é—´æ¿€æ´»å€¼
- åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—ï¼ˆç”¨æ—¶é—´æ¢ç©ºé—´ï¼‰
- æ˜¾å­˜å‡å°‘ ~40%ï¼Œè®­ç»ƒæ—¶é—´å¢åŠ  ~20%

**âš ï¸ æ³¨æ„**ï¼š
- ä½ çš„ Spatial Adapter ä¹Ÿéœ€è¦æ”¯æŒ checkpointing
- éœ€è¦åœ¨ `SpatialControlAdapter.forward()` ä¸­ä½¿ç”¨ `torch.utils.checkpoint.checkpoint()`

---

### æ–¹æ¡ˆ Cï¼šDeepSpeed ZeRO-2ï¼ˆå¤šå¡åœºæ™¯ï¼‰

```bash
# âœ… å®‰è£… DeepSpeed
pip install deepspeed

# âœ… åˆ›å»ºé…ç½®æ–‡ä»¶ ds_config.json
{
  "train_batch_size": 3,
  "gradient_accumulation_steps": 1,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 1e-4,
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "weight_decay": 0.01
    }
  },
  "fp16": {
    "enabled": false
  },
  "bf16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 2e8,
    "reduce_scatter": true,
    "reduce_bucket_size": 2e8,
    "overlap_comm": true,
    "contiguous_gradients": true
  }
}
```

```python
# âœ… ä¿®æ”¹ main.py
import deepspeed

# åˆå§‹åŒ– DeepSpeed
model_engine, optimizer, _, _ = deepspeed.initialize(
    model=model,
    model_parameters=trainable_params,
    config="ds_config.json"
)

# è®­ç»ƒå¾ªç¯
for batch in dataloader:
    loss = model_engine(batch)
    model_engine.backward(loss)
    model_engine.step()
```

**ä¼˜ç‚¹**ï¼š
- ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡åˆ° 3 å¼ å¡ï¼ˆæ¯å¡åªå­˜ 1/3ï¼‰
- å¯é€‰ CPU Offloadï¼ˆè¿›ä¸€æ­¥èŠ‚ï¿½ï¿½æ˜¾å­˜ï¼‰
- æ”¯æŒæ›´å¤§çš„ batch size

**ç¼ºç‚¹**ï¼š
- éœ€è¦é‡æ„è®­ç»ƒä»£ç 
- é€šä¿¡å¼€é”€ï¼ˆ3 å¡é—´éœ€è¦åŒæ­¥æ¢¯åº¦ï¼‰

---

## ğŸ¯ æ¨èé…ç½®ï¼ˆ3x 4090ï¼‰

### æ•°æ®æ¸…æ´—é˜¶æ®µ
```bash
# ä½¿ç”¨ä¿®å¤ç‰ˆè„šæœ¬
python scripts/prepare_layout_dataset_fixed.py \
    --input-tsv data/wukong_train.tsv \
    --image-dir /data/wukong/images \
    --output-jsonl data/layout_dataset.jsonl \
    --num-gpus 3 \
    --batch-size 8 \
    --model-path Qwen/Qwen2-VL-7B-Instruct
```

**é¢„æœŸé€Ÿåº¦**ï¼š
- å•å¡ååï¼š~5 images/s
- 3 å¡å¹¶è¡Œï¼š~15 images/s
- 10 ä¸‡å¼ å›¾ï¼š~2 å°æ—¶

---

### è®­ç»ƒé˜¶æ®µ

#### é…ç½® 1ï¼šå•å¡è®­ç»ƒï¼ˆæœ€ç®€å•ï¼‰
```bash
CUDA_VISIBLE_DEVICES=0 python main.py \
    --dataset layout \
    --batch-size 1 \
    --grad-accumulation-steps 8 \
    --precision bf16 \
    --lr 1e-4 \
    --epochs 10
```

**ç‰¹ç‚¹**ï¼š
- æ— éœ€ä¿®æ”¹ DDP ä»£ç 
- æœ‰æ•ˆ batch size = 1 Ã— 8 = 8
- è®­ç»ƒæ—¶é—´ï¼š~3 å¤©ï¼ˆ10 ä¸‡æ ·æœ¬ï¼‰

---

#### é…ç½® 2ï¼š3 å¡ DDPï¼ˆæ¨èï¼‰
```bash
# ä½¿ç”¨æ–¹æ¡ˆ Aï¼ˆåˆ†æ­¥ Backwardï¼‰
torchrun --nproc_per_node=3 main.py \
    --dataset layout \
    --batch-size 1 \
    --grad-accumulation-steps 3 \
    --precision bf16 \
    --lr 1e-4 \
    --epochs 10 \
    --multiprocessing-distributed
```

**ç‰¹ç‚¹**ï¼š
- æœ‰æ•ˆ batch size = 1 Ã— 3 Ã— 3 = 9
- è®­ç»ƒæ—¶é—´ï¼š~1 å¤©
- éœ€è¦ä¿®æ”¹ `train()` å‡½æ•°ï¼ˆè§æ–¹æ¡ˆ Aï¼‰

---

#### é…ç½® 3ï¼š3 å¡ DeepSpeedï¼ˆæœ€ä¼˜ï¼‰
```bash
deepspeed --num_gpus=3 main.py \
    --deepspeed \
    --deepspeed_config ds_config.json \
    --dataset layout \
    --batch-size 2 \
    --precision bf16 \
    --lr 1e-4 \
    --epochs 10
```

**ç‰¹ç‚¹**ï¼š
- æœ‰æ•ˆ batch size = 2 Ã— 3 = 6
- ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ï¼ˆèŠ‚çœ ~4GB æ˜¾å­˜ï¼‰
- å¯ä»¥è·‘ batch_size=2ï¼ˆæå‡è®­ç»ƒç¨³å®šæ€§ï¼‰

---

## ğŸ”§ ä»£ç ä¿®æ”¹æ¸…å•

### 1. ä¿®æ”¹ `main.py` çš„ `train()` å‡½æ•°

```python
# åœ¨ train() å‡½æ•°ä¸­æ‰¾åˆ°è¿™æ®µä»£ç ï¼š
for i, batch in enumerate(train_loader):
    # ... æ•°æ®é¢„å¤„ç† ...
    
    # âŒ åˆ é™¤è¿™æ®µï¼ˆåŸä»£ç ï¼‰
    # total_loss = 0
    # for mode in model_modes:
    #     loss = forward(mode)
    #     total_loss += loss
    # total_loss.backward()
    
    # âœ… æ›¿æ¢ä¸ºï¼ˆæ–¹æ¡ˆ Aï¼‰
    optimizer.zero_grad()
    for mode_idx, mode in enumerate(model_modes):
        # Forward
        with torch.cuda.amp.autocast(enabled=(args.precision == 'fp16')):
            result = model(images, tgt_tokens, token_len, mode=mode, ...)
            loss = compute_loss(result, mode)  # ä½ çš„ loss è®¡ç®—é€»è¾‘
        
        # Backwardï¼ˆç«‹å³é‡Šæ”¾è®¡ç®—å›¾ï¼‰
        scaled_loss = loss / len(model_modes)
        scaler.scale(scaled_loss).backward()
        
        # æ¸…ç†
        del loss, result
        if mode_idx < len(model_modes) - 1:
            torch.cuda.empty_cache()
    
    # æ¢¯åº¦è£å‰ª + ä¼˜åŒ–å™¨æ­¥è¿›
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(trainable_params, args.grad_clip)
    scaler.step(optimizer)
    scaler.update()
```

---

### 2. ä¿®æ”¹ `gill/spatial_adapter.py`

```bash
# ç›´æ¥æ›¿æ¢ä¸ºä¿®å¤ç‰ˆ
cp gill/spatial_adapter_fixed.py gill/spatial_adapter.py
```

**å…³é”®ä¿®æ”¹**ï¼š
1. âœ… æ³¨å…¥ä½ç½®ï¼š`attn2`ï¼ˆCross-Attentionï¼‰è€Œé `attn1`
2. âœ… åæ ‡éªŒè¯ï¼šè‡ªåŠ¨æ£€æŸ¥ BBox æ˜¯å¦å½’ä¸€åŒ–
3. âœ… ç»´åº¦é€‚é…ï¼šæ”¯æŒ SDXL çš„å¤šå±‚ç»´åº¦ï¼ˆ320/640/1280/2048ï¼‰

---

### 3. ä¿®æ”¹æ•°æ®æ¸…æ´—è„šæœ¬

```bash
# ä½¿ç”¨ä¿®å¤ç‰ˆï¼ˆRay å¹¶è¡Œï¼‰
python scripts/prepare_layout_dataset_fixed.py \
    --input-tsv data/wukong_train.tsv \
    --image-dir /data/wukong/images \
    --output-jsonl data/layout_dataset.jsonl \
    --num-gpus 3
```

**ä¾èµ–å®‰è£…**ï¼š
```bash
pip install ray qwen-vl-utils
```

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| æ–¹æ¡ˆ | æ˜¾å­˜/å¡ | è®­ç»ƒé€Ÿåº¦ | å®ç°éš¾åº¦ | æ¨èåº¦ |
|------|---------|----------|----------|--------|
| åŸä»£ç ï¼ˆç´¯åŠ  lossï¼‰ | **OOM** | - | - | âŒ |
| æ–¹æ¡ˆ Aï¼ˆåˆ†æ­¥ backwardï¼‰ | 21GB | 1.0x | â­ | â­â­â­â­â­ |
| æ–¹æ¡ˆ Bï¼ˆGradient Checkpointingï¼‰ | 15GB | 0.8x | â­â­ | â­â­â­ |
| æ–¹æ¡ˆ Cï¼ˆDeepSpeed ZeRO-2ï¼‰ | 18GB | 1.2x | â­â­â­ | â­â­â­â­ |

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶æ˜¾å­˜ä»ç„¶ä¸è¶³ï¼Ÿ
**A**: å°è¯•ä»¥ä¸‹ç»„åˆï¼š
```python
# 1. å¯ç”¨ Gradient Checkpointing
unet.enable_gradient_checkpointing()

# 2. å‡å°‘ batch size
--batch-size 1 --grad-accumulation-steps 16

# 3. å†»ç»“ Text Encoderï¼ˆå¦‚æœæœªå†»ç»“ï¼‰
for param in text_encoder.parameters():
    param.requires_grad = False
```

---

### Q2: DDP æŠ¥é”™ "RuntimeError: Expected to mark a variable ready only once"ï¼Ÿ
**A**: è¿™æ˜¯å› ä¸ºä¸åŒ mode ä½¿ç”¨äº†ä¸åŒçš„å‚æ•°å­é›†ã€‚è§£å†³æ–¹æ³•ï¼š
```python
# åœ¨ main.py çš„ DDP åˆå§‹åŒ–æ—¶
model = torch.nn.parallel.DistributedDataParallel(
    model,
    device_ids=[args.gpu],
    find_unused_parameters=True  # âœ… å…³é”®
)
```

---

### Q3: æ•°æ®æ¸…æ´—æ—¶ Qwen2-VL è¾“å‡ºæ ¼å¼ä¸ç¨³å®šï¼Ÿ
**A**: ä½¿ç”¨æ›´ä¸¥æ ¼çš„ promptï¼š
```python
prompt = """è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
[
  {"label": "ç‰©ä½“åç§°", "bbox": [x1, y1, x2, y2]}
]
åæ ‡èŒƒå›´ï¼š0-1000
ä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. **GLIGEN è®ºæ–‡**: https://arxiv.org/abs/2301.07093
2. **Diffusers Attention Processor**: https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview
3. **DeepSpeed ZeRO**: https://www.deepspeed.ai/tutorials/zero/
4. **Gradient Checkpointing**: https://pytorch.org/docs/stable/checkpoint.html

---

## âœ… æ€»ç»“

ä½ çš„åŸå§‹æ–¹æ¡ˆ**æ€è·¯æ­£ç¡®**ï¼Œä½†æœ‰ 4 ä¸ªå…³é”®é—®é¢˜ï¼š

1. âŒ **vLLM å¤šæ¨¡æ€æ”¯æŒä¸å®Œå–„** â†’ âœ… ä½¿ç”¨ Ray + Transformers
2. âŒ **Spatial Adapter æ³¨å…¥åˆ° Self-Attention** â†’ âœ… æ”¹ä¸º Cross-Attention
3. âŒ **ç´¯åŠ  loss å¯¼è‡´æ˜¾å­˜çˆ†ç‚¸** â†’ âœ… åˆ†æ­¥ backward
4. âŒ **ç¼ºå°‘åæ ‡éªŒè¯** â†’ âœ… è‡ªåŠ¨æ£€æŸ¥å½’ä¸€åŒ–

**ç«‹å³è¡ŒåŠ¨**ï¼š
1. æ›¿æ¢ `spatial_adapter.py` ä¸ºä¿®å¤ç‰ˆ
2. ä¿®æ”¹ `main.py` çš„ `train()` å‡½æ•°ï¼ˆæ–¹æ¡ˆ Aï¼‰
3. ä½¿ç”¨ `prepare_layout_dataset_fixed.py` æ¸…æ´—æ•°æ®
4. å¼€å§‹è®­ç»ƒï¼

ç¥ä½ çš„ **GILL-Next-CN** é¡¹ç›®é¡ºåˆ©ï¼ğŸš€

