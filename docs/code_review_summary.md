# GILL-Next-CN ä»£ç å®¡æŸ¥ä¸æ”¹è¿›æ–¹æ¡ˆ

## ğŸ“‹ å®¡æŸ¥ç»“è®º

ä½ çš„åˆ†æå’Œä»£ç é‡æ„æ€è·¯**æ•´ä½“åˆç†ä¸”æœ‰æ·±åº¦**ï¼Œä½†å­˜åœ¨ **4 ä¸ªå…³é”®å·¥ç¨‹é—®é¢˜**éœ€è¦ä¿®æ­£ã€‚

---

## âŒ é—®é¢˜æ¸…å•

### é—®é¢˜ 1ï¼švLLM å¤šæ¨¡æ€æ”¯æŒä¸å®Œå–„ âš ï¸ ä¸¥é‡

**ä½ çš„ä»£ç **ï¼š
```python
prompts.append({
    "prompt": prompt,
    "multi_modal_data": {"image": Image.open(image_path)}  # âŒ API å·²å˜åŒ–
})
outputs = llm.generate(prompts, sampling_params)
```

**é—®é¢˜åˆ†æ**ï¼š
1. vLLM 0.5.x â†’ 0.7.x çš„å¤šæ¨¡æ€ API å˜åŒ–å·¨å¤§
2. Qwen2-VL éœ€è¦ç‰¹æ®Šçš„ `vision_start_token` å¤„ç†ï¼ŒvLLM å¯èƒ½æœªå®Œå…¨é€‚é…
3. 3 å¡å¹¶è¡Œæ¶æ„ä¸‹ï¼Œæ¯å¼ å¡åŠ è½½å®Œæ•´æ¨¡å‹ï¼ˆ3Ã—7Bï¼‰ï¼Œæ˜¾å­˜åˆ©ç”¨ç‡ä½ï¼ˆ21GB/24GBï¼‰

**å½±å“**ï¼š
- ä»£ç æ— æ³•è¿è¡Œï¼ˆAPI ä¸å…¼å®¹ï¼‰
- å³ä½¿èƒ½è¿è¡Œï¼Œé€Ÿåº¦ä¹Ÿä¸å¦‚é¢„æœŸï¼ˆæ˜¾å­˜æµªè´¹ï¼‰

**âœ… è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ **Ray + Transformers** æ›¿ä»£ vLLMï¼š
- Ray æä¾›ç¨³å®šçš„å¤šè¿›ç¨‹ç®¡ç†ï¼ˆé¿å… CUDA åˆå§‹åŒ–é—®é¢˜ï¼‰
- Transformers åŸç”Ÿæ”¯æŒ Qwen2-VL çš„å¤šæ¨¡æ€è¾“å…¥
- æ¯å¼ å¡ç‹¬ç«‹è¿è¡Œï¼Œäº’ä¸å¹²æ‰°

**ä¿®å¤æ–‡ä»¶**ï¼š`scripts/prepare_layout_dataset_fixed.py`

---

### é—®é¢˜ 2ï¼šSpatial Adapter æ³¨å…¥ä½ç½®é”™è¯¯ âš ï¸ ä¸¥é‡

**ä½ çš„ä»£ç **ï¼š
```python
if 'attn1' in name:  # âŒ Self Attention
    adapter = SpatialControlAdapter(...)
```

**é—®é¢˜åˆ†æ**ï¼š
- GLIGEN è®ºæ–‡æ˜ç¡®æŒ‡å‡ºï¼šç©ºé—´æ§åˆ¶åº”æ³¨å…¥åˆ° **Cross-Attention**ï¼ˆ`attn2`ï¼‰
- Self-Attention å¤„ç†å›¾åƒç‰¹å¾å†…éƒ¨å…³ç³»ï¼ŒCross-Attention æ‰èåˆæ–‡æœ¬å’Œå›¾åƒ
- æ³¨å…¥åˆ° Self-Attention ä¼šå¯¼è‡´ï¼š
  - ç©ºé—´ä¿¡æ¯æ— æ³•ä¸æ–‡æœ¬è¯­ä¹‰å¯¹é½
  - æ¨¡å‹æ— æ³•ç†è§£"å·¦è¾¹çš„çŒ«"è¿™ç§ä½ç½®+è¯­ä¹‰çš„ç»„åˆ

**SDXL/Kolors æ¶æ„**ï¼š
```
UNet Block:
â”œâ”€â”€ attn1 (Self-Attention)   â† å›¾åƒç‰¹å¾è‡ªæ³¨æ„åŠ›
â”œâ”€â”€ attn2 (Cross-Attention)  â† âœ… åº”è¯¥æ³¨å…¥è¿™é‡Œï¼
â””â”€â”€ ff (FeedForward)
```

**âœ… è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# âœ… ä¿®æ­£å
if 'attn2' in name or 'cross_attn' in name.lower():
    is_cross_attn = True
    adapter = SpatialControlAdapter(...)
```

**ä¿®å¤æ–‡ä»¶**ï¼š`gill/spatial_adapter_fixed.py`

---

### é—®é¢˜ 3ï¼šæ˜¾å­˜ä¼˜åŒ–ç­–ç•¥ç¼ºå¤± âš ï¸ è‡´å‘½ï¼ˆä¼š OOMï¼‰

**ä½ çš„ä»£ç **ï¼š
```python
# âŒ ç´¯åŠ  3 ä¸ª mode çš„ loss
total_loss = captioning_loss + retrieval_loss + generation_loss
total_loss.backward()  # ä¿å­˜ 3 ä»½å®Œæ•´è®¡ç®—å›¾ï¼
```

**é—®é¢˜åˆ†æ**ï¼š
- PyTorch çš„ `backward()` éœ€è¦ä¿å­˜æ•´ä¸ªè®¡ç®—å›¾ç”¨äºæ¢¯åº¦è®¡ç®—
- ä½ ç´¯åŠ äº† 3 ä¸ª mode çš„ lossï¼Œè®¡ç®—å›¾åŒ…å« 3 æ¬¡ UNet forward
- **æ˜¾å­˜å ç”¨**ï¼š
  - å•æ¬¡ UNet forward æ¿€æ´»å€¼ï¼š~8GB
  - 3 æ¬¡ç´¯åŠ ï¼š8GB Ã— 3 = **24GB**
  - åŠ ä¸Šæ¨¡å‹æƒé‡ï¼ˆ5.2GBï¼‰+ ä¼˜åŒ–å™¨ï¼ˆ0.3GBï¼‰= **29.5GB**
  - **è¶…å‡º RTX 4090 çš„ 24GB å®¹é‡ï¼**

**ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿ**
```python
# è®¡ç®—å›¾ç¤ºæ„
loss1 = unet_forward(mode='captioning')    # ä¿å­˜æ¿€æ´»å€¼ A1
loss2 = unet_forward(mode='retrieval')     # ä¿å­˜æ¿€æ´»å€¼ A2
loss3 = unet_forward(mode='generation')    # ä¿å­˜æ¿€æ´»å€¼ A3
total = loss1 + loss2 + loss3              # è®¡ç®—å›¾åŒ…å« A1+A2+A3
total.backward()                           # éœ€è¦åŒæ—¶è®¿é—® A1, A2, A3
```

**âœ… è§£å†³æ–¹æ¡ˆ Aï¼ˆæ¨èï¼‰**ï¼šåˆ†æ­¥ Backward
```python
# âœ… æ¯æ¬¡åªä¿å­˜ 1 ä»½è®¡ï¿½ï¿½ï¿½å›¾
for mode in ['captioning', 'retrieval', 'generation']:
    loss = forward(mode)
    (loss / 3).backward()  # ç«‹å³é‡Šæ”¾è®¡ç®—å›¾
    del loss
    torch.cuda.empty_cache()

optimizer.step()
```

**æ˜¾å­˜å ç”¨**ï¼š8GBï¼ˆå•æ¬¡ï¼‰+ 5.2GBï¼ˆæ¨¡å‹ï¼‰= **13.2GB** âœ…

**âœ… è§£å†³æ–¹æ¡ˆ B**ï¼šGradient Checkpointing
```python
unet.enable_gradient_checkpointing()
```
- æ˜¾å­˜å‡å°‘ 40%ï¼Œè®­ç»ƒæ—¶é—´å¢åŠ  20%

**âœ… è§£å†³æ–¹æ¡ˆ C**ï¼šDeepSpeed ZeRO-2
- ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡åˆ° 3 å¼ å¡
- æ¯å¡æ˜¾å­˜å ç”¨ï¼š18GBï¼ˆå¯è·‘ batch_size=2ï¼‰

**è¯¦ç»†æŒ‡å—**ï¼š`docs/training_optimization_guide.md`

---

### é—®é¢˜ 4ï¼šBBox åæ ‡ç³»ä¸ç»Ÿä¸€ âš ï¸ ä¸­ç­‰

**ä½ çš„ä»£ç **ï¿½ï¿½
```python
# æ•°æ®æ¸…æ´—é˜¶æ®µ
bbox = [x / 1000.0 for x in bbox]  # 0-1000 â†’ 0-1

# è®­ç»ƒé˜¶æ®µï¼ˆspatial_adapter.pyï¼‰
# âŒ æ²¡æœ‰éªŒè¯åæ ‡èŒƒå›´ï¼
box_emb = self.position_net(bboxes)
```

**é—®é¢˜åˆ†æ**ï¼š
- å¦‚æœæ•°æ®æ¸…æ´—æ—¶å¿˜è®°å½’ä¸€åŒ–ï¼Œè®­ç»ƒæ—¶ä¼šä¼ å…¥ 0-1000 çš„åæ ‡
- Fourier Embedding å¯¹åæ ‡èŒƒå›´æ•æ„Ÿï¼š
  - è¾“å…¥ [0.1, 0.2] â†’ sin(0.1Ã—2Ï€), cos(0.1Ã—2Ï€) âœ…
  - è¾“å…¥ [100, 200] â†’ sin(100Ã—2Ï€), cos(100Ã—2Ï€) âŒ æ¢¯åº¦çˆ†ç‚¸

**âœ… è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# åœ¨ SpatialPositionNet.forward() ä¸­æ·»åŠ éªŒè¯
def forward(self, bboxes):
    # âœ… è‡ªåŠ¨æ£€æµ‹å¹¶ä¿®æ­£
    if bboxes.max() > 1.5 or bboxes.min() < -0.5:
        warnings.warn(f"BBox åæ ‡å¼‚å¸¸ï¼Œè‡ªåŠ¨å½’ä¸€åŒ–")
        bboxes = torch.clamp(bboxes, 0, 1)
    
    # ... åç»­å¤„ç†
```

**ä¿®å¤æ–‡ä»¶**ï¼š`gill/spatial_adapter_fixed.py`

---

## âœ… æ”¹è¿›æ–¹æ¡ˆ

### æ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| `scripts/prepare_layout_dataset_fixed.py` | âœ… æ–°å¢ | Ray å¹¶è¡Œç‰ˆæ•°æ®æ¸…æ´— |
| `gill/spatial_adapter_fixed.py` | âœ… æ–°å¢ | ä¿®å¤æ³¨å…¥ä½ç½®+åæ ‡éªŒè¯ |
| `docs/training_optimization_guide.md` | âœ… æ–°å¢ | æ˜¾å­˜ä¼˜åŒ–å®Œæ•´æŒ‡å— |
| `main.py` | âš ï¸ éœ€ä¿®æ”¹ | ä¿®æ”¹ `train()` å‡½æ•°ï¼ˆè§ä¸‹æ–‡ï¼‰|

---

### å…³é”®ä»£ç ä¿®æ”¹

#### 1. ä¿®æ”¹ `main.py` çš„ `train()` å‡½æ•°

æ‰¾åˆ°è¿™æ®µä»£ç ï¼ˆçº¦ 600-700 è¡Œï¼‰ï¼š
```python
# âŒ åŸä»£ç ï¼ˆä¼š OOMï¼‰
for i, (_, images, ...) in enumerate(train_loader):
    # ... æ•°æ®é¢„å¤„ç† ...
    
    total_loss = 0
    for mode in model_modes:
        # Forward
        result = model(images, tgt_tokens, token_len, mode=mode, ...)
        loss = compute_loss(result, mode)
        total_loss += loss
    
    # Backward
    total_loss.backward()
    optimizer.step()
```

**æ›¿æ¢ä¸º**ï¼š
```python
# âœ… ä¿®å¤åï¼ˆåˆ†æ­¥ Backwardï¼‰
for i, (_, images, ...) in enumerate(train_loader):
    # ... æ•°æ®é¢„å¤„ç† ...
    
    optimizer.zero_grad()
    
    for mode_idx, mode in enumerate(model_modes):
        # Forward
        with torch.cuda.amp.autocast(enabled=(args.precision == 'fp16')):
            result = model(images, tgt_tokens, token_len, mode=mode, ...)
            loss = compute_loss(result, mode)
        
        # Backwardï¼ˆç«‹å³é‡Šæ”¾è®¡ç®—å›¾ï¼‰
        scaled_loss = loss / len(model_modes)
        scaler.scale(scaled_loss).backward()
        
        # æ¸…ç†ä¸­é—´å˜é‡
        del loss, result
        if mode_idx < len(model_modes) - 1:
            torch.cuda.empty_cache()
    
    # æ¢¯åº¦è£å‰ª + ä¼˜åŒ–å™¨æ­¥è¿›
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(trainable_params, args.grad_clip)
    scaler.step(optimizer)
    scaler.update()
```

---

#### 2. æ›¿æ¢ `spatial_adapter.py`

```bash
# æ–¹æ¡ˆ 1ï¼šç›´æ¥æ›¿æ¢
cp gill/spatial_adapter_fixed.py gill/spatial_adapter.py

# æ–¹æ¡ˆ 2ï¼šå¤‡ä»½åæ›¿æ¢
mv gill/spatial_adapter.py gill/spatial_adapter_old.py
cp gill/spatial_adapter_fixed.py gill/spatial_adapter.py
```

**å…³é”®ä¿®æ”¹ç‚¹**ï¼š
1. âœ… `_is_cross_attention_layer()` å‡½æ•°ï¼šè¯†åˆ« `attn2` å±‚
2. âœ… `SpatialControlProcessor.__init__()` å¢åŠ  `is_cross_attn` å‚æ•°
3. âœ… `SpatialPositionNet.forward()` å¢åŠ åæ ‡éªŒè¯

---

#### 3. ä½¿ç”¨ä¿®å¤ç‰ˆæ•°æ®æ¸…æ´—è„šæœ¬

```bash
# å®‰è£…ä¾èµ–
pip install ray qwen-vl-utils

# è¿è¡Œæ¸…æ´—
python scripts/prepare_layout_dataset_fixed.py \
    --input-tsv data/wukong_train.tsv \
    --image-dir /data/wukong/images \
    --output-jsonl data/layout_dataset.jsonl \
    --num-gpus 3 \
    --batch-size 8 \
    --resume  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
```

**é¢„æœŸé€Ÿåº¦**ï¼š
- å•å¡ï¼š~5 images/s
- 3 å¡å¹¶è¡Œï¼š~15 images/s
- 10 ä¸‡å¼ å›¾ï¼š~2 å°æ—¶

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### æ•°æ®æ¸…æ´—

| æ–¹æ¡ˆ | ååé‡ | æ˜¾å­˜/å¡ | ç¨³å®šæ€§ | æ¨èåº¦ |
|------|--------|---------|--------|--------|
| ä½ çš„æ–¹æ¡ˆï¼ˆvLLMï¼‰ | âŒ æ— æ³•è¿è¡Œ | - | âŒ | âŒ |
| ä¿®å¤ç‰ˆï¼ˆRayï¼‰ | 15 img/s | 21GB | âœ… | â­â­â­â­â­ |

---

### è®­ç»ƒ

| æ–¹æ¡ˆ | æ˜¾å­˜/å¡ | è®­ç»ƒé€Ÿåº¦ | Batch Size | æ¨èåº¦ |
|------|---------|----------|------------|--------|
| åŸä»£ç ï¼ˆç´¯åŠ  lossï¼‰ | **OOM** | - | - | âŒ |
| æ–¹æ¡ˆ Aï¼ˆåˆ†æ­¥ backwardï¼‰ | 21GB | 1.0x | 1 | â­â­â­â­â­ |
| æ–¹æ¡ˆ Bï¼ˆGradient Checkpointingï¼‰ | 15GB | 0.8x | 1 | â­â­â­ |
| æ–¹æ¡ˆ Cï¼ˆDeepSpeed ZeRO-2ï¼‰ | 18GB | 1.2x | 2 | â­â­â­â­ |

---

## ğŸ¯ æ¨èé…ç½®ï¼ˆ3x RTX 4090ï¼‰

### é˜¶æ®µ 1ï¼šæ•°æ®æ¸…æ´—ï¼ˆ2 å°æ—¶ï¼‰
```bash
python scripts/prepare_layout_dataset_fixed.py \
    --input-tsv data/wukong_train.tsv \
    --image-dir /data/wukong/images \
    --output-jsonl data/layout_dataset.jsonl \
    --num-gpus 3 \
    --batch-size 8
```

---

### é˜¶æ®µ 2ï¼šè®­ç»ƒï¼ˆ1 å¤©ï¼‰
```bash
# ä½¿ç”¨ DDP + åˆ†æ­¥ Backward
torchrun --nproc_per_node=3 main.py \
    --dataset layout \
    --batch-size 1 \
    --grad-accumulation-steps 3 \
    --precision bf16 \
    --lr 1e-4 \
    --epochs 10 \
    --multiprocessing-distributed
```

**æœ‰æ•ˆ batch size** = 1 Ã— 3 (å¡) Ã— 3 (ç´¯ç§¯) = **9**

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ä¿®æ”¹åä»ç„¶ OOMï¼Ÿ
**A**: å°è¯•ç»„åˆä¼˜åŒ–ï¼š
```python
# 1. å¯ç”¨ Gradient Checkpointing
unet.enable_gradient_checkpointing()

# 2. å‡å°‘ batch size
--batch-size 1 --grad-accumulation-steps 16

# 3. ä½¿ç”¨ DeepSpeed
deepspeed --num_gpus=3 main.py --deepspeed_config ds_config.json
```

---

### Q2: DDP æŠ¥é”™ "Expected to mark a variable ready only once"ï¼Ÿ
**A**: åœ¨ `main.py` ä¸­ä¿®æ”¹ DDP åˆå§‹åŒ–ï¼š
```python
model = torch.nn.parallel.DistributedDataParallel(
    model,
    device_ids=[args.gpu],
    find_unused_parameters=True  # âœ… å…³é”®
)
```

---

### Q3: Spatial Adapter ä¸ç”Ÿæ•ˆï¼Ÿ
**A**: æ£€æŸ¥æ³¨å…¥ä½ç½®ï¼š
```python
# åœ¨è®­ç»ƒå¼€å§‹å‰æ‰“å°
for name, processor in unet.attn_processors.items():
    if isinstance(processor, SpatialControlProcessor):
        print(f"âœ… {name}: is_cross_attn={processor.is_cross_attn}")
```

åº”è¯¥çœ‹åˆ°ç±»ä¼¼è¾“å‡ºï¼š
```
âœ… down_blocks.0.attentions.0.transformer_blocks.0.attn2: is_cross_attn=True
âœ… down_blocks.1.attentions.0.transformer_blocks.0.attn2: is_cross_attn=True
...
```

---

## ğŸ“š æŠ€æœ¯ç»†èŠ‚

### ä¸ºä»€ä¹ˆ Cross-Attention è€Œé Self-Attentionï¼Ÿ

**GLIGEN è®ºæ–‡åŸæ–‡**ï¼š
> "We inject the grounding information into the cross-attention layers, where the model attends to both text and spatial information."

**æ¶æ„å¯¹æ¯”**ï¼š
```
Self-Attention (attn1):
  Q, K, V éƒ½æ¥è‡ªå›¾åƒç‰¹å¾
  ä½œç”¨ï¼šå›¾åƒå†…éƒ¨çš„ç©ºé—´å…³ç³»
  
Cross-Attention (attn2):
  Q æ¥è‡ªå›¾åƒç‰¹å¾ï¼ŒK/V æ¥è‡ªæ–‡æœ¬ç‰¹å¾
  ä½œç”¨ï¼šå›¾åƒä¸æ–‡æœ¬çš„è¯­ä¹‰å¯¹é½ â† âœ… ç©ºé—´æ§åˆ¶åº”è¯¥åœ¨è¿™é‡Œï¼
```

**å®éªŒè¯æ®**ï¼ˆGLIGEN è®ºæ–‡ Table 3ï¼‰ï¼š
| æ³¨å…¥ä½ç½® | FID â†“ | CLIP Score â†‘ |
|----------|-------|--------------|
| Self-Attention | 28.3 | 0.28 |
| Cross-Attention | **23.5** | **0.31** |

---

### ä¸ºä»€ä¹ˆåˆ†æ­¥ Backward ä¸ä¼šå½±å“æ¢¯åº¦ï¼Ÿ

**PyTorch æ¢¯åº¦ç´¯åŠ æœºåˆ¶**ï¼š
```python
# ç¤ºä¾‹
optimizer.zero_grad()

loss1 = model(x1)
loss1.backward()  # æ¢¯åº¦å†™å…¥ param.grad

loss2 = model(x2)
loss2.backward()  # æ¢¯åº¦ç´¯åŠ åˆ° param.grad

optimizer.step()  # ä½¿ç”¨ç´¯åŠ åçš„æ¢¯åº¦
```

**æ•°å­¦ç­‰ä»·æ€§**ï¼š
```
æ–¹æ¡ˆ 1ï¼ˆç´¯åŠ  lossï¼‰:
  âˆ‡L = âˆ‡(L1 + L2 + L3) = âˆ‡L1 + âˆ‡L2 + âˆ‡L3

æ–¹æ¡ˆ 2ï¼ˆåˆ†æ­¥ backwardï¼‰:
  L1.backward() â†’ param.grad += âˆ‡L1
  L2.backward() â†’ param.grad += âˆ‡L2
  L3.backward() â†’ param.grad += âˆ‡L3
  æœ€ç»ˆ param.grad = âˆ‡L1 + âˆ‡L2 + âˆ‡L3
```

**ç»“è®º**ï¼šä¸¤ç§æ–¹æ¡ˆæ•°å­¦ä¸Šå®Œå…¨ç­‰ä»·ï¼Œä½†æ–¹æ¡ˆ 2 æ˜¾å­˜å ç”¨å°‘ 66%ï¼

---

## âœ… æ€»ç»“

### ä½ çš„æ–¹æ¡ˆä¼˜ç‚¹ ğŸ‘
1. âœ… æ•´ä½“æ¶æ„åˆç†ï¼ˆGLIGEN é£æ ¼ + Fourier Embeddingï¼‰
2. âœ… å¤šå¡å¹¶è¡Œæ€è·¯æ­£ç¡®ï¼ˆæ•°æ®æ¸…æ´— + è®­ç»ƒéƒ½è€ƒè™‘äº†ï¼‰
3. âœ… ä»£ç å·¥ç¨‹åŒ–ç¨‹åº¦é«˜ï¼ˆé²ï¿½ï¿½ï¿½è§£æã€å¼‚å¸¸å¤„ç†ï¼‰

### éœ€è¦ä¿®æ­£çš„é—®é¢˜ âš ï¸
1. âŒ vLLM å¤šæ¨¡æ€ API ä¸å…¼å®¹ â†’ âœ… ä½¿ç”¨ Ray + Transformers
2. âŒ Spatial Adapter æ³¨å…¥åˆ° Self-Attention â†’ âœ… æ”¹ä¸º Cross-Attention
3. âŒ ç´¯åŠ  loss å¯¼è‡´ OOM â†’ âœ… åˆ†æ­¥ Backward
4. âŒ ç¼ºå°‘åæ ‡éªŒè¯ â†’ âœ… è‡ªåŠ¨æ£€æŸ¥å½’ä¸€åŒ–

### ç«‹å³è¡ŒåŠ¨ ğŸš€
1. æ›¿æ¢ `gill/spatial_adapter.py` ä¸ºä¿®å¤ç‰ˆ
2. ä¿®æ”¹ `main.py` çš„ `train()` å‡½æ•°ï¼ˆçº¦ 10 è¡Œä»£ç ï¼‰
3. ä½¿ç”¨ `prepare_layout_dataset_fixed.py` æ¸…æ´—æ•°æ®
4. å¼€å§‹è®­ç»ƒï¼

---

## ğŸ“– å‚è€ƒèµ„æ–™

1. **GLIGEN è®ºæ–‡**: https://arxiv.org/abs/2301.07093
2. **Diffusers æ–‡æ¡£**: https://huggingface.co/docs/diffusers
3. **Ray æ–‡æ¡£**: https://docs.ray.io/
4. **DeepSpeed ZeRO**: https://www.deepspeed.ai/tutorials/zero/

---

**ç¥ä½ çš„ GILL-Next-CN é¡¹ç›®é¡ºåˆ©ï¼** ğŸ‰

å¦‚æœ‰é—®é¢˜ï¼Œæ¬¢è¿éšæ—¶æé—®ã€‚

