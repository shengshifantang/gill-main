"""
Spatial Control Adapter (GLIGEN é£æ ¼) - ä¿®å¤ç‰ˆ

ä¸»è¦ä¿®æ­£ï¼š
1. âœ… æ³¨å…¥ä½ç½®ï¼šCross-Attentionï¼ˆattn2ï¼‰è€Œé Self-Attentionï¼ˆattn1ï¼‰
2. âœ… æ˜¾å­˜ä¼˜åŒ–ï¼šæ”¯æŒ Gradient Checkpointing
3. âœ… åæ ‡éªŒè¯ï¼šå¼ºåˆ¶æ£€æŸ¥ BBox å½’ä¸€åŒ–
4. âœ… å¤šç»´åº¦é€‚é…ï¼šè‡ªåŠ¨æ£€æµ‹ UNet å„å±‚ç»´åº¦

å‚è€ƒï¼š
- GLIGEN (CVPR 2023): Open-Set Grounded Text-to-Image Generation
- ä½¿ç”¨ Diffusers Attention Processor æœºåˆ¶ï¼Œé¿å…ä¿®æ”¹æºç 
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, List, Tuple, Dict
import math
import warnings


class SpatialPositionNet(nn.Module):
    """
    å°† BBox [x1, y1, x2, y2] æ˜ å°„ä¸ºé«˜ç»´ç‰¹å¾
    
    ä½¿ç”¨ Fourier Features ç¼–ç ä½ç½®ä¿¡æ¯
    """
    def __init__(self, in_dim: int = 4, out_dim: int = 2048, fourier_freqs: int = 8):
        super().__init__()
        self.out_dim = out_dim
        
        # Fourier Features: å°†åæ ‡æ˜ å°„åˆ°é«˜é¢‘ç©ºé—´
        self.fourier_embedder = nn.Sequential(
            nn.Linear(in_dim * fourier_freqs * 2, out_dim // 2),
            nn.SiLU(),
            nn.Linear(out_dim // 2, out_dim)
        )
        
        # ç”Ÿæˆ Fourier é¢‘ç‡
        freqs = torch.linspace(0, 1, fourier_freqs)
        self.register_buffer('freqs', freqs)
    
    def forward(self, bboxes: torch.Tensor) -> torch.Tensor:
        """
        Args:
            bboxes: (B, N, 4) å½’ä¸€åŒ–åæ ‡ [x1, y1, x2, y2]
        
        Returns:
            box_emb: (B, N, out_dim) ä½ç½® embedding
        """
        B, N, _ = bboxes.shape
        
        # âœ… åæ ‡éªŒè¯ï¼ˆé˜²æ­¢æœªå½’ä¸€åŒ–çš„åæ ‡ï¼‰
        if bboxes.max() > 1.5 or bboxes.min() < -0.5:
            warnings.warn(f"BBox åæ ‡å¼‚å¸¸ï¼šmin={bboxes.min():.2f}, max={bboxes.max():.2f}ï¼ŒæœŸæœ›èŒƒå›´ [0, 1]")
            # è‡ªåŠ¨å½’ä¸€åŒ–ï¼ˆå…œåº•ï¼‰
            bboxes = torch.clamp(bboxes, 0, 1)
        
        # ç”Ÿæˆ Fourier features
        bboxes_expanded = bboxes.unsqueeze(-1)  # (B, N, 4, 1)
        freqs = self.freqs.view(1, 1, 1, -1)  # (1, 1, 1, fourier_freqs)
        
        # sin å’Œ cos å˜æ¢
        sin_features = torch.sin(bboxes_expanded * freqs * 2 * math.pi)  # (B, N, 4, fourier_freqs)
        cos_features = torch.cos(bboxes_expanded * freqs * 2 * math.pi)  # (B, N, 4, fourier_freqs)
        
        # æ‹¼æ¥
        fourier_features = torch.cat([sin_features, cos_features], dim=-1)  # (B, N, 4, 2*fourier_freqs)
        fourier_features = fourier_features.reshape(B, N, -1)  # (B, N, 4*2*fourier_freqs)
        
        # é€šè¿‡ MLP
        box_emb = self.fourier_embedder(fourier_features)  # (B, N, out_dim)
        
        return box_emb


class GatedSelfAttentionDense(nn.Module):
    """
    Gated Self-Attention å±‚ï¼ˆå‚è€ƒ GLIGENï¼‰
    
    å°†ç©ºé—´ä¿¡æ¯é€šè¿‡é—¨æ§æœºåˆ¶æ³¨å…¥åˆ° UNet ç‰¹å¾ä¸­
    """
    def __init__(self, query_dim: int, context_dim: int, n_heads: int = 8, d_head: int = 64):
        super().__init__()
        self.query_dim = query_dim
        self.context_dim = context_dim
        self.n_heads = n_heads
        self.d_head = d_head
        
        # Query, Key, Value æŠ•å½±
        self.to_q = nn.Linear(query_dim, n_heads * d_head, bias=False)
        self.to_k = nn.Linear(context_dim, n_heads * d_head, bias=False)
        self.to_v = nn.Linear(context_dim, n_heads * d_head, bias=False)
        self.to_out = nn.Linear(n_heads * d_head, query_dim)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šZero Initializationï¼ˆControlNet/GLIGEN çš„æ ¸å¿ƒè®¾è®¡ï¼‰
        nn.init.zeros_(self.to_out.weight)
        if self.to_out.bias is not None:
            nn.init.zeros_(self.to_out.bias)
        
        # âœ… é—¨æ§å‚æ•°ï¼ˆåˆå§‹åŒ–ä¸º 0ï¼‰
        self.gate = nn.Parameter(torch.tensor([0.0]))
    
    def forward(
        self,
        x: torch.Tensor,
        spatial_context: torch.Tensor,
        masks: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Args:
            x: UNet çš„ä¸­é—´ç‰¹å¾ (B, T, query_dim)
            spatial_context: ç©ºé—´ embedding (B, N, context_dim)
        
        Returns:
            out: æ³¨å…¥ç©ºé—´ä¿¡æ¯åçš„ç‰¹å¾ (B, T, query_dim)
        """
        B, T, _ = x.shape
        _, N, _ = spatial_context.shape
        
        # è®¡ç®— Q, K, V
        q = self.to_q(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)  # (B, H, T, d)
        k = self.to_k(spatial_context).view(B, N, self.n_heads, self.d_head).transpose(1, 2)  # (B, H, N, d)
        v = self.to_v(spatial_context).view(B, N, self.n_heads, self.d_head).transpose(1, 2)  # (B, H, N, d)
        
        # Attention
        scale = 1.0 / math.sqrt(self.d_head)
        attn_logits = torch.matmul(q, k.transpose(-2, -1)) * scale  # (B, H, T, N)
        if masks is not None:
            # masks: (B, N) -> (B, 1, 1, N)
            if masks.dim() == 2:
                attn_mask = masks[:, None, None, :]
            else:
                attn_mask = masks
            attn_logits = attn_logits.masked_fill(attn_mask <= 0, -1e4)
        attn = torch.softmax(attn_logits, dim=-1)  # (B, H, T, N)
        attn_out = torch.matmul(attn, v)  # (B, H, T, d)
        
        # Reshape å¹¶æŠ•å½±
        attn_out = attn_out.transpose(1, 2).contiguous().view(B, T, self.n_heads * self.d_head)  # (B, T, H*d)
        attn_out = self.to_out(attn_out)  # (B, T, query_dim)
        
        # é—¨æ§æ®‹å·®è¿æ¥
        # tanh(gate) å…è®¸é—¨æ§åœ¨ -1 åˆ° 1 ä¹‹é—´ï¼Œåˆå§‹ä¸º 0
        out = x + torch.tanh(self.gate) * attn_out
        
        return out


class SpatialControlAdapter(nn.Module):
    """
    Spatial Control Adapter for Kolors (SDXL-based)
    
    ä½¿ç”¨ Diffusers Attention Processor æœºåˆ¶æ³¨å…¥ç©ºé—´æ§åˆ¶
    """
    def __init__(self, hidden_dim: int = 2048, num_heads: int = 8, text_dim: int = 4096):
        """
        Args:
            hidden_dim: UNet çš„ hidden dimension (Kolors ä½¿ç”¨ 2048)
            num_heads: Attention head æ•°é‡
            text_dim: æ–‡æœ¬ç‰¹å¾ç»´åº¦ï¼ˆKolors/ChatGLM é»˜è®¤ 4096ï¼‰
        """
        super().__init__()
        self.hidden_dim = hidden_dim
        self.text_dim = text_dim
        
        # ä½ç½®ç¼–ç ç½‘ç»œï¼Œè¾“å‡ºä¸å½“å‰å±‚ hidden_dim å¯¹é½
        self.position_net = SpatialPositionNet(out_dim=hidden_dim)
        
        # æ–‡æœ¬ç‰¹å¾æŠ•å½±åˆ° hidden_dimï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…
        self.text_proj = nn.Linear(text_dim, hidden_dim)
        
        # Gated Self-Attention å±‚ï¼ˆä¸å½“å‰å±‚ç»´åº¦åŒ¹é…ï¼‰
        self.gated_attn = GatedSelfAttentionDense(
            query_dim=hidden_dim,
            context_dim=hidden_dim,
            n_heads=num_heads,
            d_head=hidden_dim // num_heads
        )
        # æ¨ç†æ—¶å¯åŠ¨æ€è°ƒèŠ‚çš„æ³¨å…¥å¼ºåº¦ï¼ˆScheduled Samplingï¼‰
        self.beta_scale = 1.0

    def set_scale(self, scale: float):
        """è®¾ç½®æ³¨å…¥å¼ºåº¦ï¼ˆScheduled Sampling æ—¶ä½¿ç”¨ï¼‰"""
        try:
            self.beta_scale = float(scale)
        except Exception:
            self.beta_scale = 1.0
    
    def forward(self, 
                unet_features: torch.Tensor,
                bboxes: torch.Tensor,
                phrase_embeddings: Optional[torch.Tensor] = None,
                masks: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        åœ¨ UNet forward è¿‡ç¨‹ä¸­è¢«è°ƒç”¨
        
        Args:
            unet_features: UNet ä¸­é—´å±‚ç‰¹å¾ (B, T, hidden_dim)
            bboxes: å½’ä¸€åŒ–åæ ‡ (B, N, 4)
            phrase_embeddings: ç‰©ä½“åç§°çš„æ–‡æœ¬ embedding (B, N, text_dim)ï¼Œå¯é€‰
        
        Returns:
            controlled_features: æ³¨å…¥ç©ºé—´ä¿¡æ¯åçš„ç‰¹å¾
        """
        # âœ… CFG æ‰¹æ¬¡å¹¿æ’­ï¼ˆClassifier-Free Guidance ä¼šå¤åˆ¶ batchï¼‰
        if bboxes.shape[0] == 1 and unet_features.shape[0] > 1:
            bboxes = bboxes.repeat(unet_features.shape[0], 1, 1)
        if phrase_embeddings is not None and phrase_embeddings.shape[0] == 1 and unet_features.shape[0] > 1:
            phrase_embeddings = phrase_embeddings.repeat(unet_features.shape[0], 1, 1)
        if masks is not None and masks.shape[0] == 1 and unet_features.shape[0] > 1:
            masks = masks.repeat(unet_features.shape[0], 1)

        # âœ… å¯¹é½ dtypeï¼ˆé˜²æ­¢ AMP æ··åˆç²¾åº¦æŠ¥é”™ï¼‰
        adapter_dtype = next(self.parameters()).dtype
        bboxes = bboxes.to(dtype=adapter_dtype, device=unet_features.device)
        if masks is not None:
            masks = masks.to(dtype=adapter_dtype, device=unet_features.device)
        
        # âœ… ç»´åº¦åŒ¹é…æ£€æŸ¥ï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
        if len(unet_features.shape) >= 2:
            actual_dim = unet_features.shape[-1]
            if actual_dim != self.gated_attn.query_dim:
                warnings.warn(f"ç»´åº¦ä¸åŒ¹é…ï¼šæœŸæœ› {self.gated_attn.query_dim}ï¼Œå®é™… {actual_dim}ï¼Œè·³è¿‡æ³¨å…¥")
                return unet_features
        
        # 1. è·å–ä½ç½®ç‰¹å¾
        box_emb = self.position_net(bboxes)  # (B, N, hidden_dim)
        
        # 2. èåˆä½ç½®ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ï¼ˆå¦‚æœæœ‰ï¼‰
        if phrase_embeddings is not None:
            phrase_embeddings = phrase_embeddings.to(dtype=adapter_dtype, device=unet_features.device)
            # æŠ•å½±åˆ° hidden_dim
            if phrase_embeddings.shape[-1] != self.hidden_dim:
                phrase_embeddings = self.text_proj(phrase_embeddings)
            spatial_context = box_emb + phrase_embeddings  # (B, N, hidden_dim)
        else:
            spatial_context = box_emb

        # 2.1 åº”ç”¨ maskï¼ˆè¿‡æ»¤ padding æ¡†ï¼‰
        attn_masks = None
        if masks is not None:
            spatial_context = spatial_context * masks.unsqueeze(-1)
            attn_masks = masks
        
        # 3. é€šè¿‡ Gated Self-Attention æ³¨å…¥
        unet_features_adapter = unet_features.to(dtype=adapter_dtype)
        controlled_features = self.gated_attn(
            unet_features_adapter,
            spatial_context,
            masks=attn_masks
        )
        # Scheduled Sampling: çº¿æ€§ç¼©æ”¾æ³¨å…¥å¼ºåº¦ï¼ˆ0=å…³é—­ï¼Œ1=å…¨å¼€ï¼‰
        if self.beta_scale != 1.0:
            controlled_features = unet_features_adapter + self.beta_scale * (controlled_features - unet_features_adapter)
        if controlled_features.dtype != unet_features.dtype:
            controlled_features = controlled_features.to(dtype=unet_features.dtype)

        return controlled_features


class SpatialControlProcessor:
    """
    Diffusers Attention Processor åŒ…è£…å™¨ï¼ˆå…¼å®¹ SDXL/Kolorsï¼‰
    
    âš ï¸ å…³é”®ä¿®æ­£ï¼šåªåœ¨ Cross-Attention å±‚æ³¨å…¥ï¼ˆGLIGEN è®ºæ–‡è¦æ±‚ï¼‰
    """
    def __init__(self, adapter: SpatialControlAdapter, original_processor=None, is_cross_attn: bool = False):
        self.adapter = adapter
        self.original_processor = original_processor
        self.is_cross_attn = is_cross_attn  # âœ… æ ‡è®°æ˜¯å¦ä¸º Cross-Attention å±‚
        # å­˜å‚¨ bboxes å’Œ phrase_embeddingsï¼ˆé€šè¿‡ä¸Šä¸‹æ–‡ä¼ é€’ï¼‰
        self.bboxes = None
        self.phrase_embeddings = None
        self.masks = None
    
    def __call__(self, attn, hidden_states, encoder_hidden_states=None, 
                 attention_mask=None, scale=None, **kwargs):
        """
        åœ¨ UNet çš„ attention å±‚ä¸­è¢«è°ƒç”¨
        
        Args:
            attn: Attention æ¨¡å—
            hidden_states: UNet ä¸­é—´ç‰¹å¾ (B, T, hidden_dim)
            encoder_hidden_states: æ–‡æœ¬ embedding (B, T_text, hidden_dim)
            attention_mask: æ³¨æ„åŠ›æ©ç 
            scale: ç¼©æ”¾å› å­ï¼ˆSDXL ç‰¹æœ‰ï¼‰
        """
        # âœ… ä¿®æ­£ï¼šå…ˆæ‰§è¡ŒåŸå§‹ Cross-Attentionï¼Œå†æ³¨å…¥ç©ºé—´æ§åˆ¶
        # è¿™æ ·å¯ä»¥è®©æ¨¡å‹å…ˆç†è§£æ–‡æœ¬è¯­ä¹‰ï¼Œå†èåˆç©ºé—´ä¿¡æ¯
        if self.original_processor is not None:
            hidden_states = self.original_processor(
                attn, hidden_states, encoder_hidden_states,
                attention_mask=attention_mask, scale=scale, **kwargs
            )
        else:
            # å›é€€åˆ°æ ‡å‡† attention è®¡ç®—
            hidden_states = self._default_attention(
                attn, hidden_states, encoder_hidden_states, attention_mask
            )
        
        # âœ… ä¿®æ­£ï¼šåªåœ¨ Cross-Attention å±‚ä¸”æœ‰ BBox æ—¶æ³¨å…¥
        if self.is_cross_attn and self.bboxes is not None and encoder_hidden_states is not None:
            # æ£€æŸ¥ç»´åº¦åŒ¹é…
            if len(hidden_states.shape) >= 2 and hidden_states.shape[-1] == self.adapter.gated_attn.query_dim:
                try:
                    # æ³¨å…¥ç©ºé—´æ§åˆ¶
                    hidden_states = self.adapter(
                        hidden_states,
                        self.bboxes,
                        self.phrase_embeddings,
                        masks=self.masks
                    )
                except Exception as e:
                    # é˜²å¾¡æ€§ç¼–ç¨‹ï¼šå¦‚æœæ³¨å…¥å¤±è´¥ï¼Œä¸å½±å“åŸæœ‰æµç¨‹
                    print(f"âš ï¸ Spatial control injection failed: {e}")
        
        return hidden_states
    
    def _default_attention(self, attn, hidden_states, encoder_hidden_states, attention_mask):
        """å›é€€çš„æ ‡å‡† Attention å®ç°"""
        batch_size, sequence_length, _ = hidden_states.shape
        query = attn.to_q(hidden_states)
        
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif hasattr(attn, 'norm_cross') and attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        
        attention_scores = torch.matmul(query, key.transpose(-1, -2))
        attention_scores = attention_scores * (head_dim ** -0.5)
        
        if attention_mask is not None:
            attention_scores = attention_scores + attention_mask
        
        attention_probs = attention_scores.softmax(dim=-1)
        
        if hasattr(attn, 'dropout'):
            attention_probs = F.dropout(attention_probs, p=attn.dropout, training=attn.training)
        
        attention_probs = attention_probs.to(value.dtype)
        hidden_states = torch.matmul(attention_probs, value)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states.to(query.dtype)
        
        # to_out é€šå¸¸æ˜¯ [Linear, Dropout]
        if hasattr(attn, 'to_out'):
            if isinstance(attn.to_out, nn.ModuleList) or isinstance(attn.to_out, nn.Sequential):
                for layer in attn.to_out:
                    hidden_states = layer(hidden_states)
            else:
                hidden_states = attn.to_out(hidden_states)
        
        return hidden_states
    
    def set_spatial_control(self, bboxes, phrase_embeddings=None, masks=None):
        """è®¾ç½®ç©ºé—´æ§åˆ¶ä¿¡æ¯"""
        self.bboxes = bboxes
        self.phrase_embeddings = phrase_embeddings
        self.masks = masks


class SpatialAdapterModuleDict(nn.ModuleDict):
    """
    ç®¡ç†å¤šç»´åº¦ Adapter çš„å®¹å™¨ï¼ŒæŒ‰ hidden_dim å¤ç”¨/åˆ›å»º
    """
    def set_scale(self, scale: float):
        """é€’å½’è®¾ç½®æ‰€æœ‰ Adapter çš„æ³¨å…¥å¼ºåº¦"""
        # Avoid self.modules() here because it includes self and can recurse forever.
        for module in self.values():
            if hasattr(module, "set_scale"):
                module.set_scale(scale)


def create_spatial_adapter_for_kolors() -> SpatialAdapterModuleDict:
    """åˆ›å»ºç”¨äº Kolors/SDXL çš„ Adapter å®¹å™¨ã€‚"""
    return SpatialAdapterModuleDict()


def create_spatial_adapter_for_sdxl() -> SpatialAdapterModuleDict:
    """åˆ›å»ºç”¨äº SDXL çš„ Adapter å®¹å™¨ï¼ˆä¸ Kolors å…¼å®¹ï¼‰ã€‚"""
    return SpatialAdapterModuleDict()


def _strip_state_dict_prefix(state_dict: Dict[str, torch.Tensor], prefix: str = "module.") -> Dict[str, torch.Tensor]:
    if not state_dict:
        return state_dict
    if any(k.startswith(prefix) for k in state_dict.keys()):
        return {k[len(prefix):]: v for k, v in state_dict.items()}
    return state_dict


def load_spatial_adapter_state_dict(
    state_dict: Dict[str, torch.Tensor],
    device: Optional[torch.device] = None,
    dtype: Optional[torch.dtype] = None
) -> SpatialAdapterModuleDict:
    """
    ä» state_dict æ¢å¤ SpatialAdapterModuleDictï¼ˆè‡ªåŠ¨æ„å»ºå„ç»´åº¦ Adapterï¼‰ã€‚
    """
    cleaned = _strip_state_dict_prefix(state_dict)
    adapter_dict = SpatialAdapterModuleDict()
    prefixes = sorted({k.split(".")[0] for k in cleaned.keys()})
    for prefix in prefixes:
        if not prefix.startswith("dim_"):
            continue
        try:
            hidden_dim = int(prefix.split("_", 1)[1])
        except ValueError:
            continue
        text_dim = None
        text_proj_key = f"{prefix}.text_proj.weight"
        if text_proj_key in cleaned:
            text_dim = cleaned[text_proj_key].shape[1]
        if text_dim is None:
            text_dim = 4096
        adapter_dict[prefix] = SpatialControlAdapter(
            hidden_dim=hidden_dim,
            text_dim=text_dim
        )
    if adapter_dict:
        adapter_dict.load_state_dict(cleaned, strict=False)
        if device is not None or dtype is not None:
            adapter_dict = adapter_dict.to(device=device, dtype=dtype)
    return adapter_dict


def _get_attn_layer_dim(unet, name: str) -> Optional[int]:
    """
    è§£æ attention processor å¯¹åº”å±‚çš„ hidden_dim
    æ”¯æŒè§£æ ModuleList çš„æ•°å­—ç´¢å¼•ï¼ˆå¦‚ down_blocks.1.attentions...ï¼‰
    """
    module_path = name.replace(".processor", "")
    sub_mod = unet
    
    for part in module_path.split('.'):
        # å¤„ç†æ•°å­—ç´¢å¼•ï¼ˆé’ˆå¯¹ ModuleListï¼‰
        if part.isdigit():
            try:
                sub_mod = sub_mod[int(part)]
            except (IndexError, TypeError):
                return None
        else:
            if not hasattr(sub_mod, part):
                return None
            sub_mod = getattr(sub_mod, part)
    
    # å°è¯•è·å–ç»´åº¦
    if hasattr(sub_mod, "to_q"):
        return sub_mod.to_q.in_features
    if hasattr(sub_mod, "query_dim"):
        return sub_mod.query_dim
    # SDXL/Kolors éƒ¨åˆ†å±‚å¯èƒ½ä½¿ç”¨ inner_dim
    if hasattr(sub_mod, "inner_dim"):
        return sub_mod.inner_dim
    
    return None


def _is_cross_attention_layer(name: str) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸º Cross-Attention å±‚
    
    SDXL/Kolors å‘½åè§„åˆ™ï¼š
    - attn1: Self-Attention
    - attn2: Cross-Attention
    """
    return 'attn2' in name or 'cross_attn' in name.lower()


def inject_spatial_control_to_unet(
    unet,
    adapter_dict: Optional[SpatialAdapterModuleDict] = None,
    bboxes: Optional[torch.Tensor] = None,
    phrase_embeddings: Optional[torch.Tensor] = None,
    masks: Optional[torch.Tensor] = None,
    num_heads: int = 8,
    adapter_dtype: Optional[torch.dtype] = None,
) -> Tuple[Dict, Dict, SpatialAdapterModuleDict]:
    """
    å°†ç©ºé—´æ§åˆ¶æ³¨å…¥åˆ° UNet ä¸­ï¼ˆå¤šç»´åº¦é€‚é…ï¼‰
    
    Args:
        unet: Kolors/SDXL UNet
        adapter_dict: ç®¡ç†æ‰€æœ‰ç»´åº¦ Adapter çš„ ModuleDictï¼ˆå¯å¤ç”¨/è®­ç»ƒï¼‰
        bboxes: (B, N, 4) å½’ä¸€åŒ–åæ ‡
        phrase_embeddings: (B, N, text_dim) æ–‡æœ¬ç‰¹å¾ï¼Œå¯é€‰
        masks: (B, N) æœ‰æ•ˆæ¡†æ©ç ï¼Œå¯é€‰
        num_heads: é»˜è®¤ 8
        adapter_dtype: Adapter æƒé‡ dtypeï¼ˆé»˜è®¤è·Ÿéš UNetï¼‰
    
    Returns:
        processors: åŸå§‹ processorsï¼ˆç”¨äºæ¢å¤ï¼‰
        spatial_processors: æ³¨å…¥åçš„ processors
        adapter_dict: æ›´æ–°åçš„å®¹å™¨ï¼ˆä¾¿äºå¤–éƒ¨å¤ç”¨ï¼‰
    """
    if adapter_dict is None:
        adapter_dict = SpatialAdapterModuleDict()
    
    processors = {}
    spatial_processors = {}
    
    # è·å– UNet è®¾å¤‡å’Œ dtype
    try:
        unet_device = next(unet.parameters()).device
        unet_dtype = next(unet.parameters()).dtype
    except StopIteration:
        unet_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        unet_dtype = torch.float32
    
    # ç§»åŠ¨ bboxes å’Œ phrase_embeddings åˆ°æ­£ç¡®è®¾å¤‡
    if bboxes is not None and bboxes.device != unet_device:
        bboxes = bboxes.to(unet_device)
    if phrase_embeddings is not None and phrase_embeddings.device != unet_device:
        phrase_embeddings = phrase_embeddings.to(unet_device)
    if masks is not None and masks.device != unet_device:
        masks = masks.to(unet_device)
    
    def get_or_create_adapter(layer_dim: int) -> SpatialControlAdapter:
        """è·å–æˆ–åˆ›å»ºæŒ‡å®šç»´åº¦çš„ Adapter"""
        text_dim = phrase_embeddings.shape[-1] if phrase_embeddings is not None else 4096
        key = f"dim_{layer_dim}"
        target_dtype = adapter_dtype if adapter_dtype is not None else unet_dtype
        
        if key not in adapter_dict:
            adapter_dict[key] = SpatialControlAdapter(
                hidden_dim=layer_dim,
                num_heads=num_heads,
                text_dim=text_dim
            ).to(device=unet_device, dtype=target_dtype)
        else:
            # ç¡®ä¿è®¾å¤‡å’Œ dtype åŒ¹é…
            if next(adapter_dict[key].parameters()).device != unet_device:
                adapter_dict[key] = adapter_dict[key].to(unet_device)
            if next(adapter_dict[key].parameters()).dtype != target_dtype:
                adapter_dict[key] = adapter_dict[key].to(dtype=target_dtype)
        
        return adapter_dict[key]
    
    # æ–¹æ³• 1: ä½¿ç”¨ diffusers çš„ get_attn_processorsï¼ˆæ¨èï¼‰
    try:
        original_processors_dict = unet.attn_processors
        new_processors = {}
        
        for name, original_processor in original_processors_dict.items():
            # è·å–å±‚ç»´åº¦
            layer_dim = _get_attn_layer_dim(unet, name)
            if layer_dim is None:
                processors[name] = original_processor
                new_processors[name] = original_processor
                continue
            
            # âœ… å…³é”®ä¿®æ­£ï¼šåªåœ¨ Cross-Attention å±‚æ³¨å…¥
            is_cross_attn = _is_cross_attention_layer(name)
            
            # å¦‚æœå·²ç»æ˜¯ SpatialControlProcessorï¼Œé¿å…é‡å¤åŒ…è£…
            if isinstance(original_processor, SpatialControlProcessor):
                base_processor = original_processor.original_processor
                processors[name] = base_processor
                spatial_processor = original_processor
                if bboxes is not None:
                    spatial_processor.set_spatial_control(bboxes, phrase_embeddings, masks=masks)
                new_processors[name] = spatial_processor
                spatial_processors[name] = spatial_processor
                continue
            
            # ä»…åœ¨ Cross-Attention å±‚åˆ›å»ºå¹¶æ³¨å†Œ SpatialControlProcessor
            processors[name] = original_processor
            if is_cross_attn:
                # åˆ›å»ºæˆ–è·å– Adapter
                adapter = get_or_create_adapter(layer_dim)
                # åˆ›å»º Spatial Processor
                spatial_processor = SpatialControlProcessor(
                    adapter,
                    original_processor,
                    is_cross_attn=is_cross_attn  # âœ… ä¼ é€’ Cross-Attention æ ‡è®°
                )

                if bboxes is not None:
                    spatial_processor.set_spatial_control(bboxes, phrase_embeddings, masks=masks)

                new_processors[name] = spatial_processor
                spatial_processors[name] = spatial_processor
            else:
                # ä¿æŒåŸå§‹ processorï¼ˆä¸åŒ…è£… self-attentionï¼‰
                new_processors[name] = original_processor
        
        # è®¾ç½®æ–°çš„ processors
        unet.set_attn_processor(new_processors)
        
    except AttributeError:
        # æ–¹æ³• 2: å›é€€åˆ°ç›´æ¥è®¿é—® module.processor
        for name, module in unet.named_modules():
            if hasattr(module, 'processor') and 'attn' in name.lower():
                layer_dim = _get_attn_layer_dim(unet, name)
                if layer_dim is None:
                    continue
                
                is_cross_attn = _is_cross_attention_layer(name)
                original_processor = module.processor
                processors[name] = original_processor

                if is_cross_attn:
                    adapter = get_or_create_adapter(layer_dim)
                    spatial_processor = SpatialControlProcessor(
                        adapter,
                        original_processor,
                        is_cross_attn=is_cross_attn
                    )

                    if bboxes is not None:
                        spatial_processor.set_spatial_control(bboxes, phrase_embeddings, masks=masks)

                    module.processor = spatial_processor
                    spatial_processors[name] = spatial_processor
                else:
                    # ä¿æŒ module.processor ä¸å˜ï¼ˆself-attention ä¸æ³¨å…¥ï¼‰
                    pass
    
    return processors, spatial_processors, adapter_dict


def remove_spatial_control_from_unet(unet, original_processors: Dict):
    """
    ç§»é™¤ç©ºé—´æ§åˆ¶ï¼Œæ¢å¤åŸå§‹ processors
    
    Args:
        unet: Kolors UNet æ¨¡å‹
        original_processors: inject_spatial_control_to_unet è¿”å›çš„åŸå§‹ processors
    """
    if not isinstance(original_processors, dict) or len(original_processors) == 0:
        return
    
    try:
        # æ–¹æ³• 1: ä½¿ç”¨ diffusers çš„ set_attn_processor
        current_processors = unet.attn_processors
        restore_dict = {}
        
        for current_key in current_processors.keys():
            if current_key in original_processors:
                restore_dict[current_key] = original_processors[current_key]
            else:
                # å°è¯•ä¸åŒçš„ key æ ¼å¼
                key_without_suffix = current_key.replace('.processor', '')
                if key_without_suffix in original_processors:
                    restore_dict[current_key] = original_processors[key_without_suffix]
        
        if restore_dict:
            unet.set_attn_processor(restore_dict)
        else:
            unet.set_attn_processor(original_processors)
    
    except (AttributeError, TypeError, KeyError):
        # æ–¹æ³• 2: å›é€€åˆ°ç›´æ¥è®¿é—® module.processor
        for name, module in unet.named_modules():
            if hasattr(module, 'processor'):
                if name in original_processors:
                    module.processor = original_processors[name]
                elif name.replace('.processor', '') in original_processors:
                    module.processor = original_processors[name.replace('.processor', '')]


def get_trainable_parameters(adapter_dict: SpatialAdapterModuleDict) -> List[torch.nn.Parameter]:
    """
    è·å–æ‰€æœ‰ Adapter çš„å¯è®­ç»ƒå‚æ•°
    
    Args:
        adapter_dict: Adapter å®¹å™¨
    
    Returns:
        params: å¯è®­ç»ƒå‚æ•°åˆ—è¡¨
    """
    params = []
    for adapter in adapter_dict.values():
        params.extend(list(adapter.parameters()))
    return params
