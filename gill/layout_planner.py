"""
å¸ƒå±€è§„åˆ’å™¨ (Layout Planner)

åŸºäº DeepSeek-7Bï¼Œä½¿ç”¨ LoRA å¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿè¾“å‡ºç»“æ„åŒ–å¸ƒå±€ä¿¡æ¯ã€‚

è¾“å‡ºæ ¼å¼ï¼š<obj>å¯¹è±¡å</obj><box>[x1,y1,x2,y2]</box>
"""

import torch
import torch.nn as nn
from typing import List, Dict, Optional
import re
import json


def parse_layout_output(text: str) -> List[Dict]:
    """
    è§£æå¸ƒå±€è¾“å‡ºæ–‡æœ¬ï¼Œæå–å¯¹è±¡å’Œåæ ‡
    
    è¾“å…¥æ ¼å¼ï¼š<obj>å¯¹è±¡å</obj><box>[x1,y1,x2,y2]</box>...
    
    è¿”å›ï¼š[{"name": "å¯¹è±¡å", "bbox": [x1, y1, x2, y2]}]
    """
    objects = []
    
    # åŒ¹é… <obj>...</obj><box>...</box> æ¨¡å¼
    pattern = r'<obj>([^<]+)</obj><box>\[([^\]]+)\]</box>'
    matches = re.findall(pattern, text)
    
    for name, bbox_str in matches:
        try:
            # è§£æåæ ‡
            bbox = [float(x.strip()) for x in bbox_str.split(',')]
            if len(bbox) == 4:
                objects.append({
                    "name": name.strip(),
                    "bbox": bbox
                })
        except:
            continue
    
    return objects


def format_layout_input(prompt: str) -> str:
    """
    æ ¼å¼åŒ–è¾“å…¥ prompt ä¸º Instruction Tuning æ ¼å¼
    
    ç¤ºä¾‹ï¼š
    è¾“å…¥ï¼š"ç”»ä¸€åªåœ¨æ¡Œå­å·¦è¾¹çš„çŒ«"
    è¾“å‡ºï¼š"ç”¨æˆ·ï¼šç”»ä¸€åªåœ¨æ¡Œå­å·¦è¾¹çš„çŒ«\nåŠ©æ‰‹ï¼š<obj>çŒ«</obj><box>[0.0,0.3,0.4,0.7]</box>"
    """
    return f"ç”¨æˆ·ï¼š{prompt}\nåŠ©æ‰‹ï¼š"


class LayoutPlanner(nn.Module):
    """
    å¸ƒå±€è§„åˆ’å™¨
    
    åŸºäºé¢„è®­ç»ƒçš„ LLMï¼ˆå¦‚ DeepSeek-7Bï¼‰ï¼Œé€šè¿‡ LoRA å¾®è°ƒ
    å­¦ä¹ å°†è‡ªç„¶è¯­è¨€æè¿°è½¬æ¢ä¸ºç»“æ„åŒ–å¸ƒå±€ä¿¡æ¯ã€‚
    """
    
    def __init__(self, base_model_path: str, device: str = 'cuda', use_lora: bool = True):
        """
        Args:
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå¦‚ DeepSeek-7Bï¼‰
            device: è®¾å¤‡
            use_lora: æ˜¯å¦ä½¿ç”¨ LoRA å¾®è°ƒï¼ˆæ¨èï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰
        """
        super().__init__()
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        # è§„èŒƒåŒ– device å‚æ•°ï¼Œä¾¿äºåç»­ç»Ÿä¸€å¤„ç†
        # - "cuda" è§†ä¸º "cuda:0"
        # - "cuda:0,1" è¡¨ç¤ºä½¿ç”¨ 0ã€1 ä¸¤å¼ å¡åš tensor parallel
        if isinstance(device, str):
            if device == "cuda":
                norm_device = "cuda:0"
            else:
                norm_device = device
        else:
            norm_device = device
        self.device = norm_device
        self.tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=True
        )
        
        # æ·»åŠ å¸ƒå±€ç›¸å…³çš„ç‰¹æ®Š token
        special_tokens = {
            "additional_special_tokens": ["<obj>", "</obj>", "<box>", "</box>"]
        }
        num_added = self.tokenizer.add_special_tokens(special_tokens)
        print(f"âœ“ æ·»åŠ äº† {num_added} ä¸ªå¸ƒå±€ç‰¹æ®Š token")
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        print(f"ğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
        # å¤šå¡è®¾ç½®ï¼š
        # - å¦‚æœ device å½¢å¦‚ "cuda:0,1"ï¼Œä½¿ç”¨ Hugging Face çš„ tensor parallelï¼Œ
        #   é™åˆ¶æƒé‡åªåˆ‡åˆ° 0ã€1 ä¸¤å¼ å¡ä¸Šï¼Œå¹¶ç¦æ­¢ offload åˆ° CPUï¼ˆé¿å…åƒæ»¡å†…å­˜ï¼‰
        # - å¦åˆ™ï¼š
        #   - device == "auto" æ—¶ç”± HF è‡ªå·±å†³å®šï¼ˆå¯èƒ½ç”¨åˆ°å¤šå¡+CPUï¼‰
        #   - å…¶ä»–æƒ…å†µè®¤ä¸ºæ˜¯å•è®¾å¤‡ï¼Œå¦‚ "cuda:0"
        if isinstance(norm_device, str) and norm_device.startswith("cuda") and "," in norm_device:
            # è§£æ GPU id åˆ—è¡¨ï¼Œä¾‹å¦‚ "cuda:0,1"
            gpu_ids = []
            for part in norm_device.split(","):
                part = part.strip()
                if ":" in part:
                    idx = int(part.split(":")[1])
                else:
                    idx = int(part)
                gpu_ids.append(idx)

            # accelerate è¦æ±‚ max_memory çš„ key ä¸ºæ•´æ•° GPU id æˆ– 'cpu' / 'disk'
            max_memory = {i: "22GiB" for i in gpu_ids}
            # ç¦æ­¢ offload åˆ° CPUï¼Œå°½é‡åªç”¨æ˜¾å­˜ï¼ˆå¦‚æœæƒ³å…è®¸å°‘é‡ offloadï¼Œå¯ä»¥æ”¹æˆæ¯”å¦‚ '8GiB'ï¼‰
            max_memory["cpu"] = "0GiB"

            print(f"âœ“ ä½¿ç”¨å¤šå¡ tensor parallel: GPUs={gpu_ids}, max_memory={max_memory}")
            self.model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                max_memory=max_memory,
                trust_remote_code=True,
            )
            # tokenizer / è¾“å…¥ç»Ÿä¸€èµ°ä¸»å¡
            self.device = f"cuda:{gpu_ids[0]}"
        else:
            if norm_device == "auto":
                device_map_arg = "auto"
            else:
                # å• GPUï¼šæ˜¾å¼ç»‘å®šåˆ°æŒ‡å®šå¡ï¼Œé¿å…è‡ªåŠ¨ offload åˆ° CPU
                device_map_arg = norm_device
            print(f"âœ“ ä½¿ç”¨ device_map={device_map_arg}")
            self.model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                torch_dtype=torch.bfloat16,
                device_map=device_map_arg,
                trust_remote_code=True
            )
        
        # è°ƒæ•´ embedding å¤§å°
        if num_added > 0:
            self.model.resize_token_embeddings(len(self.tokenizer))
        
        # å…¨å‚æ•°å¾®è°ƒæ—¶å¯ç”¨ gradient checkpointing ä»¥èŠ‚çœæ˜¾å­˜
        if not use_lora and hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()
            print("âœ“ å¯ç”¨ gradient checkpointingï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
        
        # ä½¿ç”¨ LoRA å¾®è°ƒï¼ˆæ¨èï¼‰
        if use_lora:
            try:
                from peft import LoraConfig, get_peft_model
                
                peft_config = LoraConfig(
                    r=16,
                    lora_alpha=32,
                    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # æ ¹æ®æ¨¡å‹ç»“æ„è°ƒæ•´
                    lora_dropout=0.05,
                    bias="none",
                    task_type="CAUSAL_LM"
                )
                self.model = get_peft_model(self.model, peft_config)
                print("âœ“ ä½¿ç”¨ LoRA å¾®è°ƒï¼ˆå‚æ•°é‡å¤§å¹…å‡å°‘ï¼‰")
            except ImportError:
                print("âš ï¸ peft æœªå®‰è£…ï¼Œä½¿ç”¨å…¨é‡å¾®è°ƒï¼ˆéœ€è¦æ›´å¤šæ˜¾å­˜ï¼‰")
                use_lora = False
        
        self.use_lora = use_lora
        
        # #region agent log
        import json as _json, time as _time
        if torch.cuda.is_available():
            try:
                log_device = torch.device(device) if device != "auto" else torch.device("cuda:0")
            except Exception:
                log_device = torch.device("cuda:0")
            mem_allocated = torch.cuda.memory_allocated(log_device) / 1024**3
            mem_reserved = torch.cuda.memory_reserved(log_device) / 1024**3
            with open("/home/lxh/Project/gill-main/.cursor/debug.log", "a") as _f:
                _f.write(
                    _json.dumps(
                        {
                            "sessionId": "debug-session",
                            "runId": "oom_debug",
                            "hypothesisId": "H1",
                            "location": "layout_planner.py:__init__",
                            "message": "model_loaded_memory",
                            "data": {
                                "use_lora": use_lora,
                                "mem_allocated_gb": round(mem_allocated, 2),
                                "mem_reserved_gb": round(mem_reserved, 2),
                            },
                            "timestamp": int(_time.time() * 1000),
                        }
                    )
                    + "\n"
                )
        # #endregion
        
        self.model.eval()
    
    def forward(self, input_ids, labels=None, **kwargs):
        """Forward pass for training"""
        return self.model(input_ids=input_ids, labels=labels, **kwargs)
    
    def generate_layout(self, prompt: str, max_length: int = 128,
                       temperature: float = 0.2, top_p: float = 1.0,
                       apply_refinement: bool = True) -> Dict:
        """
        ç”Ÿæˆå¸ƒå±€è§„åˆ’
        
        Args:
            prompt: è¾“å…¥æ–‡æœ¬ï¼ˆå¦‚"ç”»ä¸€åªåœ¨æ¡Œå­å·¦è¾¹çš„çŒ«"ï¼‰
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
            top_p: nucleus sampling å‚æ•°
        
        Returns:
            {
                "layout_text": "<obj>...</obj><box>...</box>",
                "objects": [{"name": "...", "bbox": [...]}]
            }
        """
        self.model.eval()
        
        # æ ¼å¼åŒ–è¾“å…¥
        formatted_input = format_layout_input(prompt)
        
        # Tokenize
        inputs = self.tokenizer(
            formatted_input,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.device)
        
        # ç”Ÿæˆï¼šæ¨ç†é˜¶æ®µä½¿ç”¨ä½æ¸©åº¦ + greedyï¼Œå‡å°‘éšæœºæ€§å’Œé‡å¤
        # ä½¿ç”¨ max_new_tokens è€Œä¸æ˜¯ max_lengthï¼Œé¿å…è¾“å…¥é•¿åº¦è¶…è¿‡ max_length çš„é—®é¢˜
        input_length = inputs['input_ids'].shape[1]
        max_new_tokens = max(max_length - input_length, 1)  # ç¡®ä¿è‡³å°‘ç”Ÿæˆ 1 ä¸ª token
        
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=False,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        # è§£ç 
        generated_text = self.tokenizer.decode(
            generated_ids[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=False
        )
        
        # æå–å¸ƒå±€éƒ¨åˆ†ï¼ˆä» "åŠ©æ‰‹ï¼š" ä¹‹åï¼‰
        if "åŠ©æ‰‹ï¼š" in generated_text:
            layout_text = generated_text.split("åŠ©æ‰‹ï¼š")[-1].strip()
        else:
            layout_text = generated_text.strip()

        # æ¸…ç†å¯èƒ½æ®‹ç•™çš„ BOS tokenï¼ˆå¦‚ DeepSeek çš„ <ï½œbeginâ–ofâ–sentenceï½œ>ï¼‰
        bos = getattr(self.tokenizer, "bos_token", None)
        if isinstance(bos, str) and bos in layout_text:
            layout_text = layout_text.replace(bos, "").strip()
        
        # è§£æå¯¹è±¡å’Œåæ ‡
        objects = parse_layout_output(layout_text)

        # æ ¹æ®ä¸­æ–‡ä½ç½®è¯å¯¹ bbox åšä¸€æ¬¡å¯å‘å¼â€œå¸é™„ä¿®æ­£â€ï¼Œå¢å¼ºå·¦/å³/ä¸Š/ä¸‹ç­‰æ–¹å‘ä¸€è‡´æ€§
        if apply_refinement:
            objects = refine_layout_with_caption(prompt, objects)
        
        return {
            "layout_text": layout_text,
            "objects": objects
        }
    
    def batch_generate(self, prompts: List[str], **kwargs) -> List[Dict]:
        """æ‰¹é‡ç”Ÿæˆå¸ƒå±€"""
        results = []
        for prompt in prompts:
            result = self.generate_layout(prompt, **kwargs)
            results.append(result)
        return results


def create_layout_planner_from_gill(gill_model, tokenizer, use_lora: bool = True) -> LayoutPlanner:
    """
    ä» GILL æ¨¡å‹åˆ›å»ºå¸ƒå±€è§„åˆ’å™¨
    
    ä½¿ç”¨ GILL ä¸­çš„ LLM ä½œä¸ºåŸºç¡€æ¨¡å‹
    """
    base_lm = gill_model.model.lm
    
    # è·å–æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼‰
    if hasattr(base_lm, 'config') and hasattr(base_lm.config, '_name_or_path'):
        model_path = base_lm.config._name_or_path
    else:
        # é»˜è®¤è·¯å¾„
        model_path = "./model/deepseek-llm-7b-base"
    
    planner = LayoutPlanner(model_path, device=base_lm.device, use_lora=use_lora)
    
    return planner


def refine_layout_with_caption(caption: str, objects: List[Dict]) -> List[Dict]:
    """
    åå¤„ç†ï¼šæ›´å¼ºåœ°ä¾èµ– caption çš„ä¸­æ–‡ä½ç½®è¯å’Œåè¯ï¼Œ
    åœ¨æ£€æµ‹åˆ°æ˜æ˜¾çš„ã€Œå·¦/å³/ä¸Š/ä¸‹/ä¸­é—´/å·¦ä¸‹è§’/å³ä¸‹è§’ã€ç»“æ„æ—¶ï¼Œ
    ç›´æ¥ç”¨å¯å‘å¼è§„åˆ™é‡å»ºä¸€ä¸ªå°çš„ objects åˆ—è¡¨ï¼Œå¿½ç•¥ LM åŸå§‹ objectsã€‚
    """
    if not objects:
        objects = []

    text = str(caption)

    # å¦‚æœ caption é‡Œæ²¡æœ‰ä»»ä½•ä½ç½®è¯ï¼Œå°±ç›´æ¥è¿”å›åŸå§‹ objectsï¼ˆåªåšåå­—æ¸…æ´—ï¼‰
    position_keywords = ["å·¦è¾¹", "å·¦ä¾§", "å·¦æ–¹", "å³è¾¹", "å³ä¾§", "å³æ–¹",
                         "ä¸Šæ–¹", "ä¸Šè¾¹", "ä¸Šé¢", "ä¸‹æ–¹", "ä¸‹è¾¹", "ä¸‹é¢",
                         "ä¸­é—´", "ä¸­å¤®", "ä¸­å¿ƒ", "å·¦ä¸‹è§’", "å³ä¸‹è§’"]
    has_position = any(k in text for k in position_keywords)

    # ç®€å•åè¯æŠ½å–ï¼šä¼˜å…ˆç”¨ jieba.possegï¼Œä¸è¡Œå°±ç”¨ä¸€ä¸ªå¾ˆç²—ç³™çš„å¤‡é€‰æ–¹æ¡ˆ
    nouns: List[str] = []
    try:
        import jieba.posseg as pseg  # type: ignore

        words = pseg.cut(text)
        for w, flag in words:
            if flag.startswith("n"):  # åè¯
                w = w.strip()
                if w and w not in nouns:
                    nouns.append(w)
    except Exception:
        # ç®€å•å…œåº•ï¼šæŒ‰å¸¸è§åˆ†éš”ç¬¦åˆ‡åˆ†ï¼Œå–é•¿åº¦ 1~4 çš„çŸ­ç‰‡æ®µ
        rough_parts = []
        for seg in re.split(r"[ï¼Œã€‚ã€â€œâ€ï¼!ï¼Ÿ?\s]", text):
            seg = seg.strip()
            if 1 <= len(seg) <= 4:
                rough_parts.append(seg)
        nouns = list(dict.fromkeys(rough_parts))  # å»é‡ä¸”ä¿æŒé¡ºåº

    # åªä¿ç•™å‰ä¸¤ä¸ªåè¯ï¼Œåˆ†åˆ«å½“ä½œã€Œä¸»/å‰¯ã€å¯¹è±¡
    if nouns:
        main_name = nouns[0]
        second_name = nouns[1] if len(nouns) > 1 else None
    else:
        main_name = objects[0].get("name", "ç‰©ä½“") if objects else "ç‰©ä½“"
        second_name = objects[1].get("name", None) if len(objects) > 1 else None

    # é¢„å®šä¹‰æ§½ä½
    slots = {
        "left": [0.0, 0.1, 0.4, 0.9],
        "right": [0.6, 0.1, 1.0, 0.9],
        "top": [0.1, 0.0, 0.9, 0.4],
        "bottom": [0.1, 0.6, 0.9, 1.0],
        "center": [0.3, 0.3, 0.7, 0.7],
        "bottom_left": [0.0, 0.6, 0.4, 1.0],
        "bottom_right": [0.6, 0.6, 1.0, 1.0],
    }

    # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„ä½ç½®è¯ï¼Œå°±åªåšåå­—æ¸…æ´—ï¼Œä¿æŒåŸç»“æœ
    if not has_position:
        for obj in objects:
            name = str(obj.get("name", "")).strip()
            if name.startswith(("æ˜¯", "åœ¨", "æœ‰")) and len(name) > 1:
                name = name[1:]
            obj["name"] = name
        return objects

    # æœ‰æ˜æ˜¾ä½ç½®è¯æ—¶ï¼šç›´æ¥æ ¹æ® caption é‡å»º objects åˆ—è¡¨ï¼ˆæœ€å¤šä¸¤ä¸ªå¯¹è±¡ï¼‰
    new_objects: List[Dict] = []

    def add_obj(name: str, slot_key: str):
        name = name.strip() or "ç‰©ä½“"
        # å»æ‰å‰å¯¼è™šè¯
        if name.startswith(("æ˜¯", "åœ¨", "æœ‰")) and len(name) > 1:
            name = name[1:]
        bbox = slots[slot_key]
        new_objects.append({"name": name, "bbox": bbox})

    # é»˜è®¤æ§½ä½
    main_slot = "center"
    second_slot = "center"

    # å·¦ / å³
    if any(k in text for k in ["å·¦è¾¹", "å·¦ä¾§", "å·¦æ–¹"]):
        main_slot = "left"
    if any(k in text for k in ["å³è¾¹", "å³ä¾§", "å³æ–¹"]):
        second_slot = "right"

    # ä¸Š / ä¸‹
    if any(k in text for k in ["ä¸Šæ–¹", "ä¸Šè¾¹", "ä¸Šé¢"]):
        main_slot = "top"
    if any(k in text for k in ["ä¸‹æ–¹", "ä¸‹è¾¹", "ä¸‹é¢"]):
        second_slot = "bottom"

    # ä¸­é—´ / ä¸­å¤®
    if any(k in text for k in ["ä¸­é—´", "ä¸­å¤®", "ä¸­å¿ƒ"]):
        main_slot = "center"

    # å·¦ä¸‹è§’ / å³ä¸‹è§’ ä¼˜å…ˆçº§æ›´é«˜ï¼Œè¦†ç›–å‰é¢çš„ bottom / left/right
    if "å·¦ä¸‹è§’" in text:
        second_slot = "bottom_left"
    if "å³ä¸‹è§’" in text:
        second_slot = "bottom_right"

    # ä¸»å¯¹è±¡
    add_obj(main_name, main_slot)
    # å‰¯å¯¹è±¡ï¼ˆå¦‚æœæœ‰ï¼‰
    if second_name is not None:
        add_obj(second_name, second_slot)

    return new_objects


def train_layout_planner(planner: LayoutPlanner, train_loader, 
                        optimizer, num_epochs: int = 3, device: str = 'cuda'):
    """
    è®­ç»ƒå¸ƒå±€è§„åˆ’å™¨ï¼ˆInstruction Tuningï¼‰
    
    è®­ç»ƒæ•°æ®æ ¼å¼ï¼š
    {
        "input": "ç”»ä¸€åªåœ¨æ¡Œå­å·¦è¾¹çš„çŒ«",
        "output": "<obj>çŒ«</obj><box>[0.0,0.3,0.4,0.7]</box>"
    }
    """
    planner.model.train()
    
    for epoch in range(num_epochs):
        total_loss = 0.0
        num_batches = 0
        
        for batch in train_loader:
            # æ ¼å¼åŒ–è¾“å…¥è¾“å‡º
            inputs = [format_layout_input(item["input"]) for item in batch]
            targets = [item["output"] for item in batch]
            
            # Tokenize
            # å¯¹äºå¤š GPUï¼ˆdevice_map="auto"ï¼‰ï¼Œéœ€è¦ç¡®å®šè¾“å…¥åº”è¯¥æ”¾åœ¨å“ªä¸ªè®¾å¤‡
            # é€šå¸¸æ”¾åœ¨ç¬¬ä¸€ä¸ª GPU æˆ–æ¨¡å‹çš„ç¬¬ä¸€ä¸ªè®¾å¤‡
            if device == "auto":
                # å¤š GPU æ¨¡å¼ï¼šæ‰¾åˆ°ç¬¬ä¸€ä¸ªè®¾å¤‡
                if hasattr(planner.model, 'hf_device_map') and planner.model.hf_device_map:
                    # hf_device_map çš„æ ¼å¼å¯èƒ½æ˜¯ {"layer_name": device_index} æˆ– {"layer_name": "cuda:0"}
                    first_device_value = list(planner.model.hf_device_map.values())[0]
                    if isinstance(first_device_value, torch.device):
                        input_device = first_device_value
                    elif isinstance(first_device_value, str):
                        input_device = torch.device(first_device_value)
                    elif isinstance(first_device_value, int):
                        # è®¾å¤‡ç´¢å¼•ï¼Œå¦‚ 0, 1
                        input_device = torch.device(f"cuda:{first_device_value}")
                    else:
                        input_device = torch.device("cuda:0")
                else:
                    # å›é€€åˆ° cuda:0
                    input_device = torch.device("cuda:0")
            elif isinstance(device, str) and device.startswith("cuda"):
                # å• GPU æ¨¡å¼ï¼Œå¦‚ "cuda:0"
                input_device = torch.device(device)
            else:
                # å…¶ä»–æƒ…å†µï¼ˆå¦‚ torch.device å¯¹è±¡ï¼‰
                input_device = device if isinstance(device, torch.device) else torch.device(device)
            
            input_encodings = planner.tokenizer(
                inputs,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(input_device)
            
            target_encodings = planner.tokenizer(
                targets,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=128
            ).to(input_device)
            
            # æ‹¼æ¥è¾“å…¥å’Œè¾“å‡ºï¼ˆç”¨äº causal LM è®­ç»ƒï¼‰
            input_ids = torch.cat([input_encodings.input_ids, target_encodings.input_ids], dim=1)
            labels = input_ids.clone()
            # åªå¯¹è¾“å‡ºéƒ¨åˆ†è®¡ç®— loss
            labels[:, :input_encodings.input_ids.shape[1]] = -100
            
            # #region agent log
            import json as _json, time as _time
            if torch.cuda.is_available():
                try:
                    # ç¡®ä¿ log_device æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ cuda device
                    if isinstance(input_device, torch.device) and input_device.type == "cuda":
                        log_device = input_device
                    else:
                        log_device = torch.device("cuda:0")
                    mem_before_forward = torch.cuda.memory_allocated(log_device) / 1024**3
                    seq_len = input_ids.shape[1]
                    with open("/home/lxh/Project/gill-main/.cursor/debug.log", "a") as _f:
                        _f.write(
                            _json.dumps(
                                {
                                    "sessionId": "debug-session",
                                    "runId": "oom_debug",
                                    "hypothesisId": "H2",
                                    "location": "layout_planner.py:train_layout_planner",
                                    "message": "before_forward",
                                    "data": {
                                        "batch_size": len(batch),
                                        "seq_len": int(seq_len),
                                        "log_device": str(log_device),
                                        "mem_allocated_gb": round(mem_before_forward, 2),
                                    },
                                    "timestamp": int(_time.time() * 1000),
                                }
                            )
                            + "\n"
                        )
                except Exception:
                    pass
            # #endregion
            
            # Forward
            outputs = planner.model(input_ids=input_ids, labels=labels)
            loss = outputs.loss
            
            # #region agent log
            if torch.cuda.is_available():
                try:
                    # ç¡®ä¿ log_device æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ cuda device
                    if isinstance(input_device, torch.device) and input_device.type == "cuda":
                        log_device = input_device
                    else:
                        log_device = torch.device("cuda:0")
                    mem_after_forward = torch.cuda.memory_allocated(log_device) / 1024**3
                    with open("/home/lxh/Project/gill-main/.cursor/debug.log", "a") as _f:
                        _f.write(
                            _json.dumps(
                                {
                                    "sessionId": "debug-session",
                                    "runId": "oom_debug",
                                    "hypothesisId": "H3",
                                    "location": "layout_planner.py:train_layout_planner",
                                    "message": "after_forward_before_backward",
                                    "data": {
                                        "log_device": str(log_device),
                                        "mem_allocated_gb": round(mem_after_forward, 2),
                                        "loss": float(loss.item()),
                                    },
                                    "timestamp": int(_time.time() * 1000),
                                }
                            )
                            + "\n"
                        )
                except Exception:
                    pass
            # #endregion
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")
    
    planner.model.eval()
    return planner
