"""
å¸ƒå±€è§„åˆ’å™¨ (Layout Planner) - Refactored

åŸºäº DeepSeek/Qwenï¼Œä½¿ç”¨ LoRA å¾®è°ƒï¼Œé€šè¿‡ Instruction Tuning è¾“å‡ºç»“æ„åŒ–å¸ƒå±€ã€‚
ä¸è®­ç»ƒè„šæœ¬å¯¹é½ï¼Œç»Ÿä¸€ä½¿ç”¨ Chat Templateã€‚
"""

import torch
import torch.nn as nn
from typing import List, Dict, Optional
import re
import json

FORMAT_INSTRUCTION = (
    "è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š<obj>åç§°</obj><box>[x1,y1,x2,y2]</box>... "
    "åæ ‡èŒƒå›´ä¸º0-1000ï¼Œä¸”å¿…é¡»è¦†ç›–æç¤ºä¸­çš„æ‰€æœ‰å®ä½“ï¼Œä¸è¦é—æ¼ã€‚"
    "å¦‚æœæ— æ³•ç»™å‡ºå¸ƒå±€ï¼Œè¯·è¾“å‡º <no_layout>ã€‚åªè¾“å‡ºæ ¼å¼ï¼Œä¸è¦è§£é‡Šã€‚"
)

STRICT_ENTITY_INSTRUCTION = (
    "ã€è¦†ç›–å¼ºçº¦æŸã€‘è¯·å…ˆåœ¨å¿ƒé‡Œåˆ—å‡ºæç¤ºä¸­çš„æ‰€æœ‰å®ä½“ï¼ˆä¸è¦è¾“å‡ºæ¸…å•ï¼‰ï¼Œ"
    "è¾“å‡ºå¿…é¡»é€ä¸€è¦†ç›–è¿™äº›å®ä½“ï¼Œæ•°é‡å¿…é¡»ä¸€è‡´ï¼›"
    "ç¦æ­¢åˆå¹¶/æ³›åŒ–/çœç•¥ï¼Œç¦æ­¢ç”¨â€œç‰©ä½“/ä¸œè¥¿/æ™¯ç‰©â€ç­‰æ›¿ä»£ã€‚"
)


def _append_format_instruction(text: str, strict_entities: bool = False) -> str:
    if "<obj>" in text or "åªè¾“å‡ºæ ¼å¼" in text:
        base = text
    else:
        base = f"{text}\n\n{FORMAT_INSTRUCTION}"
    if strict_entities and "è¦†ç›–å¼ºçº¦æŸ" not in base:
        return f"{base}\n{STRICT_ENTITY_INSTRUCTION}"
    return base


def parse_layout_output(text: str) -> List[Dict]:
    """
    è§£æå¸ƒå±€è¾“å‡ºæ–‡æœ¬ï¼Œæå–å¯¹è±¡å’Œåæ ‡
    è¾“å…¥æ ¼å¼ï¼š<obj>å¯¹è±¡å</obj><box>[x1,y1,x2,y2]</box>
    
    æ”¯æŒä¸¤ç§è§£ææ¨¡å¼ï¼š
    1. æ ‡å‡†æ ¼å¼ï¼š<obj>...</obj><box>[...]</box>
    2. åå¤‡æ ¼å¼ï¼šä»ä¹±ç ä¸­æå–åæ ‡ [x1,y1,x2,y2] å’Œå¯¹è±¡å
    """
    objects = []

    if "<no_layout>" in text:
        return objects

    def _normalize(text_in: str) -> str:
        # Normalize common full-width punctuation to improve parsing robustness.
        text_in = text_in.replace("ï¼Œ", ",").replace("ï¼›", ";")
        text_in = text_in.replace("ï¼»", "[").replace("ï¼½", "]")
        return text_in

    def _normalize_bbox_values(vals: List[float]) -> List[float]:
        max_v = max(vals)
        # Already normalized (0-1)
        if max_v <= 1.5:
            return vals
        # 0-100 scale
        if max_v <= 100:
            return [v / 100.0 for v in vals]
        # 0-1000 scale (allow mixed with normalized values)
        if max_v <= 1000:
            return [v / 1000.0 if v > 1.5 else v for v in vals]
        # Fallback: very large values, scale down large ones
        return [v / 1000.0 if v > 1.5 else v for v in vals]

    def _parse_bbox(bbox_str: str) -> Optional[List[float]]:
        # Extract floats to handle spaces, Chinese comma, or mixed separators.
        nums = re.findall(r"[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?", bbox_str)
        if len(nums) < 4:
            return None
        bbox = [float(x) for x in nums[:4]]
        bbox = _normalize_bbox_values(bbox)
        # Reorder if necessary and clamp to [0, 1]
        x1, y1, x2, y2 = bbox
        if x2 < x1:
            x1, x2 = x2, x1
        if y2 < y1:
            y1, y2 = y2, y1
        bbox = [max(0.0, min(1.0, v)) for v in (x1, y1, x2, y2)]
        return bbox

    text = _normalize(text)

    # æ¨¡å¼1ï¼šæ ‡å‡†æ ¼å¼è§£æï¼ˆå…è®¸ç©ºæ ¼/æ¢è¡Œï¼‰
    pattern = re.compile(r"<obj>\s*([^<]+?)\s*</obj>\s*<box>\s*\[([^\]]+)\]\s*</box>", re.S)
    matches = pattern.findall(text)

    if matches:  # å¦‚æœæ ‡å‡†æ ¼å¼è§£ææˆåŠŸï¼Œç›´æ¥è¿”å›
        for name, bbox_str in matches:
            try:
                bbox = _parse_bbox(bbox_str)
                if bbox is None or len(bbox) != 4:
                    continue
                objects.append({
                    "name": name.strip(),
                    "bbox": bbox
                })
            except Exception:
                continue
        return objects
    
    # æ¨¡å¼2ï¼šåå¤‡è§£æï¼ˆä»ä¹±ç ä¸­æå–åæ ‡å’Œå¯¹è±¡åï¼‰
    # æå–æ‰€æœ‰åæ ‡æ ¼å¼ï¼š[...]
    bbox_pattern = r'\[([^\]]+)\]'
    bbox_matches = re.findall(bbox_pattern, text)
    
    # æå–å¯èƒ½çš„å¯¹è±¡åï¼ˆä¸­æ–‡è¯æ±‡ï¼Œé•¿åº¦ 1-6ï¼‰
    # æ¸…ç†ç‰¹æ®Š token å’Œä¹±ç å­—ç¬¦
    cleaned_text = re.sub(r'</?tool_call>', '', text)
    cleaned_text = re.sub(r'<\|[^|]+\|>', '', cleaned_text)
    cleaned_text = re.sub(r'[a-zA-Z]{3,}', '', cleaned_text)  # ç§»é™¤é•¿è‹±æ–‡å•è¯ï¼ˆå¦‚ useRalï¼‰
    
    # æå–ä¸­æ–‡è¯æ±‡ä½œä¸ºå¯¹è±¡å
    chinese_words = re.findall(r'[\u4e00-\u9fff]+', cleaned_text)
    
    for i, bbox_str in enumerate(bbox_matches):
        try:
            bbox = _parse_bbox(bbox_str)
            if bbox is None or len(bbox) != 4:
                continue

            # è·å–å¯¹è±¡åï¼ˆä¼˜å…ˆä½¿ç”¨å¯¹åº”ä½ç½®çš„ä¸­æ–‡è¯ï¼Œå¦åˆ™ä½¿ç”¨"ç‰©ä½“"ï¼‰
            name = chinese_words[i] if i < len(chinese_words) else "ç‰©ä½“"

            objects.append({
                "name": name.strip(),
                "bbox": bbox
            })
        except Exception:
            continue
    
    return objects


def format_layout_input(
    tokenizer,
    prompt: str,
    enable_cot: bool = False,
    feedback: Optional[str] = None,
    strict_entities: bool = False,
) -> str:
    """
    ä½¿ç”¨ Tokenizer çš„ Chat Template æ ¼å¼åŒ–è¾“å…¥
    """
    if feedback:
        user_content = f"{prompt}\n\nä¸Šä¸€è½®åé¦ˆï¼š{feedback}\nè¯·æ ¹æ®åé¦ˆè°ƒæ•´å¸ƒå±€ã€‚"
    elif enable_cot:
        user_content = f"""{prompt}

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ€è€ƒå¹¶è§„åˆ’å¸ƒå±€ï¼š
1. é¦–å…ˆï¼Œåˆ†ææç¤ºè¯ä¸­çš„ç©ºé—´å…³ç³»ï¼ˆå¦‚"å·¦è¾¹"ã€"ä¸Šæ–¹"ç­‰ï¼‰
2. ç„¶åï¼Œç¡®å®šæ¯ä¸ªç‰©ä½“çš„ç›¸å¯¹ä½ç½®
3. æœ€åï¼Œè¾“å‡ºå¸ƒå±€åæ ‡"""
    else:
        user_content = prompt

    user_content = _append_format_instruction(user_content, strict_entities=strict_entities)

    messages = [{"role": "user", "content": user_content}]
    
    # ä½¿ç”¨ apply_chat_templateï¼Œå¹¶æ·»åŠ  generation prompt
    formatted_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    return formatted_text


class LayoutPlanner(nn.Module):
    def __init__(self, base_model_path: str, device: str = 'cuda', use_lora: bool = True):
        super().__init__()
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        # å¤„ç† device å‚æ•°
        self.device = device if isinstance(device, str) else "cuda"
        if self.device == "cuda": 
            self.device = "cuda:0"

        print(f"ğŸ“¦ åŠ è½½ Tokenizer: {base_model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        
        # æ·»åŠ å¸ƒå±€ç›¸å…³çš„ç‰¹æ®Š token
        special_tokens = {"additional_special_tokens": ["<obj>", "</obj>", "<box>", "</box>", "<no_layout>"]}
        num_added = self.tokenizer.add_special_tokens(special_tokens)
        if num_added > 0:
            print(f"âœ“ æ·»åŠ äº† {num_added} ä¸ªå¸ƒå±€ç‰¹æ®Š token")

        # åŠ è½½æ¨¡å‹
        print(f"ğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
        # ç®€å•å¤„ç† device_map
        device_map = "auto" if self.device == "auto" else self.device
        
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.bfloat16,
            device_map=device_map,
            trust_remote_code=True
        )
        
        if num_added > 0:
            self.model.resize_token_embeddings(len(self.tokenizer))
            
        self.use_lora = use_lora
        if use_lora:
            try:
                from peft import LoraConfig, get_peft_model
                peft_config = LoraConfig(
                    r=16,
                    lora_alpha=32,
                    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
                    lora_dropout=0.05,
                    bias="none",
                    task_type="CAUSAL_LM"
                )
                self.model = get_peft_model(self.model, peft_config)
                print("âœ“ ä½¿ç”¨ LoRA å¾®è°ƒ")
            except ImportError:
                print("âš ï¸ peft æœªå®‰è£…ï¼Œä½¿ç”¨å…¨é‡å¾®è°ƒ")
                self.use_lora = False
        
        self.model.eval()

    def forward(self, input_ids, labels=None, **kwargs):
        """Forward pass for training"""
        return self.model(input_ids=input_ids, labels=labels, **kwargs)

    def generate_layout(self, prompt: str, max_length: int = 512,
                       temperature: float = 0.2, top_p: float = 0.9,
                       apply_refinement: bool = True, enable_cot: bool = False,
                       feedback: Optional[str] = None,
                       strict_entities: bool = False) -> Dict:
        """
        ç”Ÿæˆå¸ƒå±€è§„åˆ’
        
        Args:
            prompt: è¾“å…¥æ–‡æœ¬ï¼ˆå¦‚"ç”»ä¸€åªåœ¨æ¡Œå­å·¦è¾¹çš„çŒ«"ï¼‰
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
            top_p: nucleus sampling å‚æ•°
            apply_refinement: æ˜¯å¦åº”ç”¨å¯å‘å¼ä¿®æ­£
            enable_cot: æ˜¯å¦å¯ç”¨ Chain-of-Thoughtï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
            feedback: ä¸Šä¸€è½®çš„åé¦ˆï¼ˆç”¨äºä¿®æ­£ï¼‰
        
        Returns:
            {
                "layout_text": "<obj>...</obj><box>...</box>",
                "objects": [{"name": "...", "bbox": [...]}]
            }
        """
        self.model.eval()
        
        # 1. æ ¼å¼åŒ–è¾“å…¥ (ä½¿ç”¨ Chat Template)
        formatted_input = format_layout_input(
            self.tokenizer, prompt, enable_cot=enable_cot, feedback=feedback, strict_entities=strict_entities
        )
        
        # 2. Tokenize
        inputs = self.tokenizer(
            formatted_input, return_tensors="pt"
        ).to(self.model.device)
        
        # 3. ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,  # å»ºè®®å¼€å¯é‡‡æ ·ä»¥é¿å…æ­»å¾ªç¯
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        # 4. è§£ç  (åªä¿ç•™æ–°ç”Ÿæˆçš„éƒ¨åˆ†)
        input_len = inputs.input_ids.shape[1]
        generated_part = generated_ids[0][input_len:]
        layout_text = self.tokenizer.decode(generated_part, skip_special_tokens=False)
        
        # 5. æ¸…ç† (Qwen/DeepSeek ç‰¹æ®Š token)
        layout_text = layout_text.strip()
        for token in ["<|im_end|>", "<|endoftext|>", "<|im_start|>"]:
            layout_text = layout_text.replace(token, "").strip()
            
        # 6. è§£æ
        objects = parse_layout_output(layout_text)
        
        if apply_refinement:
            objects = refine_layout_with_caption(prompt, objects)
            
        return {
            "layout_text": layout_text,
            "objects": objects
        }
    
    def batch_generate(self, prompts: List[str], **kwargs) -> List[Dict]:
        """æ‰¹é‡ç”Ÿæˆå¸ƒå±€"""
        results = []
        for prompt in prompts:
            result = self.generate_layout(prompt, **kwargs)
            results.append(result)
        return results


def create_layout_planner_from_gill(gill_model, tokenizer, use_lora: bool = True) -> LayoutPlanner:
    """
    ä» GILL æ¨¡å‹åˆ›å»ºå¸ƒå±€è§„åˆ’å™¨
    
    ä½¿ç”¨ GILL ä¸­çš„ LLM ä½œä¸ºåŸºç¡€æ¨¡å‹
    """
    base_lm = gill_model.model.lm
    
    # è·å–æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼‰
    if hasattr(base_lm, 'config') and hasattr(base_lm.config, '_name_or_path'):
        model_path = base_lm.config._name_or_path
    else:
        # é»˜è®¤è·¯å¾„
        model_path = "./model/Qwen2.5-7B-Instruct"
    
    planner = LayoutPlanner(model_path, device=base_lm.device, use_lora=use_lora)
    
    return planner


def refine_layout_with_caption(caption: str, objects: List[Dict]) -> List[Dict]:
    """
    åå¤„ç†ï¼šæ›´å¼ºåœ°ä¾èµ– caption çš„ä¸­æ–‡ä½ç½®è¯å’Œåè¯ï¼Œ
    åœ¨æ£€æµ‹åˆ°æ˜æ˜¾çš„ã€Œå·¦/å³/ä¸Š/ä¸‹/ä¸­é—´/å·¦ä¸‹è§’/å³ä¸‹è§’ã€ç»“æ„æ—¶ï¼Œ
    ç›´æ¥ç”¨å¯å‘å¼è§„åˆ™é‡å»ºä¸€ä¸ªå°çš„ objects åˆ—è¡¨ï¼Œå¿½ç•¥ LM åŸå§‹ objectsã€‚
    """
    if not objects:
        objects = []

    text = str(caption)

    # å¦‚æœ caption é‡Œæ²¡æœ‰ä»»ä½•ä½ç½®è¯ï¼Œå°±ç›´æ¥è¿”å›åŸå§‹ objectsï¼ˆåªåšåå­—æ¸…æ´—ï¼‰
    position_keywords = ["å·¦è¾¹", "å·¦ä¾§", "å·¦æ–¹", "å³è¾¹", "å³ä¾§", "å³æ–¹",
                         "ä¸Šæ–¹", "ä¸Šè¾¹", "ä¸Šé¢", "ä¸‹æ–¹", "ä¸‹è¾¹", "ä¸‹é¢",
                         "ä¸­é—´", "ä¸­å¤®", "ä¸­å¿ƒ", "å·¦ä¸‹è§’", "å³ä¸‹è§’"]
    has_position = any(k in text for k in position_keywords)

    # ç®€å•åè¯æŠ½å–ï¼šä¼˜å…ˆç”¨ jieba.possegï¼Œä¸è¡Œå°±ç”¨ä¸€ä¸ªå¾ˆç²—ç³™çš„å¤‡é€‰æ–¹æ¡ˆ
    nouns: List[str] = []
    try:
        import jieba.posseg as pseg  # type: ignore

        words = pseg.cut(text)
        for w, flag in words:
            if flag.startswith("n"):  # åè¯
                w = w.strip()
                if w and w not in nouns:
                    nouns.append(w)
    except Exception:
        # ç®€å•å…œåº•ï¼šæŒ‰å¸¸è§åˆ†éš”ç¬¦åˆ‡åˆ†ï¼Œå–é•¿åº¦ 1~4 çš„çŸ­ç‰‡æ®µ
        rough_parts = []
        for seg in re.split(r"[ï¼Œã€‚ã€â€œâ€ï¼!ï¼Ÿ?\s]", text):
            seg = seg.strip()
            if 1 <= len(seg) <= 4:
                rough_parts.append(seg)
        nouns = list(dict.fromkeys(rough_parts))  # å»é‡ä¸”ä¿æŒé¡ºåº

    # åªä¿ç•™å‰ä¸¤ä¸ªåè¯ï¼Œåˆ†åˆ«å½“ä½œã€Œä¸»/å‰¯ã€å¯¹è±¡
    if nouns:
        main_name = nouns[0]
        second_name = nouns[1] if len(nouns) > 1 else None
    else:
        main_name = objects[0].get("name", "ç‰©ä½“") if objects else "ç‰©ä½“"
        second_name = objects[1].get("name", None) if len(objects) > 1 else None

    # é¢„å®šä¹‰æ§½ä½
    slots = {
        "left": [0.0, 0.1, 0.4, 0.9],
        "right": [0.6, 0.1, 1.0, 0.9],
        "top": [0.1, 0.0, 0.9, 0.4],
        "bottom": [0.1, 0.6, 0.9, 1.0],
        "center": [0.3, 0.3, 0.7, 0.7],
        "bottom_left": [0.0, 0.6, 0.4, 1.0],
        "bottom_right": [0.6, 0.6, 1.0, 1.0],
    }

    # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„ä½ç½®è¯ï¼Œå°±åªåšåå­—æ¸…æ´—ï¼Œä¿æŒåŸç»“æœ
    if not has_position:
        for obj in objects:
            name = str(obj.get("name", "")).strip()
            if name.startswith(("æ˜¯", "åœ¨", "æœ‰")) and len(name) > 1:
                name = name[1:]
            obj["name"] = name
        return objects

    # æœ‰æ˜æ˜¾ä½ç½®è¯æ—¶ï¼šç›´æ¥æ ¹æ® caption é‡å»º objects åˆ—è¡¨ï¼ˆæœ€å¤šä¸¤ä¸ªå¯¹è±¡ï¼‰
    new_objects: List[Dict] = []

    def add_obj(name: str, slot_key: str):
        name = name.strip() or "ç‰©ä½“"
        # å»æ‰å‰å¯¼è™šè¯
        if name.startswith(("æ˜¯", "åœ¨", "æœ‰")) and len(name) > 1:
            name = name[1:]
        bbox = slots[slot_key]
        new_objects.append({"name": name, "bbox": bbox})

    # é»˜è®¤æ§½ä½
    main_slot = "center"
    second_slot = "center"

    # å·¦ / å³
    if any(k in text for k in ["å·¦è¾¹", "å·¦ä¾§", "å·¦æ–¹"]):
        main_slot = "left"
    if any(k in text for k in ["å³è¾¹", "å³ä¾§", "å³æ–¹"]):
        second_slot = "right"

    # ä¸Š / ä¸‹
    if any(k in text for k in ["ä¸Šæ–¹", "ä¸Šè¾¹", "ä¸Šé¢"]):
        main_slot = "top"
    if any(k in text for k in ["ä¸‹æ–¹", "ä¸‹è¾¹", "ä¸‹é¢"]):
        second_slot = "bottom"

    # ä¸­é—´ / ä¸­å¤®
    if any(k in text for k in ["ä¸­é—´", "ä¸­å¤®", "ä¸­å¿ƒ"]):
        main_slot = "center"

    # å·¦ä¸‹è§’ / å³ä¸‹è§’ ä¼˜å…ˆçº§æ›´é«˜ï¼Œè¦†ç›–å‰é¢çš„ bottom / left/right
    if "å·¦ä¸‹è§’" in text:
        second_slot = "bottom_left"
    if "å³ä¸‹è§’" in text:
        second_slot = "bottom_right"

    # ä¸»å¯¹è±¡
    add_obj(main_name, main_slot)
    # å‰¯å¯¹è±¡ï¼ˆå¦‚æœæœ‰ï¼‰
    if second_name is not None:
        add_obj(second_name, second_slot)

    return new_objects
