"""Modified from https://github.com/mlfoundations/open_clip"""

from typing import Optional, Tuple, List

import collections
import logging
import os
import random
import numpy as np
import pandas as pd
import torch
import torchvision.datasets as datasets
from torchvision import transforms as T
from PIL import Image, ImageFont
from torch.utils.data import Dataset

from gill import utils


# ============================================
# ä¸­æ–‡åŠ¨æ€æç¤ºæ¨¡æ¿æ± ï¼ˆé€‚é… DeepSeek-Base ç»­å†™é£æ ¼ï¼‰
# ============================================
CHINESE_CAPTION_PROMPTS = [
    "è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†",
    "å›¾ä¸­æç»˜äº†",
    "å¯ä»¥çœ‹åˆ°",
    "ç”»é¢å†…å®¹ä¸º",
    "å›¾ç‰‡å†…å®¹æ˜¯",
    "è¯¥å›¾åƒå‘ˆç°äº†",
    "æ‰€å±•ç¤ºçš„åœºæ™¯æ˜¯",
    "ä»å›¾ä¸­å¯ä»¥è§‚å¯Ÿåˆ°",
    "è¿™æ˜¯ä¸€å¼ å…³äº",
    "å›¾ç‰‡ä¸­æ˜¾ç¤ºçš„æ˜¯",
]

# ä¸­æ–‡å›¾åƒç”Ÿæˆæç¤ºæ¨¡æ¿ï¼ˆç”¨äºè®­ç»ƒæ¨¡å‹è¾“å‡º [IMG] tokenï¼‰
CHINESE_GENERATION_PROMPTS = [
    "è¯·ç”Ÿæˆä¸€å¼ å›¾ç‰‡",
    "ç”Ÿæˆå›¾åƒ",
    "åˆ›å»ºä¸€å¼ å›¾",
    "ç”»ä¸€å¼ ",
    "ç”Ÿæˆä¸€å¹…",
]

# Caption æœ€å¤§é•¿åº¦ï¼ˆä¸­æ–‡å­—ç¬¦æ•°ï¼‰
MAX_CAPTION_CHARS = 30

# Increase PIL's decompression bomb limit to handle large images
Image.MAX_IMAGE_PIXELS = None  # Disable the limit entirely (or set to a large value like 1000000000)

# ============================================
# Kolors Text Encoder æ”¯æŒ
# ============================================
KOLORS_TEXT_ENCODER = None
KOLORS_TOKENIZER = None


def load_kolors_text_encoder(kolors_path: str, device: str = 'cuda'):
    """
    åŠ è½½ Kolors çš„ text encoder ç”¨äºç”Ÿæˆè®­ç»ƒæ ‡ç­¾ã€‚
    
    Args:
        kolors_path: Kolors æ¨¡å‹è·¯å¾„
        device: è®¾å¤‡
    
    Returns:
        (tokenizer, text_encoder)
    """
    global KOLORS_TEXT_ENCODER, KOLORS_TOKENIZER
    
    if KOLORS_TEXT_ENCODER is not None:
        return KOLORS_TOKENIZER, KOLORS_TEXT_ENCODER
    
    from transformers import AutoTokenizer, AutoModel
    
    text_encoder_path = os.path.join(kolors_path, 'text_encoder')
    
    print(f"åŠ è½½ Kolors text encoder: {text_encoder_path}")
    
    # æ·»åŠ  text_encoder è·¯å¾„åˆ° sys.path ä»¥æ”¯æŒ ChatGLM è‡ªå®šä¹‰æ¨¡å—
    import sys
    if text_encoder_path not in sys.path:
        sys.path.insert(0, text_encoder_path)
    
    KOLORS_TOKENIZER = AutoTokenizer.from_pretrained(
        text_encoder_path, 
        trust_remote_code=True
    )
    KOLORS_TEXT_ENCODER = AutoModel.from_pretrained(
        text_encoder_path,
        trust_remote_code=True,
        torch_dtype=torch.float16
    ).to(device).eval()
    
    print(f"âœ“ Kolors text encoder åŠ è½½æˆåŠŸ")
    return KOLORS_TOKENIZER, KOLORS_TEXT_ENCODER


def encode_text_with_kolors(text: str, tokenizer, text_encoder, 
                            max_length: int = 256, device: str = 'cuda'):
    """
    ä½¿ç”¨ Kolors text encoder ç¼–ç æ–‡æœ¬ã€‚
    
    å‚è€ƒ Kolors å®˜æ–¹ pipeline çš„ encode_prompt æ–¹æ³•ï¼š
    - ä½¿ç”¨ hidden_states[-2]ï¼ˆå€’æ•°ç¬¬äºŒå±‚ï¼‰è€Œé last_hidden_state
    - pooled embedding ä½¿ç”¨ç¬¬ä¸€ä¸ª token ç»è¿‡æŠ•å½±
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        tokenizer: Kolors tokenizer
        text_encoder: Kolors text encoder
        max_length: æœ€å¤§åºåˆ—é•¿åº¦ (Kolors ä½¿ç”¨ 256)
        device: è®¾å¤‡
    
    Returns:
        (prompt_embeds, pooled_prompt_embeds): 
            prompt_embeds: (1, max_length, 2048)
            pooled_prompt_embeds: (1, 2048)
    """
    with torch.no_grad():
        # ChatGLM tokenizer å…¼å®¹æ€§å¤„ç†ï¼šä½¿ç”¨ tokenize + convert_tokens_to_ids
        # é¿å… tokenizer.encode() å†…éƒ¨è°ƒç”¨ _pad() æ—¶çš„ padding_side å‚æ•°é—®é¢˜
        try:
            # æ–¹æ³• 1: ä½¿ç”¨ tokenize + convert_tokens_to_idsï¼ˆæœ€å®‰å…¨ï¼‰
            text_tokens = tokenizer.tokenize(str(text))
            tokens = tokenizer.convert_tokens_to_ids(text_tokens)
            
            # æ·»åŠ ç‰¹æ®Š tokenï¼ˆå¦‚æœéœ€è¦ï¼‰
            if hasattr(tokenizer, 'bos_token_id') and tokenizer.bos_token_id is not None:
                tokens = [tokenizer.bos_token_id] + tokens
            if hasattr(tokenizer, 'eos_token_id') and tokenizer.eos_token_id is not None:
                tokens = tokens + [tokenizer.eos_token_id]
        except Exception as e:
            # å›é€€æ–¹æ¡ˆï¼šå®Œå…¨é¿å…ä½¿ç”¨ encode()ï¼Œæ‰‹åŠ¨æ„å»º token IDs
            # ChatGLM tokenizer çš„ encode() å†…éƒ¨ä¼šè°ƒç”¨ _pad() å¹¶ä¼ é€’ padding_sideï¼Œå¯¼è‡´é”™è¯¯
            try:
                # æ–¹æ³• 2: ä½¿ç”¨ tokenizer çš„ __call__ æ–¹æ³•ï¼Œä½†ç¦ç”¨æ‰€æœ‰å¯èƒ½å¯¼è‡´ padding çš„æ“ä½œ
                # ä¸´æ—¶è®¾ç½® padding_side ä¸º Noneï¼ˆå¦‚æœå¯èƒ½ï¼‰
                original_padding_side = getattr(tokenizer, 'padding_side', None)
                try:
                    if hasattr(tokenizer, 'padding_side'):
                        tokenizer.padding_side = 'right'  # è®¾ç½®ä¸ºæœ‰æ•ˆå€¼
                    
                    # ä½¿ç”¨ __call__ ä½†ç¦ç”¨ padding
                    inputs = tokenizer(str(text), add_special_tokens=True, padding=False, return_tensors=None, truncation=False)
                    
                    # æ¢å¤åŸå§‹ padding_side
                    if hasattr(tokenizer, 'padding_side') and original_padding_side is not None:
                        tokenizer.padding_side = original_padding_side
                    
                    # æå– token IDs
                    if isinstance(inputs, list):
                        tokens = inputs
                    elif isinstance(inputs, dict):
                        tokens = inputs.get('input_ids', [])
                        if not isinstance(tokens, list):
                            tokens = tokens.tolist() if hasattr(tokens, 'tolist') else [tokens]
                    else:
                        tokens = []
                except Exception as e2_inner:
                    # æ¢å¤ padding_side
                    if hasattr(tokenizer, 'padding_side') and original_padding_side is not None:
                        tokenizer.padding_side = original_padding_side
                    raise e2_inner
            except Exception as e2:
                # æœ€åçš„å›é€€ï¼šä½¿ç”¨ pad_token_id
                pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
                tokens = [pad_token_id]
        
        # æ‰‹åŠ¨æˆªæ–­å’Œ padding
        if len(tokens) > max_length:
            tokens = tokens[:max_length]
        else:
            pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
            tokens = tokens + [pad_token_id] * (max_length - len(tokens))
        
        input_ids = torch.tensor([tokens], device=device)
        attention_mask = (input_ids != (tokenizer.pad_token_id or 0)).long()
        
        outputs = text_encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        # ========================================
        # å…³é”®ï¼šä½¿ç”¨ hidden_states[-2]ï¼ˆå€’æ•°ç¬¬äºŒå±‚ï¼‰
        # è¿™ä¸ Kolors å®˜æ–¹ pipeline ä¸€è‡´
        # ========================================
        if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
            # ä½¿ç”¨å€’æ•°ç¬¬äºŒå±‚ï¼ˆå®˜æ–¹æ–¹å¼ï¼‰
            hidden_states = outputs.hidden_states[-2]
        elif hasattr(outputs, 'last_hidden_state'):
            # å›é€€åˆ°æœ€åä¸€å±‚
            hidden_states = outputs.last_hidden_state
        else:
            # å°è¯•ç›´æ¥ä½¿ç”¨è¾“å‡º
            hidden_states = outputs[0] if isinstance(outputs, tuple) else outputs
        
        # ChatGLM è¾“å‡ºå½¢çŠ¶æ˜¯ (seq_len, batch, hidden_dim)ï¼Œéœ€è¦è½¬ç½®ä¸º (batch, seq_len, hidden_dim)
        if hidden_states.dim() == 3 and hidden_states.shape[1] == 1:
            # å½¢çŠ¶æ˜¯ (seq_len, 1, hidden_dim)ï¼Œè½¬ç½®ä¸º (1, seq_len, hidden_dim)
            hidden_states = hidden_states.transpose(0, 1)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æŠ•å½±ï¼ˆChatGLM è¾“å‡ºæ˜¯ 4096 ç»´ï¼Œéœ€è¦æŠ•å½±åˆ° 2048ï¼‰
        hidden_dim = hidden_states.shape[-1]
        if hidden_dim == 4096:
            # æ£€æŸ¥æ˜¯å¦æœ‰ text_projection å±‚
            if hasattr(text_encoder, 'text_projection'):
                prompt_embeds = text_encoder.text_projection(hidden_states)
            else:
                # å¦‚æœæ²¡æœ‰æŠ•å½±å±‚ï¼Œä½¿ç”¨å‰ 2048 ç»´ï¼ˆå¯èƒ½ä¸æ­£ç¡®ï¼Œä½†ä¿è¯ç»´åº¦åŒ¹é…ï¼‰
                # æ³¨æ„ï¼šè¿™ç§æƒ…å†µåº”è¯¥åœ¨éªŒè¯è„šæœ¬ä¸­æ£€æµ‹åˆ°
                prompt_embeds = hidden_states[..., :2048]
        else:
            prompt_embeds = hidden_states
        
        # Pad åˆ° max_length
        seq_len = prompt_embeds.shape[1]
        if seq_len < max_length:
            pad_size = max_length - seq_len
            prompt_embeds = torch.nn.functional.pad(prompt_embeds, (0, 0, 0, pad_size), value=0)
        elif seq_len > max_length:
            prompt_embeds = prompt_embeds[:, :max_length, :]
        
        # Pooled embedding: ä½¿ç”¨ç¬¬ä¸€ä¸ª token çš„è¡¨ç¤º
        # è¿™ä¸ CLIP çš„ [CLS] token ç±»ä¼¼
        pooled_prompt_embeds = prompt_embeds[:, 0, :]  # (1, 2048)
        
        return prompt_embeds, pooled_prompt_embeds


def collate_fn(batch):
    batch = list(filter(lambda x: x is not None, batch))
    return torch.utils.data.dataloader.default_collate(batch)


def get_dataset(args, split: str, tokenizer, precision: str = 'fp32') -> Dataset:
  assert split in ['train', 'val'
    ], 'Expected split to be one of "train" or "val", got {split} instead.'

  # å¦‚æœ args æ²¡æœ‰ precision å±æ€§ï¼Œä½¿ç”¨å‚æ•°å€¼
  if not hasattr(args, 'precision'):
    args.precision = precision
  
  dataset_paths = []
  image_data_dirs = []
  train = split == 'train'
  
  # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Kolors ç›®æ ‡ embedding
  use_kolors_targets = getattr(args, 'use_kolors_targets', False)
  kolors_path = getattr(args, 'kolors_path', './model/Kolors')
  gen_emb_dim = getattr(args, 'gen_emb_dim', 768)

  # ä¸­æ–‡æç¤ºè¯æ•°æ®å¢å¼ºå‚æ•°ï¼ˆcaption å‰ç¼€ï¼‰
  prompt_aug_mode = getattr(args, 'prompt_aug_mode', 'random')
  prompt_aug_prob = float(getattr(args, 'prompt_aug_prob', 1.0))
  prompt_aug_sep = getattr(args, 'prompt_aug_sep', '')
  
  # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¸ƒå±€æ•°æ®
  layout_mode = getattr(args, 'layout_mode', False)
  layout_json_path = None
  if layout_mode:
    if split == 'train':
      layout_json_path = getattr(args, 'layout_train_json', None)
    else:
      layout_json_path = getattr(args, 'layout_val_json', None)

  # Default configs for datasets.
  # Folder structure should look like:
  if split == 'train':
    if 'cc3m' in args.dataset:
      dataset_paths.append(os.path.join(args.dataset_dir, 'cc3m_train.tsv'))
      image_data_dirs.append(os.path.join(args.image_dir, 'cc3m/training/'))
    elif 'wukong' in args.dataset:
      # ä¸­æ–‡WuKongæ•°æ®é›†ï¼ˆå›¾ç‰‡åœ¨dataset_dirä¸‹ï¼Œä¸æ˜¯image_dirï¼‰
      dataset_paths.append(os.path.join(args.dataset_dir, 'wukong_train.tsv'))
      image_data_dirs.append(os.path.join(args.dataset_dir, 'images/'))
    else:
      raise NotImplementedError

  elif split == 'val':
    if 'cc3m' in args.val_dataset:
      dataset_paths.append(os.path.join(args.dataset_dir, 'cc3m_val.tsv'))
      image_data_dirs.append(os.path.join(args.image_dir, 'cc3m/validation'))
    elif 'wukong' in args.val_dataset:
      # ä¸­æ–‡WuKongæ•°æ®é›†ï¼ˆå›¾ç‰‡åœ¨dataset_dirä¸‹ï¼Œä¸æ˜¯image_dirï¼‰
      dataset_paths.append(os.path.join(args.dataset_dir, 'wukong_val.tsv'))
      image_data_dirs.append(os.path.join(args.dataset_dir, 'images/'))
    else:
      raise NotImplementedError

    assert len(dataset_paths) == len(image_data_dirs) == 1, (dataset_paths, image_data_dirs)
  else:
    raise NotImplementedError

  if len(dataset_paths) > 1:
    print(f'{len(dataset_paths)} datasets requested: {dataset_paths}')
    dataset = torch.utils.data.ConcatDataset([
      CsvDataset(path, image_dir, tokenizer, 'image',
        'caption', args.visual_model, train=train, max_len=args.max_len, precision=args.precision,
        image_size=args.image_size, retrieval_token_idx=args.retrieval_token_idx, gen_token_idx=args.gen_token_idx, 
        num_tokens=args.num_tokens, num_clip_tokens=args.num_clip_tokens,
        use_kolors_targets=use_kolors_targets, kolors_path=kolors_path, gen_emb_dim=gen_emb_dim,
        layout_mode=layout_mode, layout_json_path=layout_json_path,
        prompt_aug_mode=prompt_aug_mode, prompt_aug_prob=prompt_aug_prob, prompt_aug_sep=prompt_aug_sep)
      for (path, image_dir) in zip(dataset_paths, image_data_dirs)])
  elif len(dataset_paths) == 1:
    # WuKongæ•°æ®é›†ä½¿ç”¨'image_path'è€Œä¸æ˜¯'image'
    dataset_name = args.dataset if isinstance(args.dataset, str) else args.dataset[0]
    img_key = 'image_path' if 'wukong' in dataset_name.lower() else 'image'
    dataset = CsvDataset(dataset_paths[0], image_data_dirs[0], tokenizer, img_key,
      'caption', args.visual_model, train=train, max_len=args.max_len, precision=args.precision,
      image_size=args.image_size, retrieval_token_idx=args.retrieval_token_idx, gen_token_idx=args.gen_token_idx, 
      num_tokens=args.num_tokens, num_clip_tokens=args.num_clip_tokens,
      use_kolors_targets=use_kolors_targets, kolors_path=kolors_path, gen_emb_dim=gen_emb_dim,
      layout_mode=layout_mode, layout_json_path=layout_json_path,
      prompt_aug_mode=prompt_aug_mode, prompt_aug_prob=prompt_aug_prob, prompt_aug_sep=prompt_aug_sep)
  else:
    raise ValueError(f'There should be at least one valid dataset, got train={args.dataset}, val={args.val_dataset} instead.')
  return dataset


class CsvDataset(Dataset):
  def __init__(self, input_filename, base_image_dir, tokenizer, img_key,
               caption_key, feature_extractor_model: str,
               train: bool = True, max_len: int = 32, sep="\t", precision: str = 'fp32',
               image_size: int = 224, retrieval_token_idx: List[int] = [-1], gen_token_idx: List[int] = [-1],
               num_tokens: int = 1, num_clip_tokens: int = 1,
               use_kolors_targets: bool = False, kolors_path: str = './model/Kolors',
               gen_emb_dim: int = 768, layout_mode: bool = False, layout_json_path: Optional[str] = None,
               prompt_aug_mode: str = 'random', prompt_aug_prob: float = 1.0, prompt_aug_sep: str = ''):
    logging.debug(f'Loading tsv data from {input_filename}.')
    df = pd.read_csv(input_filename, sep=sep)

    self.base_image_dir = base_image_dir
    self.images = df[img_key].tolist()
    self.captions = df[caption_key].tolist()

    # ç¡®ä¿captionæ˜¯å­—ç¬¦ä¸²
    processed_captions = []
    for caption in self.captions:
      if isinstance(caption, list):
        # å¦‚æœcaptionæ˜¯åˆ—è¡¨ï¼Œåªå–ç¬¬ä¸€ä¸ªæè¿°
        caption = caption[0] if len(caption) > 0 else ""
      # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
      caption = str(caption).strip()
      processed_captions.append(caption)
    self.captions = processed_captions
    assert len(self.images) == len(self.captions)

    self.feature_extractor_model = feature_extractor_model
    self.feature_extractor = utils.get_feature_extractor_for_model(
      feature_extractor_model, image_size=image_size, train=False)
    self.image_size = image_size

    self.tokenizer = tokenizer
    self.max_len = max_len
    self.precision = precision
    self.retrieval_token_idx = retrieval_token_idx
    self.gen_token_idx = gen_token_idx
    self.num_tokens = num_tokens
    self.num_clip_tokens = num_clip_tokens
    self.train = train  # ä¿å­˜è®­ç»ƒ/éªŒè¯æ¨¡å¼

    # ä¸­æ–‡æç¤ºè¯æ•°æ®å¢å¼ºï¼ˆcaption å‰ç¼€ï¼‰
    self.prompt_aug_mode = prompt_aug_mode
    self.prompt_aug_prob = float(prompt_aug_prob)
    self.prompt_aug_sep = prompt_aug_sep if prompt_aug_sep is not None else ""
    
    # Kolors ç›®æ ‡ embedding æ”¯æŒ
    self.use_kolors_targets = use_kolors_targets
    self.kolors_path = kolors_path
    self.gen_emb_dim = gen_emb_dim
    self.kolors_tokenizer = None
    self.kolors_text_encoder = None
    
    if use_kolors_targets:
      print(f"ğŸ“¦ å¯ç”¨ Kolors ç›®æ ‡ embedding (dim={gen_emb_dim}, seq_len={num_clip_tokens})")
      # å»¶è¿ŸåŠ è½½ Kolors text encoderï¼ˆåœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶åŠ è½½ï¼‰

    # å¸ƒå±€æ•°æ®æ”¯æŒ
    self.layout_mode = layout_mode
    self.layout_data = None
    if layout_mode and layout_json_path and os.path.exists(layout_json_path):
      import json
      with open(layout_json_path, 'r', encoding='utf-8') as f:
        layout_data_list = json.load(f)
      # æ„å»º image_id -> layout_data çš„æ˜ å°„
      self.layout_data = {item["image_path"]: item for item in layout_data_list}
      print(f"ğŸ“ å¯ç”¨å¸ƒå±€æ¨¡å¼ï¼ŒåŠ è½½äº† {len(self.layout_data)} æ¡å¸ƒå±€æ•°æ®")
    elif layout_mode:
      print(f"âš ï¸ å¸ƒå±€æ¨¡å¼å·²å¯ç”¨ï¼Œä½†æœªæ‰¾åˆ°å¸ƒå±€æ•°æ®æ–‡ä»¶: {layout_json_path}")

    self.font = None

    logging.debug('Done loading data.')

  def __len__(self):
    return len(self.captions)

  def _truncate_chinese_caption(self, caption: str, max_chars: int) -> str:
    """æˆªæ–­ä¸­æ–‡ caption åˆ°æŒ‡å®šå­—ç¬¦æ•°ï¼Œä¿ç•™å®Œæ•´è¯­ä¹‰"""
    # ç§»é™¤é¦–å°¾ç©ºç™½
    caption = caption.strip()
    
    # å¦‚æœå·²ç»è¶³å¤ŸçŸ­ï¼Œç›´æ¥è¿”å›
    if len(caption) <= max_chars:
      return caption
    
    # å°è¯•åœ¨æ ‡ç‚¹å¤„æˆªæ–­ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´
    punctuations = ['ã€‚', 'ï¼Œ', 'ã€', 'ï¼›', 'ï¼', 'ï¼Ÿ', '.', ',', ';', '!', '?']
    truncated = caption[:max_chars]
    
    # ä»åå‘å‰æ‰¾æœ€è¿‘çš„æ ‡ç‚¹
    for i in range(len(truncated) - 1, max(0, len(truncated) - 10), -1):
      if truncated[i] in punctuations:
        return truncated[:i]  # ä¸åŒ…å«æ ‡ç‚¹
    
    # æ²¡æ‰¾åˆ°æ ‡ç‚¹ï¼Œç›´æ¥æˆªæ–­
    return truncated

  def _get_kolors_embedding_from_file(self, image_id: str):
    """
    ä»é¢„è®¡ç®—çš„æ–‡ä»¶åŠ è½½ Kolors embeddingï¼ˆæ¨èæ–¹å¼ï¼Œé€Ÿåº¦å¿« 3x+ï¼‰ã€‚
    
    Args:
        image_id: å›¾åƒæ–‡ä»¶å
    
    Returns:
        clip_emb: (num_clip_tokens, gen_emb_dim) ç›®æ ‡ embedding
    """
    # æ„å»º Kolors embedding è·¯å¾„
    kolors_emb_path = os.path.join(self.base_image_dir, 'kolors_embs', f'{image_id}.npy')
    
    if os.path.exists(kolors_emb_path):
      with open(kolors_emb_path, 'rb') as f:
        clip_emb = np.load(f)  # (256, 2048)
      return clip_emb
    else:
      return None

  def _get_kolors_embedding_realtime(self, caption: str):
    """
    ä½¿ç”¨ Kolors text encoder å®æ—¶è·å–ç›®æ ‡ embeddingï¼ˆå¤‡ç”¨æ–¹å¼ï¼Œè¾ƒæ…¢ï¼‰ã€‚
    
    Args:
        caption: è¾“å…¥æ–‡æœ¬
    
    Returns:
        clip_emb: (num_clip_tokens, gen_emb_dim) ç›®æ ‡ embedding
    """
    # å»¶è¿ŸåŠ è½½ Kolors text encoder
    if self.kolors_tokenizer is None:
      self.kolors_tokenizer, self.kolors_text_encoder = load_kolors_text_encoder(
        self.kolors_path, device='cuda' if torch.cuda.is_available() else 'cpu'
      )
    
    # ç¼–ç æ–‡æœ¬
    prompt_embeds, pooled_embeds = encode_text_with_kolors(
      caption, 
      self.kolors_tokenizer, 
      self.kolors_text_encoder,
      max_length=self.num_clip_tokens,
      device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # è¿”å› numpy æ ¼å¼ (num_clip_tokens, gen_emb_dim)
    return prompt_embeds.squeeze(0).cpu().numpy()

  def __getitem__(self, idx):
    max_retries = 10
    retry_count = 0
    original_idx = idx
    
    while retry_count < max_retries:
      image_path = os.path.join(self.base_image_dir, str(self.images[idx]))
      caption = str(self.captions[idx])
      clip_l_path = os.path.join(self.base_image_dir, 'clip_embs', str(self.images[idx]) + '.npy')

      try:
        img = Image.open(image_path)
        images = utils.get_pixel_values_for_model(self.feature_extractor, img)

        # ============================================
        # è·å–ç›®æ ‡ embedding (CLIP æˆ– Kolors)
        # ============================================
        image_id = str(self.images[idx])
        
        if self.use_kolors_targets:
          # ä¼˜å…ˆä»é¢„è®¡ç®—æ–‡ä»¶åŠ è½½ Kolors embeddingï¼ˆé€Ÿåº¦å¿« 3x+ï¼‰
          clip_emb = self._get_kolors_embedding_from_file(image_id)
          
          if clip_emb is None:
            # å›é€€åˆ°å®æ—¶ç¼–ç ï¼ˆè¾ƒæ…¢ï¼Œä½†ä¿è¯å¯ç”¨ï¼‰
            clip_emb = self._get_kolors_embedding_realtime(caption)
          
          # éªŒè¯ç»´åº¦
          assert clip_emb.shape == (self.num_clip_tokens, self.gen_emb_dim), \
            f"Kolors embedding shape mismatch: {clip_emb.shape}, expected ({self.num_clip_tokens}, {self.gen_emb_dim})"
        else:
          # ä½¿ç”¨é¢„è®¡ç®—çš„ CLIP embedding
          with open(clip_l_path, 'rb') as f:
            clip_emb = np.load(f, allow_pickle=True)   # (num_clip_tokens, 768) or (768,)
            # å¤„ç†ä¸€ç»´embeddingï¼ˆpooled CLIP featuresï¼‰
            if clip_emb.ndim == 1:
              clip_emb = clip_emb.reshape(1, -1)  # (768,) -> (1, 768)
            clip_emb = clip_emb[:self.num_clip_tokens, :]

        # ============================================
        # ä¸­æ–‡ Caption é¢„å¤„ç†
        # ============================================
        # 1. æˆªæ–­ caption åˆ°æœ€å¤§é•¿åº¦ï¼ˆä¿ç•™æ ¸å¿ƒä¿¡æ¯ï¼‰
        caption = self._truncate_chinese_caption(caption, MAX_CAPTION_CHARS)
        
        # 2. åŠ¨æ€é€‰æ‹©æç¤ºæ¨¡æ¿ï¼ˆè®­ç»ƒæ—¶éšæœºï¼ŒéªŒè¯æ—¶å›ºå®šï¼‰
        prefix = ""
        mode = (self.prompt_aug_mode or "random").lower()
        if mode not in ["none", "fixed", "random"]:
          mode = "random"

        if mode != "none":
          if self.train:
            # è®­ç»ƒæ—¶æŒ‰æ¦‚ç‡åŠ å‰ç¼€
            if self.prompt_aug_prob >= 1.0 or random.random() < self.prompt_aug_prob:
              if mode == "fixed":
                prefix = CHINESE_CAPTION_PROMPTS[0]
              else:  # random
                prefix = random.choice(CHINESE_CAPTION_PROMPTS)
          else:
            # éªŒè¯æ—¶é»˜è®¤å›ºå®šå‰ç¼€ï¼ˆä¿è¯è¯„ä¼°ç¨³å®šï¼‰ï¼›å¦‚éœ€ç¦ç”¨è¯·ç”¨ --prompt-aug-mode none
            prefix = CHINESE_CAPTION_PROMPTS[0]

        # 3. æ‹¼æ¥å‰ç¼€å’Œ captionï¼ˆå¯é€‰åˆ†éš”ç¬¦ï¼‰
        if prefix:
          full_caption = f"{prefix}{self.prompt_aug_sep}{caption}"
        else:
          full_caption = caption
        
        # Generation mode: æ·»åŠ  [IMG] tokens
        for i in range(self.num_tokens):
          full_caption += f'[IMG{i}]'
        
        tokenized_data = self.tokenizer(
          full_caption,
          return_tensors="pt",
          padding='max_length',
          truncation=True,
          max_length=self.max_len)
        tokens = tokenized_data.input_ids[0]
        caption_len = tokenized_data.attention_mask[0].sum()

        # If IMG tokens are overridden by padding, replace them with the correct token.
        if tokens[-1] not in [self.tokenizer.pad_token_id, self.gen_token_idx[-1]]:
          tokens[-self.num_tokens:] = torch.tensor(self.gen_token_idx).to(dtype=tokens.dtype, device=tokens.device)

        decode_caption = self.tokenizer.decode(tokens, skip_special_tokens=False)
        self.font = self.font or ImageFont.load_default()
        # ç›´æ¥ä¼ å…¥ strï¼Œé¿å… .encode('ascii','ignore') ä¸¢å¤±ä¸­æ–‡
        cap_img = utils.create_image_of_text(decode_caption, width=self.image_size, nrows=2, font=self.font)

        # å¸ƒå±€æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        objects = None
        bboxes = None
        if self.layout_mode and self.layout_data is not None:
          layout_item = self.layout_data.get(image_id, None)
          if layout_item:
            objects = layout_item.get("objects", [])
            bboxes = [obj["bbox"] for obj in objects]
            # è½¬æ¢ä¸º tensor
            if bboxes:
              bboxes = torch.tensor(bboxes, dtype=torch.float32)  # (N, 4)

        # è¿”å›æ•°æ®ï¼ˆæ ¹æ®æ˜¯å¦å¯ç”¨å¸ƒå±€æ¨¡å¼ï¼‰
        if self.layout_mode and objects is not None:
          return image_path, images, cap_img, tokens, caption_len, tokens, caption_len, clip_emb, objects, bboxes
        else:
          return image_path, images, cap_img, tokens, caption_len, tokens, caption_len, clip_emb
      except Exception as e:
        retry_count += 1
        if retry_count >= max_retries:
          print(f'ERROR: Failed to load sample after {max_retries} retries. Original idx={original_idx}, last tried={idx}')
          print(f'Last error: {image_path} with caption {caption}: {e}')
          raise RuntimeError(f'Dataset loading failed after {max_retries} retries')
        if retry_count <= 3:  # Only print first few errors to avoid spam
          print(f'Warning: Error reading {image_path}: {e} (retry {retry_count}/{max_retries})')
        # Pick a new example at random.
        idx = np.random.randint(0, len(self)-1)
