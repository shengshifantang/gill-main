"""
åé¦ˆéªŒè¯æ¨¡å— (Feedback Verifier)

ğŸŒŸ å¼‚æ„éªŒè¯å™¨æ¶æ„ï¼ˆHeterogeneous Verifier Architectureï¼‰
é¿å…"è‡ªå¾ªç¯éªŒè¯"åå·®ï¼Œä½¿ç”¨å¤šç§éªŒè¯å™¨ç»„åˆï¼š
1. Grounding DINOï¼šæ£€æµ‹ä½ç½®å‡†ç¡®æ€§ï¼ˆNeuro-Symbolic Feedbackï¼‰
2. Qwen2-VL-7Bï¼šæ£€æµ‹è¯­ä¹‰å‡†ç¡®æ€§ï¼ˆè½»é‡çº§ VLMï¼‰
3. GPT-4o/Claudeï¼ˆå¯é€‰ï¼‰ï¼šç”¨äºè¯„ä¼°å®éªŒçš„é‡‘æ ‡å‡†

è®ºæ–‡å®£ç§°ï¼šMoE-based Self-Correctionï¼ˆä¸“å®¶æ··åˆæ¨¡å‹è‡ªæˆ‘ä¿®æ­£ï¼‰
"""

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple, Literal
from PIL import Image
import numpy as np


class FeedbackVerifier:
    """
    åé¦ˆéªŒè¯å™¨ï¼ˆå¼‚æ„æ¶æ„ï¼‰
    
    ä½¿ç”¨å¤šç§éªŒè¯å™¨ç»„åˆï¼Œé¿å…"è£åˆ¤å‘˜å…¼è¿åŠ¨å‘˜"åå·®
    """
    
    def __init__(self, 
                 verifier_type: Literal["grounding_dino", "qwen2vl_7b", "hybrid", "qwen2vl"] = "hybrid",
                 vlm_model_name: Optional[str] = None,
                 device: str = "cuda",
                 use_grounding: bool = True):
        """
        Args:
            verifier_type: éªŒè¯å™¨ç±»å‹
                - "grounding_dino": ä»…ä½¿ç”¨ Grounding DINOï¼ˆä½ç½®æ£€æµ‹ï¼‰
                - "qwen2vl_7b": ä»…ä½¿ç”¨ Qwen2-VL-7Bï¼ˆè¯­ä¹‰éªŒè¯ï¼‰
                - "hybrid": æ··åˆæ¨¡å¼ï¼ˆæ¨èï¼ŒGrounding DINO + Qwen2-VL-7Bï¼‰
                - "qwen2vl": ä½¿ç”¨æŒ‡å®šçš„ Qwen-VL æ¨¡å‹ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
            vlm_model_name: VLM æ¨¡å‹åç§°æˆ–è·¯å¾„ï¼ˆç”¨äº qwen2vl æ¨¡å¼ï¼‰
            device: è®¾å¤‡
            use_grounding: æ˜¯å¦ä½¿ç”¨ grounding åŠŸèƒ½ï¼ˆæ£€æµ‹å¯¹è±¡ä½ç½®ï¼‰
        """
        self.device = device
        self.use_grounding = use_grounding
        self.verifier_type = verifier_type
        
        # æ ¹æ®ç±»å‹åŠ è½½éªŒè¯å™¨
        if verifier_type == "hybrid":
            print("ğŸ”€ ä½¿ç”¨æ··åˆéªŒè¯å™¨ï¼ˆGrounding DINO + Qwen2-VL-7Bï¼‰")
            self._load_grounding_dino()
            self._load_qwen2vl_7b()
        elif verifier_type == "grounding_dino":
            print("ğŸ¯ ä½¿ç”¨ Grounding DINO éªŒè¯å™¨")
            self._load_grounding_dino()
            self.qwen_model = None
            self.qwen_processor = None
        elif verifier_type == "qwen2vl_7b":
            print("ğŸ¤– ä½¿ç”¨ Qwen2-VL-7B éªŒè¯å™¨")
            self._load_qwen2vl_7b()
            self.grounding_model = None
            self.grounding_processor = None
        else:  # qwen2vl (å…¼å®¹æ—§ä»£ç )
            self.vlm_model_name = vlm_model_name or "Qwen/Qwen2-VL-7B-Instruct"
        self._load_vlm_model()
    
    def _load_vlm_model(self):
        """åŠ è½½ VLM æ¨¡å‹"""
        try:
            from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
            
            print(f"ğŸ“¦ åŠ è½½ VLM æ¨¡å‹: {self.vlm_model_name}")
            
            # å°è¯•åŠ è½½ Qwen-VL
            if "qwen" in self.vlm_model_name.lower() or "Qwen" in self.vlm_model_name:
                try:
                    self.processor = AutoProcessor.from_pretrained(
                        self.vlm_model_name,
                        trust_remote_code=True
                    )
                    self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                        self.vlm_model_name,
                        torch_dtype=torch.bfloat16,
                        device_map=self.device,
                        trust_remote_code=True
                    )
                    self.model_type = "qwen"
                    print("âœ“ ä½¿ç”¨ Qwen-VL ä½œä¸ºéªŒè¯å™¨")
                except Exception as e:
                    print(f"âš ï¸ Qwen-VL åŠ è½½å¤±è´¥: {e}")
                    self._load_fallback_model()
            else:
                self._load_fallback_model()
                
        except ImportError:
            print("âš ï¸ transformers æœªå®‰è£…æˆ–ç‰ˆæœ¬ä¸æ”¯æŒï¼Œä½¿ç”¨è½»é‡çº§éªŒè¯å™¨")
            self._load_fallback_model()
    
    def _load_fallback_model(self):
        """åŠ è½½å¤‡ç”¨éªŒè¯å™¨ï¼ˆåŸºäº CLIPï¼‰"""
        try:
            from transformers import CLIPProcessor, CLIPModel
            
            print("ğŸ“¦ ä½¿ç”¨ CLIP ä½œä¸ºå¤‡ç”¨éªŒè¯å™¨")
            self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
            self.model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14").to(self.device)
            self.model_type = "clip"
            print("âœ“ ä½¿ç”¨ CLIP ä½œä¸ºéªŒè¯å™¨ï¼ˆåŠŸèƒ½å—é™ï¼‰")
        except Exception as e:
            print(f"âŒ å¤‡ç”¨éªŒè¯å™¨åŠ è½½å¤±è´¥: {e}")
            self.model = None
            self.processor = None
            self.model_type = None
    
    def verify(self, 
               image: Image.Image,
               original_prompt: str,
               expected_layout: Optional[List[Dict]] = None,
               threshold: float = 0.7) -> Dict:
        """
        éªŒè¯ç”Ÿæˆå›¾åƒæ˜¯å¦ç¬¦åˆè¦æ±‚
        
        Args:
            image: ç”Ÿæˆçš„å›¾åƒï¼ˆPIL Imageï¼‰
            original_prompt: åŸå§‹ prompt
            expected_layout: æœŸæœ›çš„å¸ƒå±€ä¿¡æ¯ [{"name": "...", "bbox": [...]}]
            threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        
        Returns:
            {
                "correct": bool,  # æ˜¯å¦é€šè¿‡éªŒè¯
                "confidence": float,  # ç½®ä¿¡åº¦ (0-1)
                "feedback": str,  # åé¦ˆä¿¡æ¯
                "suggested_prompt": str,  # ä¿®æ­£å»ºè®®
                "detected_objects": List[Dict],  # æ£€æµ‹åˆ°çš„å¯¹è±¡ï¼ˆå¦‚æœæ”¯æŒï¼‰
            }
        """
        if self.model is None:
            # å¦‚æœæ²¡æœ‰æ¨¡å‹ï¼Œè¿”å›é»˜è®¤ç»“æœ
            return {
                "correct": True,
                "confidence": 0.5,
                "feedback": "éªŒè¯å™¨æœªåŠ è½½ï¼Œè·³è¿‡éªŒè¯",
                "suggested_prompt": original_prompt,
                "detected_objects": []
            }
        
        self.model.eval()
        
        with torch.no_grad():
            if self.model_type == "qwen":
                return self._verify_with_qwen(image, original_prompt, expected_layout, threshold)
            elif self.model_type == "clip":
                return self._verify_with_clip(image, original_prompt, threshold)
            else:
                return {
                    "correct": True,
                    "confidence": 0.5,
                    "feedback": "æœªçŸ¥çš„éªŒè¯å™¨ç±»å‹",
                    "suggested_prompt": original_prompt,
                    "detected_objects": []
                }
    
    def _verify_with_qwen_internal(self,
                         image: Image.Image,
                         prompt: str,
                         expected_layout: Optional[List[Dict]],
                                  threshold: float,
                                  model,
                                  processor) -> Dict:
        """
        å†…éƒ¨æ–¹æ³•ï¼šä½¿ç”¨ Qwen-VL éªŒè¯ï¼ˆå¯è¢«ä¸åŒæ¨¡å‹è°ƒç”¨ï¼‰
        """
        """ä½¿ç”¨ Qwen2-VL éªŒè¯ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒæ¨ç†æ—¶ä¿®æ­£ï¼‰"""
        try:
            # ğŸŒŸ å¢å¼ºçš„éªŒè¯ promptï¼šChain-of-Thought é£æ ¼
            verify_prompt = f"""ä½ æ˜¯ä¸€åä¸¥æ ¼çš„è§†è§‰è´¨æ£€å‘˜ã€‚è¯·æ£€æŸ¥å›¾ç‰‡æ˜¯å¦ç¬¦åˆä»¥ä¸‹æè¿°ï¼š{prompt}

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤åˆ†æï¼š
1. é¦–å…ˆï¼Œè¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰ä¸»è¦ç‰©ä½“
2. ç„¶åï¼Œæ£€æŸ¥æ¯ä¸ªç‰©ä½“çš„ä½ç½®æ˜¯å¦ç¬¦åˆæè¿°ä¸­çš„ç©ºé—´å…³ç³»è¦æ±‚
3. æœ€åï¼Œç»™å‡ºæ˜ç¡®çš„åˆ¤æ–­å’Œä¿®æ­£å»ºè®®"""
            
            if expected_layout:
                verify_prompt += "\n\næœŸæœ›çš„å¸ƒå±€è¦æ±‚ï¼š\n"
                for obj in expected_layout:
                    bbox = obj.get('bbox', [])
                    # å°†å½’ä¸€åŒ–åæ ‡è½¬æ¢ä¸ºä½ç½®æè¿°
                    x1, y1, x2, y2 = bbox
                    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
                    position_desc = ""
                    if cx < 0.33:
                        position_desc += "å·¦ä¾§"
                    elif cx > 0.67:
                        position_desc += "å³ä¾§"
                    else:
                        position_desc += "ä¸­é—´"
                    if cy < 0.33:
                        position_desc += "ä¸Šæ–¹"
                    elif cy > 0.67:
                        position_desc += "ä¸‹æ–¹"
                    
                    verify_prompt += f"- {obj['name']} åº”è¯¥åœ¨ {position_desc} (åæ ‡: [{x1:.2f}, {y1:.2f}, {x2:.2f}, {y2:.2f}])\n"
            
            verify_prompt += """
\nè¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
- å¦‚æœå®Œå…¨ç¬¦åˆï¼šå›ç­”"ç¬¦åˆ"
- å¦‚æœä¸ç¬¦åˆï¼šå›ç­”"ä¸ç¬¦åˆ"ï¼Œå¹¶è¯¦ç»†è¯´æ˜ï¼š
  1. å“ªä¸ªç‰©ä½“çš„ä½ç½®ä¸å¯¹
  2. å½“å‰ä½ç½®åœ¨å“ªé‡Œ
  3. åº”è¯¥å¦‚ä½•è°ƒæ•´ï¼ˆä¾‹å¦‚ï¼š"çŒ«å½“å‰åœ¨ä¸­é—´ï¼Œåº”è¯¥å‘å·¦ç§»åŠ¨åˆ°å·¦ä¾§åŒºåŸŸ"ï¼‰"""
            
            # å¤„ç†è¾“å…¥
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": verify_prompt}
                    ]
                }
            ]
            
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            image_inputs, video_inputs = self.processor(
                text=[text],
                images=[image],
                padding=True,
                return_tensors="pt"
            ).to(self.device)
            
            # ç”ŸæˆéªŒè¯ç»“æœ
            generated_ids = self.model.generate(
                **image_inputs,
                max_new_tokens=128,
                do_sample=False
            )
            
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(image_inputs.input_ids, generated_ids)
            ]
            
            response_text = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]
            # ğŸŒŸ å¢å¼ºçš„å“åº”è§£æï¼šæå–ä¿®æ­£å»ºè®®ï¼ˆè‡ªç„¶è¯­è¨€ Rationaleï¼‰
            response_text = response_text.strip()
            
            # åˆ¤æ–­æ˜¯å¦ç¬¦åˆï¼ˆæ›´ä¸¥æ ¼çš„åˆ¤æ–­ï¼‰
            is_correct = (
                "ç¬¦åˆ" in response_text and 
                "ä¸ç¬¦åˆ" not in response_text and
                "ä¸å¯¹" not in response_text and
                "é”™è¯¯" not in response_text and
                "é—®é¢˜" not in response_text
            )
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºå“åº”ä¸­çš„å…³é”®è¯ï¼‰
            confidence = 0.9 if is_correct else 0.2
            if "å®Œå…¨" in response_text and is_correct:
                confidence = 0.95
            elif "åŸºæœ¬" in response_text and is_correct:
                confidence = 0.8

            # ğŸŒŸ å…³é”®ï¼šæå–è‡ªç„¶è¯­è¨€åé¦ˆï¼ˆRationaleï¼‰å’Œä¿®æ­£å»ºè®®
            refinement_instruction = None
            correction_details = []
            rationale = response_text  # å®Œæ•´çš„è‡ªç„¶è¯­è¨€åé¦ˆ
            
            if not is_correct:
                # æå–ä¿®æ­£å»ºè®®ï¼ˆç”¨äº Layout Planner ä¿®æ­£ï¼‰
                lines = response_text.split('\n')
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                    # æå–åŒ…å«ä¿®æ­£ä¿¡æ¯çš„è¡Œ
                    if any(keyword in line for keyword in ["åº”è¯¥", "éœ€è¦", "å»ºè®®", "è°ƒæ•´", "ç§»åŠ¨", "ä½ç½®", "åœ¨", "åº”è¯¥"]):
                        correction_details.append(line)
                
                # æ„å»ºç»“æ„åŒ–çš„ä¿®æ­£æŒ‡ä»¤
                if correction_details:
                    # åˆå¹¶æ‰€æœ‰ä¿®æ­£å»ºè®®
                    refinement_instruction = "\n".join(correction_details)
                else:
                    # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ä¿®æ­£å»ºè®®ï¼Œä½¿ç”¨åŸå§‹åé¦ˆ
                    refinement_instruction = response_text
                
                # ç¡®ä¿ refinement_instruction åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯
                if len(refinement_instruction) < 20:
                    refinement_instruction = f"éªŒè¯åé¦ˆï¼š{response_text}"
            else:
                # å³ä½¿é€šè¿‡éªŒè¯ï¼Œä¹Ÿä¿ç•™åé¦ˆä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—ï¼‰
                refinement_instruction = "éªŒè¯é€šè¿‡ï¼Œæ— éœ€ä¿®æ­£"

            return {
                "correct": is_correct and confidence >= threshold,
                "confidence": confidence,
                "feedback": response_text,  # åŸå§‹åé¦ˆæ–‡æœ¬
                "rationale": rationale,  # è‡ªç„¶è¯­è¨€è§£é‡Šï¼ˆç”¨äºè®ºæ–‡å±•ç¤ºï¼‰
                "refinement_instruction": refinement_instruction,  # ç”¨äºåé¦ˆç»™ Layout Planner
                "correction_details": correction_details,  # è¯¦ç»†çš„ä¿®æ­£å»ºè®®åˆ—è¡¨
                "suggested_prompt": prompt,
                "detected_objects": []
            }
            
        except Exception as e:
            print(f"âš ï¸ Qwen-VL éªŒè¯å‡ºé”™: {e}")
            return {
                "correct": True,  # å‡ºé”™æ—¶é»˜è®¤é€šè¿‡
                "confidence": 0.5,
                "feedback": f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}",
                "suggested_prompt": prompt,
                "detected_objects": []
            }
    
    def _verify_with_clip(self, 
                         image: Image.Image,
                         prompt: str,
                         threshold: float) -> Dict:
        """ä½¿ç”¨ CLIP éªŒè¯ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰"""
        try:
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=[prompt],
                images=[image],
                return_tensors="pt",
                padding=True
            ).to(self.device)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            outputs = self.model(**inputs)
            logits_per_image = outputs.logits_per_image
            probs = logits_per_image.softmax(dim=1)
            
            confidence = probs[0, 0].item()
            is_correct = confidence >= threshold
            
            return {
                "correct": is_correct,
                "confidence": confidence,
                "feedback": f"CLIPç›¸ä¼¼åº¦: {confidence:.3f}",
                "suggested_prompt": prompt,
                "detected_objects": []
            }
            
        except Exception as e:
            print(f"âš ï¸ CLIP éªŒè¯å‡ºé”™: {e}")
            return {
                "correct": True,
                "confidence": 0.5,
                "feedback": f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}",
                "suggested_prompt": prompt,
                "detected_objects": []
            }
    
    def batch_verify(self, 
                    images: List[Image.Image],
                    prompts: List[str],
                    expected_layouts: Optional[List[List[Dict]]] = None,
                    threshold: float = 0.7) -> List[Dict]:
        """æ‰¹é‡éªŒè¯"""
        results = []
        for i, (image, prompt) in enumerate(zip(images, prompts)):
            expected_layout = expected_layouts[i] if expected_layouts else None
            result = self.verify(image, prompt, expected_layout, threshold)
            results.append(result)
        return results


def create_feedback_verifier(vlm_model_name: str = "Qwen/Qwen-VL",
                             device: str = "cuda",
                             use_grounding: bool = True) -> FeedbackVerifier:
    """
    åˆ›å»ºåé¦ˆéªŒè¯å™¨
    
    Args:
        vlm_model_name: VLM æ¨¡å‹åç§°
        device: è®¾å¤‡
        use_grounding: æ˜¯å¦ä½¿ç”¨ grounding
    
    Returns:
        FeedbackVerifier å®ä¾‹
    """
    return FeedbackVerifier(
        vlm_model_name=vlm_model_name,
        device=device,
        use_grounding=use_grounding
    )

