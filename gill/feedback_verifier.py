"""
åé¦ˆéªŒè¯æ¨¡å— (Feedback Verifier) - å¼‚æ„æ··åˆæ¶æ„ç‰ˆ (Hybrid MoE)

æ¶æ„è®¾è®¡ï¼š
1. Spatial Expert (Grounding DINO): è´Ÿè´£ç²¾ç¡®çš„åæ ‡æ£€æµ‹å’Œ IoU è®¡ç®—
2. Semantic Expert (Qwen2-VL): è´Ÿè´£é¢œè‰²ã€æè´¨ã€é£æ ¼ç­‰è¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥
3. Hybrid Orchestrator: èåˆä¸¤è€…æ„è§ï¼Œç”Ÿæˆæœ€ç»ˆ Rationale

è®ºæ–‡è´¡çŒ®ï¼šNeuro-Symbolic Feedback Mechanismï¼ˆç¥ç»ç¬¦å·åé¦ˆæœºåˆ¶ï¼‰
- DINO ä»£è¡¨ç¬¦å·åŒ–çš„ç²¾å‡†å®šä½
- Qwen ä»£è¡¨ç¥ç»åŒ–çš„è¯­ä¹‰ç†è§£
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Optional, Tuple, Union, Literal
from PIL import Image
import json
import os

# å°è¯•å¯¼å…¥ Grounding DINO ä¾èµ–
try:
    from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
    HAS_DINO = True
except ImportError:
    HAS_DINO = False
    print("âš ï¸ transformers æœªå®‰è£…æˆ–ç‰ˆæœ¬ä¸æ”¯æŒ Grounding DINOï¼Œå°†é™çº§ä¸ºçº¯ Qwen æ¨¡å¼")

# å°è¯•å¯¼å…¥ Qwen2-VL
try:
    from transformers import Qwen2VLForConditionalGeneration, AutoProcessor as QwenProcessor
    HAS_QWEN = True
except ImportError:
    HAS_QWEN = False
    print("âš ï¸ transformers æœªå®‰è£…æˆ–ç‰ˆæœ¬ä¸æ”¯æŒ Qwen2-VL")


def calculate_iou(box1: List[float], box2: List[float]) -> float:
    """
    è®¡ç®—ä¸¤ä¸ª [x1, y1, x2, y2] æ¡†çš„ IoU
    
    Args:
        box1: [x1, y1, x2, y2] å½’ä¸€åŒ–åæ ‡ (0-1)
        box2: [x1, y1, x2, y2] å½’ä¸€åŒ–åæ ‡ (0-1)
    
    Returns:
        IoU å€¼ (0-1)
    """
    # ç¡®ä¿åæ ‡æ˜¯ (x1, y1, x2, y2)
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])

    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    
    return intersection / (union + 1e-6)


def normalize_bbox(bbox: List[float], image_size: Tuple[int, int]) -> List[float]:
    """
    å°† bbox å½’ä¸€åŒ–åˆ° 0-1 èŒƒå›´
    
    Args:
        bbox: å¯èƒ½æ˜¯ [x1, y1, x2, y2] æ ¼å¼ï¼ŒèŒƒå›´å¯èƒ½æ˜¯ 0-1000 æˆ– 0-1
        image_size: (width, height) å›¾åƒå°ºå¯¸
    
    Returns:
        å½’ä¸€åŒ–åçš„ [x1, y1, x2, y2] (0-1)
    """
    x1, y1, x2, y2 = bbox
    
    # å¦‚æœåæ ‡åœ¨ 0-1000 èŒƒå›´ï¼Œè½¬æ¢ä¸º 0-1
    if max(bbox) > 1.0:
        x1, y1, x2, y2 = [c / 1000.0 for c in bbox]
    
    # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
    x1 = max(0.0, min(1.0, x1))
    y1 = max(0.0, min(1.0, y1))
    x2 = max(0.0, min(1.0, x2))
    y2 = max(0.0, min(1.0, y2))
    
    # ç¡®ä¿ x1 < x2, y1 < y2
    if x1 > x2:
        x1, x2 = x2, x1
    if y1 > y2:
        y1, y2 = y2, y1
    
    return [x1, y1, x2, y2]


class GroundingDinoVerifier:
    """ç©ºé—´éªŒè¯ä¸“å®¶ï¼šä½¿ç”¨ Grounding DINO è¿›è¡Œé›¶æ ·æœ¬æ£€æµ‹å’Œä½ç½®éªŒè¯"""
    
    def __init__(self, model_id: str = "IDEA-Research/grounding-dino-base", device: str = "cuda"):
        """
        Args:
            model_id: Grounding DINO æ¨¡å‹ IDï¼ˆHuggingFaceï¼‰
            device: è®¾å¤‡
        """
        self.device = device
        self.model = None
        self.processor = None
        
        if not HAS_DINO:
            print("âš ï¸ Grounding DINO ä¸å¯ç”¨ï¼Œç©ºé—´éªŒè¯å°†è·³è¿‡")
            return
        
        print(f"ğŸ“¦ åŠ è½½ç©ºé—´ä¸“å®¶ (Grounding DINO): {model_id}")
        try:
            self.processor = AutoProcessor.from_pretrained(model_id)
            self.model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)
            self.model.eval()
            print("  âœ… Grounding DINO åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"  âš ï¸ Grounding DINO åŠ è½½å¤±è´¥: {e}")
            print("  ğŸ’¡ æç¤º: è¯·ç¡®ä¿ transformers >= 4.36.0")
            self.model = None
            self.processor = None

    def verify_layout(self, 
                     image: Image.Image, 
                     expected_layout: List[Dict], 
                     threshold: float = 0.35,
                     iou_threshold: float = 0.5) -> Dict:
        """
        éªŒè¯å¸ƒå±€ä¸€è‡´æ€§
        
        Args:
            image: PIL Image
            expected_layout: [{"name": "çŒ«", "bbox": [x1, y1, x2, y2]}, ...]
            threshold: DINO æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
            iou_threshold: IoU åŒ¹é…é˜ˆå€¼
        
        Returns:
            {
                "correct": bool,
                "feedback": str,
                "score": float,  # å¹³å‡ IoU
                "details": List[Dict]  # æ¯ä¸ªç‰©ä½“çš„æ£€æµ‹è¯¦æƒ…
            }
        """
        if not self.model or not expected_layout:
            return {
                "correct": True, 
                "feedback": "æ— ç©ºé—´çº¦æŸæˆ–æ¨¡å‹æœªåŠ è½½", 
                "score": 1.0,
                "details": []
            }

        # æå–æ‰€æœ‰éœ€è¦æ£€æµ‹çš„ç‰©ä½“åç§°
        # Grounding DINO éœ€è¦ text prompt æ ¼å¼ä¸º "cat. dog. chair."
        labels = [obj['name'] for obj in expected_layout]
        text_prompt = ". ".join(labels) + "."
        
        try:
            inputs = self.processor(images=image, text=text_prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model(**inputs)

            # åå¤„ç†
            target_sizes = torch.tensor([image.size[::-1]])  # [height, width]
            results = self.processor.post_process_grounded_object_detection(
                outputs,
                inputs.input_ids,
                box_threshold=threshold,
                text_threshold=0.25,
                target_sizes=target_sizes
            )[0]
            
            # è§£ææ£€æµ‹ç»“æœ
            detected_boxes = results["boxes"].cpu().numpy()  # [N, 4] åƒç´ åæ ‡
            detected_labels = results["labels"]  # [N] str
            detected_scores = results["scores"].cpu().numpy()  # [N]
            
            w, h = image.size
            
            feedback_lines = []
            all_passed = True
            total_iou = 0
            details = []
            
            # å¯¹æ¯ä¸ªæœŸæœ›ç‰©ä½“ï¼Œå¯»æ‰¾æœ€ä½³åŒ¹é…çš„æ£€æµ‹æ¡†
            for exp_obj in expected_layout:
                exp_name = exp_obj['name']
                exp_bbox = normalize_bbox(exp_obj['bbox'], (w, h))
                
                # è½¬æ¢ä¸ºåƒç´ åæ ‡ç”¨äº IoU è®¡ç®—
                exp_box_pixel = [
                    exp_bbox[0] * w, exp_bbox[1] * h,
                    exp_bbox[2] * w, exp_bbox[3] * h
                ]
                
                # åœ¨æ£€æµ‹ç»“æœä¸­æ‰¾åŒ¹é…çš„ç±»åˆ«å’Œæœ€ä½³ IoU
                best_iou = 0
                best_box = None
                best_label = None
                best_score = 0
                
                for pred_box, pred_label, pred_score in zip(detected_boxes, detected_labels, detected_scores):
                    # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦åŒ¹é…ï¼ˆç®€å•å­—ç¬¦ä¸²åŒ¹é…ï¼‰
                    if exp_name.lower() not in pred_label.lower():
                        continue
                    
                    # è½¬æ¢ä¸ºå½’ä¸€åŒ–åæ ‡ç”¨äº IoU è®¡ç®—
                    pred_box_norm = [
                        pred_box[0] / w, pred_box[1] / h,
                        pred_box[2] / w, pred_box[3] / h
                    ]
                    
                    # è®¡ç®— IoUï¼ˆä½¿ç”¨å½’ä¸€åŒ–åæ ‡ï¼‰
                    curr_iou = calculate_iou(exp_bbox, pred_box_norm)
                    
                    if curr_iou > best_iou:
                        best_iou = curr_iou
                        best_box = pred_box.tolist()
                        best_label = pred_label
                        best_score = float(pred_score)
                
                # åˆ¤æ–­æ˜¯å¦é€šè¿‡
                passed = best_iou >= iou_threshold
                if not passed:
                    all_passed = False
                    feedback_lines.append(f"âŒ {exp_name} ä½ç½®åå·®è¿‡å¤§ (IoU={best_iou:.2f}, é˜ˆå€¼={iou_threshold:.2f}) æˆ–æœªæ£€æµ‹åˆ°")
                else:
                    feedback_lines.append(f"âœ… {exp_name} ä½ç½®æ­£ç¡® (IoU={best_iou:.2f})")
                
                total_iou += best_iou
                
                details.append({
                    "name": exp_name,
                    "expected_bbox": exp_bbox,
                    "detected_bbox": best_box,
                    "iou": float(best_iou),
                    "passed": passed,
                    "detected_label": best_label,
                    "detection_score": best_score
                })
            
            avg_iou = total_iou / len(expected_layout) if expected_layout else 0
            
            return {
                "correct": all_passed,
                "feedback": " ".join(feedback_lines),
                "score": float(avg_iou),
                "details": details
            }
            
        except Exception as e:
            print(f"  âš ï¸ Grounding DINO éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
            return {
                "correct": True,  # å‡ºé”™æ—¶é»˜è®¤é€šè¿‡ï¼Œé¿å…é˜»å¡æµç¨‹
                "feedback": f"ç©ºé—´éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}",
                "score": 0.5,
                "details": []
            }


class QwenSemanticVerifier:
    """è¯­ä¹‰éªŒè¯ä¸“å®¶ï¼šä½¿ç”¨ Qwen2-VL æ£€æŸ¥é¢œè‰²ã€å±æ€§ã€æ•°é‡ç­‰è¯­ä¹‰ä¸€è‡´æ€§"""
    
    def __init__(self, model_path: str = "Qwen/Qwen2-VL-7B-Instruct", device: str = "cuda"):
        """
        Args:
            model_path: Qwen2-VL æ¨¡å‹è·¯å¾„ï¼ˆHuggingFace ID æˆ–æœ¬åœ°è·¯å¾„ï¼‰
            device: è®¾å¤‡
        """
        self.device = device
        self.model = None
        self.processor = None
        
        if not HAS_QWEN:
            print("âš ï¸ Qwen2-VL ä¸å¯ç”¨ï¼Œè¯­ä¹‰éªŒè¯å°†è·³è¿‡")
            return
        
        print(f"ğŸ“¦ åŠ è½½è¯­ä¹‰ä¸“å®¶ (Qwen2-VL): {model_path}")
        try:
            self.processor = QwenProcessor.from_pretrained(model_path, trust_remote_code=True)
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map=device,
                trust_remote_code=True
            ).eval()
            print("  âœ… Qwen2-VL åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"  âš ï¸ Qwen2-VL åŠ è½½å¤±è´¥: {e}")
            self.model = None
            self.processor = None

    def verify_semantics(self, image: Image.Image, prompt: str) -> Dict:
        """
        æ£€æŸ¥å›¾åƒæ˜¯å¦ç¬¦åˆ Prompt çš„è¯­ä¹‰æè¿°
        
        Args:
            image: PIL Image
            prompt: åŸå§‹ prompt
        
        Returns:
            {
                "correct": bool,
                "feedback": str,
                "confidence": float
            }
        """
        if not self.model:
            return {
                "correct": True,
                "feedback": "è¯­ä¹‰éªŒè¯å™¨æœªåŠ è½½ï¼Œè·³è¿‡éªŒè¯",
                "confidence": 0.5
            }
        
        try:
            query = f"""è¯·ä½œä¸ºä¸€åæåº¦ä¸¥æ ¼çš„è§†è§‰è´¨æ£€å‘˜ã€‚
ç”¨æˆ·æç¤ºè¯: "{prompt}"
è¯·æ£€æŸ¥ç”Ÿæˆçš„å›¾åƒæ˜¯å¦ç¬¦åˆä¸Šè¿°æè¿°ã€‚

é‡ç‚¹æ£€æŸ¥ï¼š
1. ç‰©ä½“æ˜¯å¦å­˜åœ¨ï¼Ÿ
2. é¢œè‰²ã€æè´¨ã€å±æ€§æ˜¯å¦æ­£ç¡®ï¼Ÿ
3. æ•°é‡æ˜¯å¦æ­£ç¡®ï¼Ÿ
4. åŠ¨ä½œã€å§¿æ€æ˜¯å¦ç¬¦åˆæè¿°ï¼Ÿ
(å¿½ç•¥å…·ä½“çš„ä½ç½®åæ ‡ï¼Œåªå…³æ³¨å†…å®¹æ­£ç¡®æ€§)

è¯·è¾“å‡ºç»“è®ºï¼š
å¦‚æœç¬¦åˆï¼Œè¾“å‡º"ç¬¦åˆ"ã€‚
å¦‚æœä¸ç¬¦åˆï¼Œç®€è¦è¯´æ˜åŸå› ï¼ˆä¾‹å¦‚ï¼š"é¢œè‰²ä¸å¯¹ï¼Œåº”è¯¥æ˜¯çº¢è‰²ä½†å›¾ä¸­æ˜¯è“è‰²"ï¼‰ã€‚"""

            messages = [
                {"role": "user", "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": query}
                ]}
            ]
            
            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = self.processor(text=[text], images=[image], padding=True, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                output_ids = self.model.generate(**inputs, max_new_tokens=128, do_sample=False)
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
            generated_ids = [
                out_ids[len(in_ids):] 
                for in_ids, out_ids in zip(inputs.input_ids, output_ids)
            ]
            response = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
            
            # è§£æå“åº”
            is_pass = "ç¬¦åˆ" in response and "ä¸ç¬¦åˆ" not in response
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºå…³é”®è¯ï¼‰
            if is_pass:
                confidence = 0.9 if "å®Œå…¨" in response or "éå¸¸" in response else 0.7
            else:
                confidence = 0.2
            
            return {
                "correct": is_pass,
                "feedback": response,
                "confidence": confidence
            }
            
        except Exception as e:
            print(f"  âš ï¸ Qwen2-VL éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
            return {
                "correct": True,  # å‡ºé”™æ—¶é»˜è®¤é€šè¿‡
                "feedback": f"è¯­ä¹‰éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}",
                "confidence": 0.5
            }


class HybridFeedbackVerifier:
    """
    [æ ¸å¿ƒ] å¼‚æ„æ··åˆéªŒè¯å™¨
    
    ç»„åˆ Grounding DINO (ç©ºé—´) + Qwen2-VL (è¯­ä¹‰)
    å®ç° Neuro-Symbolic Feedback Mechanism
    """
    
    def __init__(self, 
                 semantic_model: str = "Qwen/Qwen2-VL-7B-Instruct",
                 spatial_model: str = "IDEA-Research/grounding-dino-base",
                 device: str = "cuda"):
        """
        Args:
            semantic_model: Qwen2-VL æ¨¡å‹è·¯å¾„
            spatial_model: Grounding DINO æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡
        """
        self.device = device
        
        # åˆå§‹åŒ–ä¸¤ä¸ªä¸“å®¶
        self.spatial_expert = GroundingDinoVerifier(spatial_model, device)
        self.semantic_expert = QwenSemanticVerifier(semantic_model, device)
        
        print("ğŸ”€ å¼‚æ„æ··åˆéªŒè¯å™¨åˆå§‹åŒ–å®Œæˆ")

    def verify(self, 
               image: Image.Image,
               original_prompt: str,
               expected_layout: Optional[List[Dict]] = None,
               threshold: float = 0.7,
               iou_threshold: float = 0.5) -> Dict:
        """
        æ‰§è¡ŒåŒé‡éªŒè¯å¹¶èåˆç»“æœ
        
        Args:
            image: ç”Ÿæˆçš„å›¾åƒ
            original_prompt: åŸå§‹ prompt
            expected_layout: æœŸæœ›çš„å¸ƒå±€ [{"name": "...", "bbox": [...]}]
            threshold: æ€»ä½“ç½®ä¿¡åº¦é˜ˆå€¼
            iou_threshold: IoU åŒ¹é…é˜ˆå€¼ï¼ˆç”¨äºç©ºé—´éªŒè¯ï¼‰
        
        Returns:
            {
                "correct": bool,
                "confidence": float,
                "feedback": str,
                "refinement_instruction": Optional[str],
                "spatial_pass": bool,
                "semantic_pass": bool,
                "rationale": str
            }
        """
        feedback_parts = []
        is_spatial_pass = True
        is_semantic_pass = True
        spatial_score = 1.0
        semantic_confidence = 1.0
        
        # 1. ç©ºé—´éªŒè¯ (å¦‚æœæœ‰å¸ƒå±€è¦æ±‚)
        if expected_layout and self.spatial_expert.model:
            spatial_res = self.spatial_expert.verify_layout(
                image, expected_layout, threshold=0.35, iou_threshold=iou_threshold
            )
            is_spatial_pass = spatial_res["correct"]
            spatial_score = spatial_res["score"]
            feedback_parts.append(f"[ç©ºé—´æ£€æŸ¥] {spatial_res['feedback']}")
        elif expected_layout:
            # æœ‰å¸ƒå±€è¦æ±‚ä½† DINO æœªåŠ è½½ï¼Œç»™å‡ºè­¦å‘Š
            feedback_parts.append("[ç©ºé—´æ£€æŸ¥] Grounding DINO æœªåŠ è½½ï¼Œè·³è¿‡ä½ç½®éªŒè¯")
        
        # 2. è¯­ä¹‰éªŒè¯
        if self.semantic_expert.model:
            semantic_res = self.semantic_expert.verify_semantics(image, original_prompt)
            is_semantic_pass = semantic_res["correct"]
            semantic_confidence = semantic_res["confidence"]
            feedback_parts.append(f"[è¯­ä¹‰æ£€æŸ¥] {semantic_res['feedback']}")
        else:
            feedback_parts.append("[è¯­ä¹‰æ£€æŸ¥] Qwen2-VL æœªåŠ è½½ï¼Œè·³è¿‡è¯­ä¹‰éªŒè¯")
        
        # 3. èåˆå†³ç­–ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼šä¸¤è€…éƒ½å¿…é¡»é€šè¿‡ï¼‰
        final_pass = is_spatial_pass and is_semantic_pass
        
        # è®¡ç®—ç»¼åˆç½®ä¿¡åº¦
        if expected_layout and self.spatial_expert.model:
            # ç©ºé—´å’Œè¯­ä¹‰å„å  50%
            combined_confidence = (spatial_score + semantic_confidence) / 2
        else:
            # åªæœ‰è¯­ä¹‰éªŒè¯
            combined_confidence = semantic_confidence
        
        final_pass = final_pass and (combined_confidence >= threshold)
        
        # 4. ç”Ÿæˆä¿®æ­£æŒ‡ä»¤
        refinement_instruction = None
        if not final_pass:
            refinement_instruction = "è¯·ä¿®æ­£ä»¥ä¸‹é—®é¢˜ï¼š\n" + "\n".join(feedback_parts)
        
        # 5. ç”Ÿæˆ Rationaleï¼ˆç”¨äºè®ºæ–‡å±•ç¤ºï¼‰
        rationale = f"ç©ºé—´éªŒè¯: {'é€šè¿‡' if is_spatial_pass else 'å¤±è´¥'} (IoU={spatial_score:.2f}); " \
                   f"è¯­ä¹‰éªŒè¯: {'é€šè¿‡' if is_semantic_pass else 'å¤±è´¥'} (ç½®ä¿¡åº¦={semantic_confidence:.2f})"
        
        return {
            "correct": final_pass,
            "confidence": combined_confidence,
            "feedback": "\n".join(feedback_parts),
            "refinement_instruction": refinement_instruction,
            "spatial_pass": is_spatial_pass,
            "semantic_pass": is_semantic_pass,
            "rationale": rationale,
            "suggested_prompt": original_prompt,
            "detected_objects": []
        }


class FeedbackVerifier:
    """
    åé¦ˆéªŒè¯å™¨ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
    
    æ”¯æŒå¤šç§éªŒè¯æ¨¡å¼ï¼š
    - "hybrid": æ··åˆæ¨¡å¼ï¼ˆGrounding DINO + Qwen2-VLï¼‰
    - "grounding_dino": ä»…ç©ºé—´éªŒè¯
    - "qwen2vl_7b": ä»…è¯­ä¹‰éªŒè¯
    - "qwen2vl": å…¼å®¹æ—§ä»£ç 
    """
    
    def __init__(self, 
                 verifier_type: Literal["grounding_dino", "qwen2vl_7b", "hybrid", "qwen2vl"] = "hybrid",
                 vlm_model_name: Optional[str] = None,
                 device: str = "cuda",
                 use_grounding: bool = True):
        """
        Args:
            verifier_type: éªŒè¯å™¨ç±»å‹
            vlm_model_name: VLM æ¨¡å‹åç§°ï¼ˆç”¨äºå…¼å®¹æ—§ä»£ç ï¼‰
            device: è®¾å¤‡
            use_grounding: æ˜¯å¦ä½¿ç”¨ groundingï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
        """
        self.device = device
        self.verifier_type = verifier_type
        
        # æ ¹æ®ç±»å‹åˆå§‹åŒ–ç›¸åº”çš„éªŒè¯å™¨
        if verifier_type == "hybrid":
            semantic_model = vlm_model_name or "Qwen/Qwen2-VL-7B-Instruct"
            self.verifier = HybridFeedbackVerifier(
                semantic_model=semantic_model,
                device=device
            )
        elif verifier_type == "grounding_dino":
            self.verifier = GroundingDinoVerifier(device=device)
        elif verifier_type in ["qwen2vl_7b", "qwen2vl"]:
            model_path = vlm_model_name or "Qwen/Qwen2-VL-7B-Instruct"
            self.semantic_expert = QwenSemanticVerifier(model_path, device)
            self.verifier = self.semantic_expert
            else:
            raise ValueError(f"æœªçŸ¥çš„éªŒè¯å™¨ç±»å‹: {verifier_type}")
    
    def verify(self, 
               image: Image.Image,
               original_prompt: str,
               expected_layout: Optional[List[Dict]] = None,
               threshold: float = 0.7) -> Dict:
        """
        ç»Ÿä¸€éªŒè¯æ¥å£
        
        Args:
            image: ç”Ÿæˆçš„å›¾åƒ
            original_prompt: åŸå§‹ prompt
            expected_layout: æœŸæœ›çš„å¸ƒå±€
            threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        if isinstance(self.verifier, HybridFeedbackVerifier):
            return self.verifier.verify(image, original_prompt, expected_layout, threshold)
        elif isinstance(self.verifier, GroundingDinoVerifier):
            if expected_layout:
                result = self.verifier.verify_layout(image, expected_layout)
            return {
                    "correct": result["correct"],
                    "confidence": result["score"],
                    "feedback": result["feedback"],
                    "refinement_instruction": result["feedback"] if not result["correct"] else None,
                "suggested_prompt": original_prompt,
                    "detected_objects": result.get("details", [])
                }
            else:
                return {
                    "correct": True,
                    "confidence": 1.0,
                    "feedback": "æ— å¸ƒå±€çº¦æŸ",
                    "suggested_prompt": original_prompt,
                    "detected_objects": []
                }
        elif isinstance(self.verifier, QwenSemanticVerifier):
            result = self.verifier.verify_semantics(image, original_prompt)
            return {
                "correct": result["correct"],
                "confidence": result["confidence"],
                "feedback": result["feedback"],
                "refinement_instruction": result["feedback"] if not result["correct"] else None,
                "suggested_prompt": original_prompt,
                "detected_objects": []
            }
        else:
            return {
                "correct": True,
                "confidence": 0.5,
                "feedback": "éªŒè¯å™¨æœªæ­£ç¡®åˆå§‹åŒ–",
                "suggested_prompt": original_prompt,
                "detected_objects": []
            }
    

def create_feedback_verifier(vlm_model_name: str = "Qwen/Qwen2-VL-7B-Instruct",
                             verifier_type: Literal["grounding_dino", "qwen2vl_7b", "hybrid", "qwen2vl"] = "hybrid",
                             device: str = "cuda",
                             use_grounding: bool = True) -> FeedbackVerifier:
    """
    åˆ›å»ºåé¦ˆéªŒè¯å™¨ï¼ˆå·¥å‚å‡½æ•°ï¼‰
    
    Args:
        vlm_model_name: VLM æ¨¡å‹åç§°ï¼ˆç”¨äº qwen2vl æ¨¡å¼ï¼‰
        verifier_type: éªŒè¯å™¨ç±»å‹
            - "grounding_dino": ä»…ä½¿ç”¨ Grounding DINO
            - "qwen2vl_7b": ä»…ä½¿ç”¨ Qwen2-VL-7B
            - "hybrid": æ··åˆæ¨¡å¼ï¼ˆGrounding DINO + Qwen2-VL-7Bï¼Œæ¨èï¼‰
            - "qwen2vl": ä½¿ç”¨æŒ‡å®šçš„ Qwen-VL æ¨¡å‹ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
        device: è®¾å¤‡
        use_grounding: æ˜¯å¦ä½¿ç”¨ groundingï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
    
    Returns:
        FeedbackVerifier å®ä¾‹
    """
    return FeedbackVerifier(
        verifier_type=verifier_type,
        vlm_model_name=vlm_model_name,
        device=device,
        use_grounding=use_grounding
    )
