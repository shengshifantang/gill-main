"""
åé¦ˆéªŒè¯æ¨¡å— (Feedback Verifier)

ä½¿ç”¨ Qwen-VL æˆ– KOSMOS-2 éªŒè¯ç”Ÿæˆå›¾åƒæ˜¯å¦ç¬¦åˆ prompt å’Œå¸ƒå±€è¦æ±‚ã€‚
"""

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple
from PIL import Image
import numpy as np


class FeedbackVerifier:
    """
    åé¦ˆéªŒè¯å™¨
    
    ä½¿ç”¨ VLMï¼ˆVision-Language Modelï¼‰éªŒè¯ç”Ÿæˆå›¾åƒæ˜¯å¦ç¬¦åˆè¦æ±‚
    """
    
    def __init__(self, 
                 vlm_model_name: str = "Qwen/Qwen-VL",
                 device: str = "cuda",
                 use_grounding: bool = True):
        """
        Args:
            vlm_model_name: VLM æ¨¡å‹åç§°æˆ–è·¯å¾„
            device: è®¾å¤‡
            use_grounding: æ˜¯å¦ä½¿ç”¨ grounding åŠŸèƒ½ï¼ˆæ£€æµ‹å¯¹è±¡ä½ç½®ï¼‰
        """
        self.device = device
        self.use_grounding = use_grounding
        self.vlm_model_name = vlm_model_name
        
        # åŠ è½½ VLM æ¨¡å‹
        self._load_vlm_model()
    
    def _load_vlm_model(self):
        """åŠ è½½ VLM æ¨¡å‹"""
        try:
            from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
            
            print(f"ğŸ“¦ åŠ è½½ VLM æ¨¡å‹: {self.vlm_model_name}")
            
            # å°è¯•åŠ è½½ Qwen-VL
            if "qwen" in self.vlm_model_name.lower() or "Qwen" in self.vlm_model_name:
                try:
                    self.processor = AutoProcessor.from_pretrained(
                        self.vlm_model_name,
                        trust_remote_code=True
                    )
                    self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                        self.vlm_model_name,
                        torch_dtype=torch.bfloat16,
                        device_map=self.device,
                        trust_remote_code=True
                    )
                    self.model_type = "qwen"
                    print("âœ“ ä½¿ç”¨ Qwen-VL ä½œä¸ºéªŒè¯å™¨")
                except Exception as e:
                    print(f"âš ï¸ Qwen-VL åŠ è½½å¤±è´¥: {e}")
                    self._load_fallback_model()
            else:
                self._load_fallback_model()
                
        except ImportError:
            print("âš ï¸ transformers æœªå®‰è£…æˆ–ç‰ˆæœ¬ä¸æ”¯æŒï¼Œä½¿ç”¨è½»é‡çº§éªŒè¯å™¨")
            self._load_fallback_model()
    
    def _load_fallback_model(self):
        """åŠ è½½å¤‡ç”¨éªŒè¯å™¨ï¼ˆåŸºäº CLIPï¼‰"""
        try:
            from transformers import CLIPProcessor, CLIPModel
            
            print("ğŸ“¦ ä½¿ç”¨ CLIP ä½œä¸ºå¤‡ç”¨éªŒè¯å™¨")
            self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
            self.model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14").to(self.device)
            self.model_type = "clip"
            print("âœ“ ä½¿ç”¨ CLIP ä½œä¸ºéªŒè¯å™¨ï¼ˆåŠŸèƒ½å—é™ï¼‰")
        except Exception as e:
            print(f"âŒ å¤‡ç”¨éªŒè¯å™¨åŠ è½½å¤±è´¥: {e}")
            self.model = None
            self.processor = None
            self.model_type = None
    
    def verify(self, 
               image: Image.Image,
               original_prompt: str,
               expected_layout: Optional[List[Dict]] = None,
               threshold: float = 0.7) -> Dict:
        """
        éªŒè¯ç”Ÿæˆå›¾åƒæ˜¯å¦ç¬¦åˆè¦æ±‚
        
        Args:
            image: ç”Ÿæˆçš„å›¾åƒï¼ˆPIL Imageï¼‰
            original_prompt: åŸå§‹ prompt
            expected_layout: æœŸæœ›çš„å¸ƒå±€ä¿¡æ¯ [{"name": "...", "bbox": [...]}]
            threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        
        Returns:
            {
                "correct": bool,  # æ˜¯å¦é€šè¿‡éªŒè¯
                "confidence": float,  # ç½®ä¿¡åº¦ (0-1)
                "feedback": str,  # åé¦ˆä¿¡æ¯
                "suggested_prompt": str,  # ä¿®æ­£å»ºè®®
                "detected_objects": List[Dict],  # æ£€æµ‹åˆ°çš„å¯¹è±¡ï¼ˆå¦‚æœæ”¯æŒï¼‰
            }
        """
        if self.model is None:
            # å¦‚æœæ²¡æœ‰æ¨¡å‹ï¼Œè¿”å›é»˜è®¤ç»“æœ
            return {
                "correct": True,
                "confidence": 0.5,
                "feedback": "éªŒè¯å™¨æœªåŠ è½½ï¼Œè·³è¿‡éªŒè¯",
                "suggested_prompt": original_prompt,
                "detected_objects": []
            }
        
        self.model.eval()
        
        with torch.no_grad():
            if self.model_type == "qwen":
                return self._verify_with_qwen(image, original_prompt, expected_layout, threshold)
            elif self.model_type == "clip":
                return self._verify_with_clip(image, original_prompt, threshold)
            else:
                return {
                    "correct": True,
                    "confidence": 0.5,
                    "feedback": "æœªçŸ¥çš„éªŒè¯å™¨ç±»å‹",
                    "suggested_prompt": original_prompt,
                    "detected_objects": []
                }
    
    def _verify_with_qwen(self, 
                         image: Image.Image,
                         prompt: str,
                         expected_layout: Optional[List[Dict]],
                         threshold: float) -> Dict:
        """ä½¿ç”¨ Qwen-VL éªŒè¯"""
        try:
            # æ„å»ºéªŒè¯ prompt
            verify_prompt = f"è¯·æ£€æŸ¥è¿™å¼ å›¾ç‰‡æ˜¯å¦ç¬¦åˆä»¥ä¸‹æè¿°ï¼š{prompt}ã€‚"
            if expected_layout:
                verify_prompt += " ç‰¹åˆ«æ£€æŸ¥ä»¥ä¸‹å¯¹è±¡çš„ä½ç½®ï¼š"
                for obj in expected_layout:
                    verify_prompt += f" {obj['name']}åº”è¯¥åœ¨ä½ç½®{obj['bbox']}ï¼›"
            
            # å¤„ç†è¾“å…¥
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": verify_prompt}
                    ]
                }
            ]
            
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            image_inputs, video_inputs = self.processor(
                text=[text],
                images=[image],
                padding=True,
                return_tensors="pt"
            ).to(self.device)
            
            # ç”ŸæˆéªŒè¯ç»“æœ
            generated_ids = self.model.generate(
                **image_inputs,
                max_new_tokens=128,
                do_sample=False
            )
            
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(image_inputs.input_ids, generated_ids)
            ]
            
            response_text = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]
            
            # è§£æå“åº”ï¼ˆç®€å•å¯å‘å¼ï¼‰
            is_correct = "ç¬¦åˆ" in response_text or "æ­£ç¡®" in response_text or "æ˜¯çš„" in response_text
            confidence = 0.8 if is_correct else 0.3
            
            # æå–åé¦ˆ
            feedback = response_text.strip()
            
            # ç”Ÿæˆä¿®æ­£å»ºè®®
            suggested_prompt = prompt
            if not is_correct and "å»ºè®®" in response_text or "åº”è¯¥" in response_text:
                # å°è¯•ä»å“åº”ä¸­æå–å»ºè®®
                suggested_prompt = prompt  # ç®€åŒ–å¤„ç†
            
            return {
                "correct": is_correct and confidence >= threshold,
                "confidence": confidence,
                "feedback": feedback,
                "suggested_prompt": suggested_prompt,
                "detected_objects": []  # Qwen-VL éœ€è¦é¢å¤–è°ƒç”¨æ‰èƒ½è·å– grounding
            }
            
        except Exception as e:
            print(f"âš ï¸ Qwen-VL éªŒè¯å‡ºé”™: {e}")
            return {
                "correct": True,  # å‡ºé”™æ—¶é»˜è®¤é€šè¿‡
                "confidence": 0.5,
                "feedback": f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}",
                "suggested_prompt": prompt,
                "detected_objects": []
            }
    
    def _verify_with_clip(self, 
                         image: Image.Image,
                         prompt: str,
                         threshold: float) -> Dict:
        """ä½¿ç”¨ CLIP éªŒè¯ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰"""
        try:
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=[prompt],
                images=[image],
                return_tensors="pt",
                padding=True
            ).to(self.device)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            outputs = self.model(**inputs)
            logits_per_image = outputs.logits_per_image
            probs = logits_per_image.softmax(dim=1)
            
            confidence = probs[0, 0].item()
            is_correct = confidence >= threshold
            
            return {
                "correct": is_correct,
                "confidence": confidence,
                "feedback": f"CLIPç›¸ä¼¼åº¦: {confidence:.3f}",
                "suggested_prompt": prompt,
                "detected_objects": []
            }
            
        except Exception as e:
            print(f"âš ï¸ CLIP éªŒè¯å‡ºé”™: {e}")
            return {
                "correct": True,
                "confidence": 0.5,
                "feedback": f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}",
                "suggested_prompt": prompt,
                "detected_objects": []
            }
    
    def batch_verify(self, 
                    images: List[Image.Image],
                    prompts: List[str],
                    expected_layouts: Optional[List[List[Dict]]] = None,
                    threshold: float = 0.7) -> List[Dict]:
        """æ‰¹é‡éªŒè¯"""
        results = []
        for i, (image, prompt) in enumerate(zip(images, prompts)):
            expected_layout = expected_layouts[i] if expected_layouts else None
            result = self.verify(image, prompt, expected_layout, threshold)
            results.append(result)
        return results


def create_feedback_verifier(vlm_model_name: str = "Qwen/Qwen-VL",
                             device: str = "cuda",
                             use_grounding: bool = True) -> FeedbackVerifier:
    """
    åˆ›å»ºåé¦ˆéªŒè¯å™¨
    
    Args:
        vlm_model_name: VLM æ¨¡å‹åç§°
        device: è®¾å¤‡
        use_grounding: æ˜¯å¦ä½¿ç”¨ grounding
    
    Returns:
        FeedbackVerifier å®ä¾‹
    """
    return FeedbackVerifier(
        vlm_model_name=vlm_model_name,
        device=device,
        use_grounding=use_grounding
    )

