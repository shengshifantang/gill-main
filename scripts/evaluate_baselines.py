#!/usr/bin/env python3
"""
å¤šå±‚æ¬¡ Baseline å¯¹æ¯”è¯„ä¼°æ¡†æ¶

æ”¯æŒä¸‰ç±»å¯¹æ¯”ï¼š
1. æ¶ˆèå®éªŒ (Ablation): Base GILL, Heuristic Layout
2. åŒç±»ç«å“ (SOTA): GLIGEN, ControlNet, Emu2
3. é€šç”¨æ¨¡å‹ (Generalist): DALL-E 3, Midjourney

Usage:
    python scripts/evaluate_baselines.py \
        --test-set data/test_set.jsonl \
        --output-dir evaluation_results/baseline_comparison \
        --baselines base_gill heuristic ours \
        --metrics layout_iou clip_score fid
"""

import argparse
import json
import os
import sys
from typing import Dict, List, Optional, Tuple
from collections import defaultdict
import numpy as np
from tqdm import tqdm
import torch

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from gill.models import GILL, load_gill
from gill.layout_planner import LayoutPlanner


class BaselineGenerator:
    """ç»Ÿä¸€çš„ Baseline ç”Ÿæˆæ¥å£"""
    
    def __init__(self, baseline_type: str, config: Dict):
        self.baseline_type = baseline_type
        self.config = config
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """æ ¹æ® baseline ç±»å‹åŠ è½½æ¨¡å‹"""
        if self.baseline_type == "base_gill":
            # Baseline A: Base GILL (æ—  Layout Planner, æ—  Spatial Adapter)
            self.model = load_gill(
                gill_model=self.config.get("gill_model", "./checkpoints/gill_opt"),
                load_sd=True,
                device=self.config.get("device", "cuda:0")
            )
            # ç¡®ä¿ä¸ä½¿ç”¨ Layout Planner å’Œ Spatial Adapter
            self.model.use_layout_planner = False
        
        elif self.baseline_type == "heuristic":
            # Baseline B: Heuristic Layout (æœ‰ Spatial Adapter, ä½†ç”¨è§„åˆ™ç”Ÿæˆåæ ‡)
            self.model = load_gill(
                gill_model=self.config.get("gill_model", "./checkpoints/gill_opt"),
                load_sd=True,
                device=self.config.get("device", "cuda:0")
            )
            # ä½¿ç”¨å¯å‘å¼å¸ƒå±€ç”Ÿæˆå™¨ï¼ˆä¸ä½¿ç”¨ LLM Plannerï¼‰
            from gill.layout_planner import heuristic_layout_from_caption
            self.heuristic_fn = heuristic_layout_from_caption
        
        elif self.baseline_type == "ours":
            # Ours: Full Pipeline (CoT Layout Planner + Spatial Adapter)
            self.model = load_gill(
                gill_model=self.config.get("gill_model", "./checkpoints/gill_opt"),
                load_sd=True,
                device=self.config.get("device", "cuda:0")
            )
            # åŠ è½½ Layout Planner
            planner_model = self.config.get("planner_model", "./checkpoints/layout_planner_cot_15k")
            self.planner = LayoutPlanner(planner_model, device=self.config.get("device", "cuda:0"))
        
        elif self.baseline_type == "gligen":
            # Baseline C: GLIGEN (éœ€è¦æ‰‹åŠ¨è¾“å…¥ bbox)
            # è¿™é‡Œå¯ä»¥åŠ è½½ GLIGEN æ¨¡å‹æˆ–ä½¿ç”¨ç±»ä¼¼çš„å®ç°
            raise NotImplementedError("GLIGEN baseline éœ€è¦å•ç‹¬å®ç°")
        
        elif self.baseline_type == "dalle3":
            # Baseline E: DALL-E 3 (é€šè¿‡ API)
            # éœ€è¦ OpenAI API Key
            self.api_key = self.config.get("openai_api_key")
            if not self.api_key:
                raise ValueError("DALL-E 3 éœ€è¦ OpenAI API Key")
        
        else:
            raise ValueError(f"Unknown baseline type: {self.baseline_type}")
    
    def generate(self, prompt: str, **kwargs) -> Tuple[Optional[torch.Tensor], Optional[Dict]]:
        """
        ç”Ÿæˆå›¾åƒ
        
        Returns:
            (image_tensor, metadata)
            - image_tensor: ç”Ÿæˆçš„å›¾åƒ (PIL Image æˆ– torch.Tensor)
            - metadata: åŒ…å«å¸ƒå±€ä¿¡æ¯ã€æ¨ç†è¿‡ç¨‹ç­‰
        """
        if self.baseline_type == "base_gill":
            # ç›´æ¥ä½¿ç”¨ prompt ç”Ÿæˆï¼Œæ— å¸ƒå±€æ§åˆ¶
            result = self.model.generate_for_images_and_texts(
                [prompt],
                num_words=16,
                guidance_scale=7.5
            )
            image = result[0] if result else None
            metadata = {"layout_used": False, "bboxes": None}
            return image, metadata
        
        elif self.baseline_type == "heuristic":
            # ä½¿ç”¨å¯å‘å¼è§„åˆ™ç”Ÿæˆå¸ƒå±€
            from gill.layout_planner import heuristic_layout_from_caption
            objects = heuristic_layout_from_caption(prompt)
            
            if not objects:
                # å¦‚æœå¯å‘å¼å¤±è´¥ï¼Œå›é€€åˆ° base_gill
                return self._generate_base(prompt)
            
            # è½¬æ¢ä¸º bbox æ ¼å¼
            bboxes = torch.tensor([[obj["bbox"][0], obj["bbox"][1], 
                                   obj["bbox"][2], obj["bbox"][3]] 
                                  for obj in objects], dtype=torch.float32)
            
            # ä½¿ç”¨ Spatial Adapter ç”Ÿæˆ
            result = self.model.generate_with_layout(
                prompt=prompt,
                objects=[obj["name"] for obj in objects],
                bboxes=bboxes.unsqueeze(0),
                enable_layout=True,
                spatial_adapter=self.model.spatial_adapter if hasattr(self.model, 'spatial_adapter') else None
            )
            
            image = result.get("generated_image") if isinstance(result, dict) else result
            metadata = {
                "layout_used": True,
                "layout_method": "heuristic",
                "bboxes": bboxes.tolist(),
                "objects": objects
            }
            return image, metadata
        
        elif self.baseline_type == "ours":
            # ä½¿ç”¨ CoT Layout Planner + Spatial Adapter
            # 1. ä½¿ç”¨ Planner ç”Ÿæˆå¸ƒå±€
            layout_output = self.planner.plan_layout(prompt)
            objects, bboxes = self.planner.parse_layout_output(layout_output)
            
            if not objects or bboxes is None:
                return None, {"error": "Layout planning failed"}
            
            # 2. ä½¿ç”¨ Spatial Adapter ç”Ÿæˆ
            result = self.model.generate_with_layout(
                prompt=prompt,
                objects=objects,
                bboxes=bboxes,
                enable_layout=True,
                spatial_adapter=self.model.spatial_adapter if hasattr(self.model, 'spatial_adapter') else None
            )
            
            image = result.get("generated_image") if isinstance(result, dict) else result
            metadata = {
                "layout_used": True,
                "layout_method": "cot_planner",
                "bboxes": bboxes.tolist() if isinstance(bboxes, torch.Tensor) else bboxes,
                "objects": objects,
                "cot_reasoning": layout_output.get("reasoning", "") if isinstance(layout_output, dict) else ""
            }
            return image, metadata
        
        elif self.baseline_type == "dalle3":
            # é€šè¿‡ OpenAI API è°ƒç”¨ DALL-E 3
            try:
                from openai import OpenAI
                client = OpenAI(api_key=self.api_key)
                
                response = client.images.generate(
                    model="dall-e-3",
                    prompt=prompt,
                    size="1024x1024",
                    quality="standard",
                    n=1,
                )
                
                image_url = response.data[0].url
                # ä¸‹è½½å›¾ç‰‡
                import requests
                from PIL import Image
                img_response = requests.get(image_url)
                image = Image.open(io.BytesIO(img_response.content))
                
                metadata = {
                    "layout_used": False,
                    "model": "dall-e-3",
                    "api_call": True
                }
                return image, metadata
            except Exception as e:
                return None, {"error": str(e)}
        
        return None, {"error": "Unknown baseline type"}
    
    def _generate_base(self, prompt: str):
        """Base GILL ç”Ÿæˆï¼ˆæ— å¸ƒå±€æ§åˆ¶ï¼‰"""
        result = self.model.generate_for_images_and_texts(
            [prompt],
            num_words=16,
            guidance_scale=7.5
        )
        image = result[0] if result else None
        metadata = {"layout_used": False}
        return image, metadata


class LayoutEvaluator:
    """å¸ƒå±€å‡†ç¡®ç‡è¯„ä¼°å™¨ï¼ˆä½¿ç”¨ GroundingDINO æˆ– YOLO-Worldï¼‰"""
    
    def __init__(self, detector_type: str = "grounding_dino"):
        self.detector_type = detector_type
        self.detector = None
        self._load_detector()
    
    def _load_detector(self):
        """åŠ è½½ç›®æ ‡æ£€æµ‹å™¨"""
        if self.detector_type == "grounding_dino":
            try:
                from groundingdino.util.inference import load_model, load_image, predict
                # éœ€è¦ä¸‹è½½ GroundingDINO æ¨¡å‹
                self.detector = {
                    "model": load_model("groundingdino_swinb_cogcoor.pth", "groundingdino/config/GroundingDINO_SwinB.cfg.py"),
                    "predict_fn": predict
                }
            except ImportError:
                print("âš ï¸ GroundingDINO æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆè¯„ä¼°")
                self.detector = None
        elif self.detector_type == "yolo_world":
            try:
                from ultralytics import YOLO
                self.detector = YOLO("yolov8x-world.pt")
            except ImportError:
                print("âš ï¸ YOLO-World æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆè¯„ä¼°")
                self.detector = None
    
    def compute_layout_iou(self, image, prompt: str, predicted_bboxes: List[List[float]], 
                          object_names: List[str]) -> Dict:
        """
        è®¡ç®—å¸ƒå±€ IoU
        
        Args:
            image: PIL Image æˆ– torch.Tensor
            prompt: åŸå§‹ prompt
            predicted_bboxes: é¢„æµ‹çš„ bbox åˆ—è¡¨ [[x1,y1,x2,y2], ...]
            object_names: ç‰©ä½“åç§°åˆ—è¡¨
        
        Returns:
            {
                "mean_iou": float,
                "per_object_iou": List[float],
                "object_recall": float,
                "count_accuracy": float
            }
        """
        if self.detector is None:
            # ç®€åŒ–ç‰ˆï¼šåªè¿”å›å ä½ç¬¦
            return {
                "mean_iou": 0.0,
                "per_object_iou": [0.0] * len(object_names),
                "object_recall": 0.0,
                "count_accuracy": 0.0
            }
        
        # ä½¿ç”¨æ£€æµ‹å™¨æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“
        detected_objects = self._detect_objects(image, prompt, object_names)
        
        # è®¡ç®— IoU
        ious = []
        for pred_bbox, obj_name in zip(predicted_bboxes, object_names):
            if obj_name in detected_objects:
                detected_bbox = detected_objects[obj_name]["bbox"]
                iou = self._compute_iou(pred_bbox, detected_bbox)
                ious.append(iou)
            else:
                ious.append(0.0)
        
        # è®¡ç®—å¬å›ç‡
        detected_count = len(detected_objects)
        expected_count = len(object_names)
        object_recall = detected_count / expected_count if expected_count > 0 else 0.0
        
        # è®¡ç®—æ•°é‡å‡†ç¡®ç‡
        count_accuracy = 1.0 if detected_count == expected_count else 0.0
        
        return {
            "mean_iou": np.mean(ious) if ious else 0.0,
            "per_object_iou": ious,
            "object_recall": object_recall,
            "count_accuracy": count_accuracy
        }
    
    def _detect_objects(self, image, prompt: str, object_names: List[str]) -> Dict:
        """ä½¿ç”¨æ£€æµ‹å™¨æ£€æµ‹ç‰©ä½“"""
        # å®ç°æ£€æµ‹é€»è¾‘
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„æ£€æµ‹å™¨ API å®ç°
        return {}
    
    def _compute_iou(self, bbox1: List[float], bbox2: List[float]) -> float:
        """è®¡ç®—ä¸¤ä¸ª bbox çš„ IoU"""
        x1_1, y1_1, x2_1, y2_1 = bbox1
        x1_2, y1_2, x2_2, y2_2 = bbox2
        
        inter_x1 = max(x1_1, x1_2)
        inter_y1 = max(y1_1, y1_2)
        inter_x2 = min(x2_1, x2_2)
        inter_y2 = min(y2_1, y2_2)
        
        if inter_x2 <= inter_x1 or inter_y2 <= inter_y1:
            return 0.0
        
        inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union_area = area1 + area2 - inter_area
        
        return inter_area / union_area if union_area > 0 else 0.0


class CLIPScoreEvaluator:
    """CLIP Score è¯„ä¼°å™¨ï¼ˆä¸­æ–‡ CLIPï¼‰"""
    
    def __init__(self, clip_model_path: str = "./model/chinese_clip_ViT-L-14"):
        self.clip_model_path = clip_model_path
        self.model = None
        self.processor = None
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½ä¸­æ–‡ CLIP æ¨¡å‹"""
        try:
            from transformers import CLIPProcessor, CLIPModel
            self.model = CLIPModel.from_pretrained(self.clip_model_path)
            self.processor = CLIPProcessor.from_pretrained(self.clip_model_path)
            self.model.eval()
        except Exception as e:
            print(f"âš ï¸ CLIP æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.model = None
    
    def compute_clip_score(self, image, text: str) -> float:
        """è®¡ç®— CLIP Score"""
        if self.model is None:
            return 0.0
        
        inputs = self.processor(text=[text], images=[image], return_tensors="pt", padding=True)
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits_per_image = outputs.logits_per_image
            score = logits_per_image.item()
        
        return score


def evaluate_baselines(test_set_path: str, output_dir: str, baselines: List[str], 
                      metrics: List[str], config: Dict):
    """è¯„ä¼°å¤šä¸ª Baseline"""
    
    # åŠ è½½æµ‹è¯•é›†
    test_samples = []
    with open(test_set_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                test_samples.append(json.loads(line))
    
    print(f"ğŸ“Š åŠ è½½æµ‹è¯•é›†: {len(test_samples)} æ¡")
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    layout_evaluator = LayoutEvaluator() if "layout_iou" in metrics else None
    clip_evaluator = CLIPScoreEvaluator() if "clip_score" in metrics else None
    
    # è¯„ä¼°æ¯ä¸ª baseline
    results = {}
    
    for baseline_name in baselines:
        print(f"\n{'='*60}")
        print(f"ğŸ” è¯„ä¼° Baseline: {baseline_name}")
        print(f"{'='*60}")
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        baseline_config = config.get(baseline_name, {})
        generator = BaselineGenerator(baseline_name, baseline_config)
        
        baseline_results = {
            "layout_iou": [],
            "object_recall": [],
            "count_accuracy": [],
            "clip_score": [],
            "metadata": []
        }
        
        for sample in tqdm(test_samples, desc=f"ç”Ÿæˆ {baseline_name}"):
            prompt = sample.get("caption", sample.get("prompt", ""))
            gt_objects = sample.get("objects", [])
            
            # ç”Ÿæˆå›¾åƒ
            image, metadata = generator.generate(prompt)
            
            if image is None:
                continue
            
            # ä¿å­˜å›¾åƒ
            os.makedirs(os.path.join(output_dir, baseline_name, "images"), exist_ok=True)
            image_path = os.path.join(output_dir, baseline_name, "images", 
                                     f"{sample.get('id', len(baseline_results['metadata']))}.png")
            if hasattr(image, 'save'):
                image.save(image_path)
            else:
                from PIL import Image
                if isinstance(image, torch.Tensor):
                    image = Image.fromarray(image.cpu().numpy())
                image.save(image_path)
            
            # è¯„ä¼°æŒ‡æ ‡
            if "layout_iou" in metrics and layout_evaluator and metadata.get("bboxes"):
                layout_metrics = layout_evaluator.compute_layout_iou(
                    image, prompt, 
                    metadata.get("bboxes", []),
                    [obj.get("name", "") for obj in metadata.get("objects", [])]
                )
                baseline_results["layout_iou"].append(layout_metrics["mean_iou"])
                baseline_results["object_recall"].append(layout_metrics["object_recall"])
                baseline_results["count_accuracy"].append(layout_metrics["count_accuracy"])
            
            if "clip_score" in metrics and clip_evaluator:
                clip_score = clip_evaluator.compute_clip_score(image, prompt)
                baseline_results["clip_score"].append(clip_score)
            
            baseline_results["metadata"].append({
                "prompt": prompt,
                "image_path": image_path,
                "metadata": metadata
            })
        
        # è®¡ç®—å¹³å‡å€¼
        results[baseline_name] = {
            "mean_layout_iou": np.mean(baseline_results["layout_iou"]) if baseline_results["layout_iou"] else 0.0,
            "mean_object_recall": np.mean(baseline_results["object_recall"]) if baseline_results["object_recall"] else 0.0,
            "mean_count_accuracy": np.mean(baseline_results["count_accuracy"]) if baseline_results["count_accuracy"] else 0.0,
            "mean_clip_score": np.mean(baseline_results["clip_score"]) if baseline_results["clip_score"] else 0.0,
            "samples": baseline_results["metadata"]
        }
    
    # ä¿å­˜ç»“æœ
    os.makedirs(output_dir, exist_ok=True)
    with open(os.path.join(output_dir, "evaluation_results.json"), 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
    generate_comparison_table(results, output_dir)
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")


def generate_comparison_table(results: Dict, output_dir: str):
    """ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼ï¼ˆLaTeX å’Œ CSVï¼‰"""
    
    # CSV æ ¼å¼
    csv_lines = ["Method,Layout IoU,Object Recall,Count Accuracy,CLIP Score"]
    for baseline_name, metrics in results.items():
        csv_lines.append(
            f"{baseline_name},"
            f"{metrics['mean_layout_iou']:.4f},"
            f"{metrics['mean_object_recall']:.4f},"
            f"{metrics['mean_count_accuracy']:.4f},"
            f"{metrics['mean_clip_score']:.4f}"
        )
    
    with open(os.path.join(output_dir, "comparison_table.csv"), 'w', encoding='utf-8') as f:
        f.write('\n'.join(csv_lines))
    
    # LaTeX æ ¼å¼
    latex_lines = [
        "\\begin{table}[h]",
        "\\centering",
        "\\begin{tabular}{lcccc}",
        "\\toprule",
        "Method & Layout IoU $\\uparrow$ & Object Recall $\\uparrow$ & Count Acc. $\\uparrow$ & CLIP Score $\\uparrow$ \\\\",
        "\\midrule"
    ]
    
    for baseline_name, metrics in results.items():
        baseline_display = {
            "base_gill": "Base GILL",
            "heuristic": "Heuristic Layout",
            "ours": "Ours",
            "gligen": "GLIGEN",
            "dalle3": "DALL-E 3"
        }.get(baseline_name, baseline_name)
        
        latex_lines.append(
            f"{baseline_display} & "
            f"{metrics['mean_layout_iou']:.3f} & "
            f"{metrics['mean_object_recall']:.3f} & "
            f"{metrics['mean_count_accuracy']:.3f} & "
            f"{metrics['mean_clip_score']:.3f} \\\\"
        )
    
    latex_lines.extend([
        "\\bottomrule",
        "\\end{tabular}",
        "\\caption{Baseline comparison results.}",
        "\\label{tab:baseline_comparison}",
        "\\end{table}"
    ])
    
    with open(os.path.join(output_dir, "comparison_table.tex"), 'w', encoding='utf-8') as f:
        f.write('\n'.join(latex_lines))
    
    print(f"âœ“ å¯¹æ¯”è¡¨æ ¼å·²ç”Ÿæˆ: {output_dir}/comparison_table.csv")


def main():
    parser = argparse.ArgumentParser(description="å¤šå±‚æ¬¡ Baseline å¯¹æ¯”è¯„ä¼°")
    parser.add_argument("--test-set", type=str, required=True,
                       help="æµ‹è¯•é›† JSONL æ–‡ä»¶")
    parser.add_argument("--output-dir", type=str, required=True,
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--baselines", type=str, nargs='+',
                       default=["base_gill", "heuristic", "ours"],
                       choices=["base_gill", "heuristic", "ours", "gligen", "dalle3"],
                       help="è¦è¯„ä¼°çš„ baseline åˆ—è¡¨")
    parser.add_argument("--metrics", type=str, nargs='+',
                       default=["layout_iou", "clip_score"],
                       choices=["layout_iou", "clip_score", "fid"],
                       help="è¯„ä¼°æŒ‡æ ‡")
    parser.add_argument("--config", type=str, default=None,
                       help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONï¼‰")
    parser.add_argument("--device", type=str, default="cuda:0",
                       help="è®¾å¤‡")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = {}
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
    
    # è®¾ç½®é»˜è®¤é…ç½®
    for baseline in args.baselines:
        if baseline not in config:
            config[baseline] = {
                "device": args.device,
                "gill_model": "./checkpoints/gill_opt",
                "planner_model": "./checkpoints/layout_planner_cot_15k"
            }
    
    evaluate_baselines(
        args.test_set,
        args.output_dir,
        args.baselines,
        args.metrics,
        config
    )


if __name__ == "__main__":
    import io
    main()

