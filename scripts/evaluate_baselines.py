#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è¯„ä¼°è„šæœ¬ï¼šBaseline å¯¹æ¯”å’Œæ¶ˆèå®éªŒ

åŠŸèƒ½ï¼š
1. å¯¹æ¯”ä¸åŒæ–¹æ³•ï¼ˆVanilla Kolors, GLIGEN, æˆ‘ä»¬çš„æ–¹æ³•ï¼‰
2. æ¶ˆèå®éªŒï¼ˆå»æ‰ Layout Planner, å»æ‰ Data Filtering, å»æ‰ Verifierï¼‰
3. é‡åŒ–æŒ‡æ ‡ï¼ˆYOLO Score, Detection Accuracy, CLIP Scoreï¼‰

è¿™æ˜¯è®ºæ–‡å®éªŒéƒ¨åˆ†çš„æ ¸å¿ƒè„šæœ¬ã€‚
"""

import argparse
import os
import json
import torch
from PIL import Image
from typing import Dict, List, Tuple
import numpy as np
from tqdm import tqdm
from pathlib import Path

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from gill.models import GILL, GILLArgs
from gill.layout_planner import LayoutPlanner
from gill.feedback_verifier import FeedbackVerifier
from gill.spatial_adapter import create_spatial_adapter_for_kolors
from scripts.inference_agent import InferenceAgent


class BaselineEvaluator:
    """
    Baseline è¯„ä¼°å™¨
    """
    
    def __init__(
        self,
        test_prompts: List[str],
        ground_truth_layouts: Optional[List[List[Dict]]] = None,
        device: str = "cuda"
    ):
        """
        Args:
            test_prompts: æµ‹è¯•æç¤ºè¯åˆ—è¡¨
            ground_truth_layouts: çœŸå®å¸ƒå±€ï¼ˆå¦‚æœæœ‰ï¼‰
            device: è®¾å¤‡
        """
        self.test_prompts = test_prompts
        self.ground_truth_layouts = ground_truth_layouts
        self.device = device
        
        # åŠ è½½æ¨¡å‹
        self._load_models()
    
    def _load_models(self):
        """åŠ è½½æ‰€æœ‰éœ€è¦çš„æ¨¡å‹"""
        print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
        # è¿™é‡ŒåŠ è½½å„ç§æ¨¡å‹
        # å®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®è·¯å¾„åŠ è½½
        pass
    
    def evaluate_vanilla_kolors(self) -> Dict:
        """è¯„ä¼° Vanilla Kolorsï¼ˆæ— å¸ƒå±€æ§åˆ¶ï¼‰"""
        print("\nğŸ” è¯„ä¼° Vanilla Kolors...")
        results = []
        
        for prompt in tqdm(self.test_prompts):
            # ä½¿ç”¨ Vanilla Kolors ç”Ÿæˆ
            # image = vanilla_kolors.generate(prompt)
            # è¯„ä¼°ç»“æœ
            result = {
                "prompt": prompt,
                "method": "Vanilla Kolors",
                # "image": image,
                # "metrics": self._calculate_metrics(image, prompt, None)
            }
            results.append(result)
        
        return self._aggregate_results(results, "Vanilla Kolors")
    
    def evaluate_gligen(self) -> Dict:
        """è¯„ä¼° GLIGENï¼ˆè‹±æ–‡ç‰ˆï¼‰"""
        print("\nğŸ” è¯„ä¼° GLIGEN...")
        results = []
        
        for prompt in tqdm(self.test_prompts):
            # ä½¿ç”¨ GLIGEN ç”Ÿæˆ
            # æ³¨æ„ï¼šGLIGEN æ˜¯è‹±æ–‡çš„ï¼Œéœ€è¦ç¿»è¯‘æˆ–ä½¿ç”¨è‹±æ–‡ prompt
            result = {
                "prompt": prompt,
                "method": "GLIGEN",
            }
            results.append(result)
        
        return self._aggregate_results(results, "GLIGEN")
    
    def evaluate_our_method(
        self,
        enable_layout: bool = True,
        enable_feedback: bool = True,
        enable_data_filtering: bool = True
    ) -> Dict:
        """
        è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼ˆæ”¯æŒæ¶ˆèå®éªŒï¼‰
        
        Args:
            enable_layout: æ˜¯å¦å¯ç”¨ Layout Planner
            enable_feedback: æ˜¯å¦å¯ç”¨ Feedback Verifier
            enable_data_filtering: æ˜¯å¦ä½¿ç”¨é«˜è´¨é‡æ•°æ®ï¼ˆæ¶ˆèå®éªŒç”¨ï¼‰
        """
        method_name = "Our Method"
        if not enable_layout:
            method_name += " (w/o Layout)"
        if not enable_feedback:
            method_name += " (w/o Feedback)"
        if not enable_data_filtering:
            method_name += " (w/o Data Filtering)"
        
        print(f"\nğŸ” è¯„ä¼° {method_name}...")
        results = []
        
        # åˆ›å»ºæ¨ç†ä»£ç†
        agent = InferenceAgent(
            device=self.device,
            max_retries=3 if enable_feedback else 1,
            enable_cot=True
        )
        
        for prompt in tqdm(self.test_prompts):
            # ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆ
            result_dict = agent.generate_with_feedback_loop(
                prompt=prompt,
                save_intermediate=False
            )
            
            result = {
                "prompt": prompt,
                "method": method_name,
                "success": result_dict["success"],
                "num_attempts": result_dict["num_attempts"],
                "final_image": result_dict["final_image"],
                "layout": result_dict.get("final_layout"),
                # "metrics": self._calculate_metrics(
                #     result_dict["final_image"],
                #     prompt,
                #     result_dict.get("final_layout")
                # )
            }
            results.append(result)
        
        return self._aggregate_results(results, method_name)
    
    def _calculate_metrics(
        self,
        image: Image.Image,
        prompt: str,
        predicted_layout: Optional[List[Dict]],
        ground_truth_layout: Optional[List[Dict]] = None
    ) -> Dict:
        """
        è®¡ç®—é‡åŒ–æŒ‡æ ‡
        
        Returns:
            {
                "clip_score": float,  # å›¾æ–‡ä¸€è‡´æ€§
                "detection_accuracy": float,  # æ£€æµ‹å‡†ç¡®ç‡ï¼ˆå¦‚æœç‰©ä½“åœ¨æŒ‡å®šä½ç½®ï¼‰
                "layout_iou": float,  # å¸ƒå±€ IoUï¼ˆå¦‚æœæœ‰ ground truthï¼‰
                "yolo_score": float  # YOLO æ£€æµ‹åˆ†æ•°
            }
        """
        metrics = {}
        
        # 1. CLIP Scoreï¼ˆå›¾æ–‡ä¸€è‡´æ€§ï¼‰
        clip_score = self._calculate_clip_score(image, prompt)
        metrics["clip_score"] = clip_score
        
        # 2. Detection Accuracyï¼ˆä½¿ç”¨ YOLO æ£€æµ‹ç‰©ä½“æ˜¯å¦åœ¨æŒ‡å®šä½ç½®ï¼‰
        if predicted_layout:
            detection_accuracy = self._calculate_detection_accuracy(
                image, predicted_layout
            )
            metrics["detection_accuracy"] = detection_accuracy
        
        # 3. Layout IoUï¼ˆå¦‚æœæœ‰ ground truthï¼‰
        if ground_truth_layout and predicted_layout:
            layout_iou = self._calculate_layout_iou(
                predicted_layout, ground_truth_layout
            )
            metrics["layout_iou"] = layout_iou
        
        return metrics
    
    def _calculate_clip_score(self, image: Image.Image, prompt: str) -> float:
        """è®¡ç®— CLIP Score"""
        try:
            from transformers import CLIPProcessor, CLIPModel
            
            # åŠ è½½ CLIP æ¨¡å‹ï¼ˆå¦‚æœè¿˜æ²¡åŠ è½½ï¼‰
            if not hasattr(self, 'clip_model'):
                self.clip_processor = CLIPProcessor.from_pretrained(
                    "openai/clip-vit-large-patch14"
                )
                self.clip_model = CLIPModel.from_pretrained(
                    "openai/clip-vit-large-patch14"
                ).to(self.device)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            inputs = self.clip_processor(
                text=[prompt],
                images=[image],
                return_tensors="pt",
                padding=True
            ).to(self.device)
            
            outputs = self.clip_model(**inputs)
            logits_per_image = outputs.logits_per_image
            probs = logits_per_image.softmax(dim=1)
            
            return probs[0, 0].item()
        except Exception as e:
            print(f"âš ï¸ CLIP Score è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _calculate_detection_accuracy(
        self,
        image: Image.Image,
        predicted_layout: List[Dict],
        iou_threshold: float = 0.5
    ) -> float:
        """
        è®¡ç®—æ£€æµ‹å‡†ç¡®ç‡
        
        ä½¿ç”¨ YOLO æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“ï¼Œç„¶åæ£€æŸ¥æ˜¯å¦åœ¨é¢„æµ‹çš„å¸ƒå±€ä½ç½®
        """
        try:
            # è¿™é‡Œéœ€è¦ä½¿ç”¨ YOLO æ£€æµ‹
            # å®é™…å®ç°æ—¶éœ€è¦åŠ è½½ YOLO æ¨¡å‹
            # detected_objects = yolo_model.detect(image)
            # 
            # ç„¶åè®¡ç®—æ¯ä¸ªé¢„æµ‹ç‰©ä½“çš„æ£€æµ‹æ¡†ä¸å¸ƒå±€æ¡†çš„ IoU
            # å¦‚æœ IoU > thresholdï¼Œåˆ™è®¤ä¸ºæ£€æµ‹æ­£ç¡®
            
            # ç®€åŒ–ç‰ˆæœ¬ï¼šè¿”å›ä¸€ä¸ªå ä½å€¼
            return 0.7  # å ä½å€¼
        except Exception as e:
            print(f"âš ï¸ Detection Accuracy è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _calculate_layout_iou(
        self,
        predicted_layout: List[Dict],
        ground_truth_layout: List[Dict]
    ) -> float:
        """è®¡ç®—å¸ƒå±€ IoU"""
        # åŒ¹é…é¢„æµ‹å’ŒçœŸå®ç‰©ä½“
        matched_pairs = []
        used_gt_indices = set()
        
        for pred_obj in predicted_layout:
            best_iou = 0
            best_gt_idx = None
            
            for gt_idx, gt_obj in enumerate(ground_truth_layout):
                if gt_idx in used_gt_indices:
                    continue
                
                # è®¡ç®— IoU
                iou = self._bbox_iou(
                    pred_obj["bbox"],
                    gt_obj["bbox"]
                )
                
                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = gt_idx
            
            if best_gt_idx is not None:
                matched_pairs.append((pred_obj, ground_truth_layout[best_gt_idx], best_iou))
                used_gt_indices.add(best_gt_idx)
        
        # è®¡ç®—å¹³å‡ IoU
        if len(matched_pairs) == 0:
            return 0.0
        
        avg_iou = sum(iou for _, _, iou in matched_pairs) / len(matched_pairs)
        return avg_iou
    
    def _bbox_iou(self, bbox1: List[float], bbox2: List[float]) -> float:
        """è®¡ç®—ä¸¤ä¸ª bbox çš„ IoU"""
        x1_1, y1_1, x2_1, y2_1 = bbox1
        x1_2, y1_2, x2_2, y2_2 = bbox2
        
        # è®¡ç®—äº¤é›†
        x1_i = max(x1_1, x1_2)
        y1_i = max(y1_1, y1_2)
        x2_i = min(x2_1, x2_2)
        y2_i = min(y2_1, y2_2)
        
        if x2_i <= x1_i or y2_i <= y1_i:
            return 0.0
        
        inter_area = (x2_i - x1_i) * (y2_i - y1_i)
        
        # è®¡ç®—å¹¶é›†
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union_area = area1 + area2 - inter_area
        
        if union_area == 0:
            return 0.0
        
        return inter_area / union_area
    
    def _aggregate_results(self, results: List[Dict], method_name: str) -> Dict:
        """èšåˆç»“æœ"""
        if len(results) == 0:
            return {}
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        if "metrics" in results[0]:
            metric_keys = results[0]["metrics"].keys()
            for key in metric_keys:
                values = [r["metrics"][key] for r in results if "metrics" in r]
                if values:
                    avg_metrics[key] = np.mean(values)
        
        # æˆåŠŸç‡
        success_rate = sum(1 for r in results if r.get("success", False)) / len(results)
        
        # å¹³å‡å°è¯•æ¬¡æ•°
        avg_attempts = np.mean([r.get("num_attempts", 1) for r in results])
        
        return {
            "method": method_name,
            "num_samples": len(results),
            "success_rate": success_rate,
            "avg_attempts": avg_attempts,
            "avg_metrics": avg_metrics,
            "detailed_results": results
        }
    
    def run_all_evaluations(self, output_dir: str = "./evaluation_results"):
        """è¿è¡Œæ‰€æœ‰è¯„ä¼°"""
        os.makedirs(output_dir, exist_ok=True)
        
        all_results = {}
        
        # 1. Baseline å¯¹æ¯”
        print("\n" + "="*60)
        print("ğŸ“Š Baseline å¯¹æ¯”å®éªŒ")
        print("="*60)
        
        all_results["vanilla_kolors"] = self.evaluate_vanilla_kolors()
        all_results["gligen"] = self.evaluate_gligen()
        all_results["our_method_full"] = self.evaluate_our_method(
            enable_layout=True,
            enable_feedback=True,
            enable_data_filtering=True
        )
        
        # 2. æ¶ˆèå®éªŒ
        print("\n" + "="*60)
        print("ğŸ”¬ æ¶ˆèå®éªŒ")
        print("="*60)
        
        all_results["ablation_no_layout"] = self.evaluate_our_method(
            enable_layout=False,
            enable_feedback=True,
            enable_data_filtering=True
        )
        all_results["ablation_no_feedback"] = self.evaluate_our_method(
            enable_layout=True,
            enable_feedback=False,
            enable_data_filtering=True
        )
        all_results["ablation_no_filtering"] = self.evaluate_our_method(
            enable_layout=True,
            enable_feedback=True,
            enable_data_filtering=False
        )
        
        # 3. ä¿å­˜ç»“æœ
        results_path = os.path.join(output_dir, "evaluation_results.json")
        with open(results_path, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
        
        # 4. æ‰“å°æ‘˜è¦
        self._print_summary(all_results)
        
        return all_results
    
    def _print_summary(self, results: Dict):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š è¯„ä¼°æ‘˜è¦")
        print("="*60)
        
        for method_name, result in results.items():
            if not result:
                continue
            
            print(f"\n{result.get('method', method_name)}:")
            print(f"  æˆåŠŸç‡: {result.get('success_rate', 0):.2%}")
            print(f"  å¹³å‡å°è¯•æ¬¡æ•°: {result.get('avg_attempts', 0):.2f}")
            
            if result.get("avg_metrics"):
                print("  å¹³å‡æŒ‡æ ‡:")
                for metric_name, value in result["avg_metrics"].items():
                    print(f"    {metric_name}: {value:.4f}")


def main():
    parser = argparse.ArgumentParser(
        description="è¯„ä¼°è„šæœ¬ï¼šBaseline å¯¹æ¯”å’Œæ¶ˆèå®éªŒ"
    )
    parser.add_argument(
        "--test_prompts_file",
        type=str,
        required=True,
        help="æµ‹è¯•æç¤ºè¯æ–‡ä»¶ï¼ˆJSONï¼Œæ¯è¡Œä¸€ä¸ª promptï¼‰"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./evaluation_results",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="è®¾å¤‡"
    )
    
    args = parser.parse_args()
    
    # åŠ è½½æµ‹è¯•æç¤ºè¯
    with open(args.test_prompts_file, "r", encoding="utf-8") as f:
        test_data = json.load(f)
        test_prompts = test_data.get("prompts", [])
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = BaselineEvaluator(
        test_prompts=test_prompts,
        device=args.device
    )
    
    # è¿è¡Œæ‰€æœ‰è¯„ä¼°
    evaluator.run_all_evaluations(output_dir=args.output_dir)


if __name__ == "__main__":
    main()

