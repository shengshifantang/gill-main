#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Wukong æ•°æ®é›†å›¾ç‰‡ä¸‹è½½è„šæœ¬ (æµå¼ä¼˜åŒ–ç‰ˆ)
åŠŸèƒ½ï¼š
 1. åˆ†å—è¯»å– CSVï¼Œç«‹å³å¼€å§‹ä¸‹è½½ï¼Œä¸å†ç­‰å¾…å¤§æ–‡ä»¶åŠ è½½
 2. å¤šçº¿ç¨‹ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°
 3. è‡ªåŠ¨ç”Ÿæˆ JSONL ç´¢å¼•æ–‡ä»¶
 4. å¢å¼ºçš„è¶…æ—¶å¤„ç†å’Œè¿›åº¦æ˜¾ç¤º
"""

import os
import json
import pandas as pd
import requests
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from pathlib import Path
import hashlib
import time

# å…¨å±€ç»Ÿè®¡
TOTAL_DOWNLOADED = 0
TOTAL_PROCESSED = 0

def download_one_image(row, save_dir, timeout=3):
    """
    ä¸‹è½½å•å¼ å›¾ç‰‡å¹¶è¿”å›å…ƒæ•°æ®
    """
    url = row.get('url')
    caption = row.get('caption')
    
    if not url or not isinstance(url, str):
        return None

    try:
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å (ä½¿ç”¨ URL hash é˜²æ­¢æ–‡ä»¶åå†²çª)
        img_hash = hashlib.md5(url.encode('utf-8')).hexdigest()
        img_filename = f"{img_hash}.jpg"
        img_path = os.path.join(save_dir, img_filename)

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”å¤§å°æ­£å¸¸ï¼Œè·³è¿‡ä¸‹è½½
        if os.path.exists(img_path) and os.path.getsize(img_path) > 1024:
            return {
                "image_path": os.path.abspath(img_path),
                "caption": caption,
                "url": url
            }

        # è¯·æ±‚å›¾ç‰‡ (è®¾ç½®æ›´çŸ­çš„è¿æ¥è¶…æ—¶)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        # timeout=(è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
        # ç¦ç”¨ä»£ç†ï¼Œé¿å…ä»£ç†æœåŠ¡æœªè¿è¡Œæ—¶å¯¼è‡´è¿æ¥å¤±è´¥
        response = requests.get(url, headers=headers, stream=True, timeout=(3, 5), proxies={'http': None, 'https': None})
        
        if response.status_code == 200:
            with open(img_path, 'wb') as f:
                for chunk in response.iter_content(1024):
                    f.write(chunk)
            
            # å†æ¬¡æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
            if os.path.getsize(img_path) > 1024:
                return {
                    "image_path": os.path.abspath(img_path),
                    "caption": caption,
                    "url": url
                }
            else:
                if os.path.exists(img_path):
                    os.remove(img_path)
    except Exception:
        # Wukong æ•°æ®é›†æ­»é“¾å¾ˆå¤šï¼Œè¿™æ˜¯æ­£å¸¸ç°è±¡ï¼Œç›´æ¥å¿½ç•¥
        pass
    
    return None

def process_chunk(chunk, args, executor, f_out, chunk_idx):
    """
    å¤„ç†ä¸€ä¸ªå°çš„æ•°æ®å—
    """
    global TOTAL_DOWNLOADED, TOTAL_PROCESSED
    
    # åˆ—åé€‚é…
    if 'url' not in chunk.columns:
        if len(chunk.columns) >= 2:
            chunk.rename(columns={chunk.columns[0]: 'url', chunk.columns[1]: 'caption'}, inplace=True)
    
    if 'text' in chunk.columns and 'caption' not in chunk.columns:
        chunk.rename(columns={'text': 'caption'}, inplace=True)
    
    records = chunk.to_dict('records')
    chunk_size = len(records)
    
    # ç«‹å³æ˜¾ç¤ºå¼€å§‹å¤„ç†çš„ä¿¡æ¯
    print(f"  ğŸ“¥ Chunk {chunk_idx}: Processing {chunk_size} URLs...", flush=True)
    
    futures = [executor.submit(download_one_image, rec, args.save_dir) for rec in records]
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
    with tqdm(total=chunk_size, desc=f"  Chunk {chunk_idx}", leave=False, ncols=80) as pbar:
        for future in as_completed(futures):
            result = future.result()
            TOTAL_PROCESSED += 1
            pbar.update(1)
            
            if result:
                f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
                TOTAL_DOWNLOADED += 1
                if TOTAL_DOWNLOADED % 50 == 0:
                    f_out.flush()
                pbar.set_postfix({"âœ…": TOTAL_DOWNLOADED, "ğŸ“Š": TOTAL_PROCESSED})
            
            # æ¯å¤„ç†100ä¸ªå°±è¾“å‡ºä¸€æ¬¡æ€»ä½“è¿›åº¦ï¼ˆåŒ…æ‹¬æˆåŠŸç‡ï¼‰
            if TOTAL_PROCESSED % 100 == 0:
                success_rate = (TOTAL_DOWNLOADED / TOTAL_PROCESSED * 100) if TOTAL_PROCESSED > 0 else 0
                print(f"\rğŸ“Š Processed: {TOTAL_PROCESSED}, Downloaded: {TOTAL_DOWNLOADED} ({success_rate:.1f}%)", end="", flush=True)
            
            if args.max_samples and TOTAL_DOWNLOADED >= args.max_samples:
                return True # Stop signal
            
    return False # Continue signal

def main(args):
    global TOTAL_DOWNLOADED, TOTAL_PROCESSED
    
    # 1. å‡†å¤‡ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    
    # 2. è¯»å–å·²æœ‰è¿›åº¦ï¼ˆæ–­ç‚¹ç»­ä¼ æ”¯æŒï¼‰
    processed_urls = set()
    if os.path.exists(args.output_jsonl):
        print(f"ğŸ“– Reading existing progress from {args.output_jsonl}...")
        try:
            with open(args.output_jsonl, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        if 'url' in data:
                            processed_urls.add(data['url'])
                    except:
                        pass
            print(f"âœ… Found {len(processed_urls)} already downloaded images.")
            TOTAL_DOWNLOADED = len(processed_urls)  # æ›´æ–°è®¡æ•°å™¨
        except Exception as e:
            print(f"âš ï¸ Warning: Could not read existing progress: {e}")
            processed_urls = set()
    
    # 3. è¯»å– CSV æ–‡ä»¶åˆ—è¡¨
    # æ”¯æŒä¸¤ç§æ–¹å¼ï¼š
    # - å¦‚æœ csv_dir æ˜¯æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
    # - å¦‚æœ csv_dir æ˜¯ç›®å½•ï¼Œé€’å½’æŸ¥æ‰¾æ‰€æœ‰ CSV æ–‡ä»¶
    if os.path.isfile(args.csv_dir):
        csv_files = [args.csv_dir]
        print(f"ğŸ“¦ Using single CSV file: {os.path.basename(args.csv_dir)}")
    else:
        csv_files = sorted([str(p) for p in Path(args.csv_dir).rglob("*.csv")])
        print(f"ğŸ“¦ Found {len(csv_files)} CSV files in {args.csv_dir}")
    
    if args.limit_csvs:
        csv_files = csv_files[:args.limit_csvs]

    # åˆ›å»ºçº¿ç¨‹æ±  (å¤ç”¨ï¼Œä¸è¦åå¤åˆ›å»ºé”€æ¯)
    executor = ThreadPoolExecutor(max_workers=args.workers)

    # --- å®‰å…¨è¡¥ä¸ï¼šç¡®ä¿æ–‡ä»¶ä»¥æ¢è¡Œç¬¦ç»“å°¾ï¼ˆé˜²æ­¢å¼‚å¸¸ä¸­æ–­å¯¼è‡´æ ¼å¼é”™è¯¯ï¼‰---
    if os.path.exists(args.output_jsonl):
        try:
            with open(args.output_jsonl, 'rb+') as f:
                f.seek(0, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
                if f.tell() > 0:  # å¦‚æœæ–‡ä»¶ä¸ä¸ºç©º
                    f.seek(-1, 2)  # ç§»åŠ¨åˆ°å€’æ•°ç¬¬ä¸€ä¸ªå­—èŠ‚
                    last_char = f.read(1)
                    if last_char != b'\n':
                        print("ğŸ”§ æ£€æµ‹åˆ°ä¸Šæ¬¡è¿è¡Œæœªæ­£å¸¸æ¢è¡Œï¼Œæ­£åœ¨è‡ªåŠ¨ä¿®å¤...")
                        f.write(b'\n')
        except Exception as e:
            print(f"âš ï¸ æ–‡ä»¶ä¿®å¤æ£€æŸ¥å¤±è´¥ (ä¸å½±å“è¿è¡Œ): {e}")
    # ------------------------------------------------------------------------

    # ä½¿ç”¨è¿½åŠ æ¨¡å¼ï¼Œé¿å…æ¸…ç©ºå·²æœ‰æ•°æ®
    file_mode = 'a' if os.path.exists(args.output_jsonl) else 'w'
    with open(args.output_jsonl, file_mode, encoding='utf-8') as f_out:
        for csv_file in csv_files:
            print(f"\nğŸš€ Processing {os.path.basename(csv_file)} in chunks...")
            
            try:
                # å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ chunksize åˆ†å—è¯»å–
                # å‡å°chunksizeåˆ°1000ï¼Œè®©è¿›åº¦æ›´å¿«å‡ºç°
                chunk_iter = pd.read_csv(csv_file, on_bad_lines='skip', chunksize=1000)
                
                chunk_idx = 0
                for chunk in chunk_iter:
                    chunk_idx += 1
                    
                    # è¿‡æ»¤æ‰å·²ç»å¤„ç†è¿‡çš„ URLï¼ˆæå‡é‡å¯åçš„é€Ÿåº¦ï¼‰
                    if 'url' in chunk.columns:
                        original_size = len(chunk)
                        chunk = chunk[~chunk['url'].isin(processed_urls)]
                        filtered_count = original_size - len(chunk)
                        if filtered_count > 0:
                            print(f"  â­ï¸  Skipped {filtered_count} already processed URLs in chunk {chunk_idx}")
                    
                    # å¦‚æœ chunk ä¸ºç©ºï¼Œè·³è¿‡
                    if chunk.empty:
                        continue
                    
                    stop = process_chunk(chunk, args, executor, f_out, chunk_idx)
                    if stop:
                        print(f"\nğŸ›‘ Reached max samples: {args.max_samples}")
                        break
                
                if args.max_samples and TOTAL_DOWNLOADED >= args.max_samples:
                    break
                    
            except Exception as e:
                print(f"âš ï¸ Error reading {csv_file}: {e}")

    executor.shutdown()
    print(f"\n\nğŸ‰ Done! Processed: {TOTAL_PROCESSED}, Downloaded: {TOTAL_DOWNLOADED} images")
    print(f"ğŸ’¾ Metadata saved to {args.output_jsonl}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--csv_dir", type=str, required=True, 
                       help="Path to CSV file or directory containing CSV files")
    parser.add_argument("--save_dir", type=str, required=True, help="Directory to save images")
    parser.add_argument("--output_jsonl", type=str, required=True, help="Output path for wukong_raw.jsonl")
    parser.add_argument("--workers", type=int, default=64, help="Number of download threads")
    parser.add_argument("--limit_csvs", type=int, default=None, help="Only process first N csv files")
    parser.add_argument("--max_samples", type=int, default=None, help="Max images to download")
    args = parser.parse_args()
    main(args)
