#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¤šGPUå¹¶è¡Œæ ‡æ³¨è„šæœ¬ï¼ˆQwen2-VL-7B-Instructï¼‰
åŠŸèƒ½ï¼š
 1. è¯»å–æœªæ ‡æ³¨çš„æ‚Ÿç©º JSONLï¼ˆæ¯è¡Œè‡³å°‘åŒ…å« image_path, captionï¼‰
 2. è¿‡æ»¤åå›¾ï¼ˆä¸å­˜åœ¨/è¿‡å°ï¼‰
 3. è°ƒç”¨æœ¬åœ° Qwen2-VL-7B-Instruct ç”Ÿæˆ bboxï¼Œè¦æ±‚è¾“å‡º JSON åˆ—è¡¨
 4. å°†ç»“æœé€è¡Œå†™å…¥æ–°çš„ JSONLï¼ˆä¿ç•™åŸå­—æ®µï¼Œæ–°å¢ objectsï¼‰
 5. æ”¯æŒå¤šGPUå¹¶è¡Œï¼ˆ3å¼ å¡ï¼‰
 6. è¯¦ç»†çš„è¿›åº¦æ˜¾ç¤ºï¼ˆå·²æ ‡æ³¨ã€å‰©ä½™ã€é€Ÿåº¦ã€é¢„è®¡æ—¶é—´ï¼‰

ç”¨æ³•ç¤ºä¾‹ï¼ˆå•å¡ï¼‰ï¼š
python scripts/annotate_wukong_qwen2vl.py \
  --input /mnt/disk/lxh/Project/gill-data/wukong_raw.jsonl \
  --image-root /mnt/disk/lxh/Project/gill-data/images \
  --output /mnt/disk/lxh/Project/gill-data/wukong_labeled.jsonl \
  --model Qwen/Qwen2-VL-7B-Instruct \
  --device cuda \
  --batch-size 4 \
  --max-samples 10000

ç”¨æ³•ç¤ºä¾‹ï¼ˆå¤šGPUï¼Œ3å¼ å¡ï¼‰ï¼š
CUDA_VISIBLE_DEVICES=0,1,2 python scripts/annotate_wukong_qwen2vl.py \
  --input /mnt/disk/lxh/Project/gill-data/wukong_raw.jsonl \
  --image-root /mnt/disk/lxh/Project/gill-data/images \
  --output /mnt/disk/lxh/Project/gill-data/wukong_labeled.jsonl \
  --model Qwen/Qwen2-VL-7B-Instruct \
  --device cuda \
  --batch-size 4 \
  --num-gpus 3
"""

import argparse
import json
import os
import re
import time
from typing import List, Dict, Any, Optional, Set
from collections import deque

import torch
from PIL import Image
from transformers import AutoProcessor
from transformers import Qwen2VLForConditionalGeneration
from tqdm import tqdm


def is_valid_image(path: str, min_size: int = 256) -> bool:
    if not os.path.exists(path):
        return False
    try:
        with Image.open(path) as img:
            w, h = img.size
            return w >= min_size and h >= min_size
    except Exception:
        return False


def build_prompt(caption: str) -> str:
    # ä»…å…³æ³¨æè¿°ä¸­æåˆ°çš„å®ä½“ï¼Œå‡å°‘èƒŒæ™¯å™ªå£°
    return (
        "æ£€æµ‹å›¾åƒä¸­ä¸æè¿°æœ€ç›¸å…³çš„ç‰©ä½“ï¼Œåªè¾“å‡ºæè¿°é‡Œå‡ºç°çš„å®ä½“ã€‚"
        "ä¸¥æ ¼è¿”å› JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ å½¢å¦‚ï¼š"
        '{"name": "ç±»åˆ«", "bbox": [x1, y1, x2, y2]}ï¼Œåæ ‡éœ€ 0-1 å½’ä¸€åŒ–ï¼›'
        "ä¸è¦æ·»åŠ é¢å¤–è§£é‡Šæˆ–æ–‡æœ¬ã€‚æè¿°ï¼š"
        f"{caption}"
    )


def parse_bboxes(text: str) -> List[Dict[str, Any]]:
    """
    å°è¯•ä»æ¨¡å‹è¾“å‡ºä¸­è§£æ JSON æ•°ç»„ï¼›è‹¥å¤±è´¥åˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚
    - æ”¯æŒåŒ…å«å¤šæ®µå†…å®¹ã€markdown ä»£ç å—ã€å¤šä¸ªæ•°ç»„ç­‰æƒ…å†µ
    """
    # å»æ‰ markdown ä»£ç å—æ ‡è®°
    text = re.sub(r"```[a-zA-Z]*", "", text)
    text = text.replace("```", "")

    # å¯èƒ½å­˜åœ¨å¤šä¸ªæ•°ç»„ï¼šä¾æ¬¡å°è¯•éè´ªå©ªåŒ¹é…çš„ [ ... ]
    for m in re.finditer(r"\[.*?\]", text, re.S):
        candidate = m.group(0)
        try:
            data = json.loads(candidate)
        except Exception:
            continue

        # åªæ¥å— list ä¸”å…¶ä¸­è‡³å°‘æœ‰ä¸€ä¸ªå¸¦ bbox çš„ dict
        if isinstance(data, list):
            valid: List[Dict[str, Any]] = []
            for obj in data:
                if (
                    isinstance(obj, dict)
                    and "bbox" in obj
                    and isinstance(obj["bbox"], list)
                    and len(obj["bbox"]) == 4
                ):
                    try:
                        bbox = [float(x) for x in obj["bbox"]]
                    except Exception:
                        continue
                    valid.append(
                        {
                            "name": obj.get("name", "object"),
                            "bbox": bbox,
                        }
                    )
            if valid:
                return valid

    # å…œåº•ï¼šæœ‰æ—¶æ¨¡å‹ç›´æ¥è¾“å‡ºå•ä¸ª dict
    try:
        data = json.loads(text)
        if (
            isinstance(data, dict)
            and "bbox" in data
            and isinstance(data["bbox"], list)
            and len(data["bbox"]) == 4
        ):
            return [
                {
                    "name": data.get("name", "object"),
                    "bbox": [float(x) for x in data["bbox"]],
                }
            ]
    except Exception:
        pass

    return []


def filter_bboxes(bboxes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """è¿‡æ»¤æ˜æ˜¾æ— æ•ˆæˆ–"å…¨å›¾"æ¡†"""
    filtered = []
    for obj in bboxes:
        name = str(obj.get("name", "")).lower()
        bbox = obj.get("bbox", [])
        if not (isinstance(bbox, list) and len(bbox) == 4):
            continue
        x1, y1, x2, y2 = bbox
        
        # åˆ¤å®šæ˜¯å¦ä¸º 0-1 æˆ– 0-1000 åæ ‡ç³»ä¸‹çš„å…¨å›¾æ¡†
        is_full_01 = (
            0.0 <= x1 <= 1.0
            and 0.0 <= y1 <= 1.0
            and 0.0 <= x2 <= 1.0
            and 0.0 <= y2 <= 1.0
            and x1 == 0.0
            and y1 == 0.0
            and x2 == 1.0
            and y2 == 1.0
        )
        is_full_1000 = (
            0.0 <= x1 <= 1000.0
            and 0.0 <= y1 <= 1000.0
            and 0.0 <= x2 <= 1000.0
            and 0.0 <= y2 <= 1000.0
            and x1 == 0.0
            and y1 == 0.0
            and x2 == 1000.0
            and y2 == 1000.0
        )
        # ä¸€äº›æ˜æ˜¾ç¼ºä¹è¯­ä¹‰å®šä½çš„ç±»åˆ«åï¼Œä¹Ÿç›´æ¥ä¸¢å¼ƒ
        if name in ["å…¨å›¾", "å›¾ç‰‡", "æ–‡å­—", "ç‰©ä½“", "äººå", "æ£€æµ‹æŠ¥å‘Š"]:
            continue
        if is_full_01 or is_full_1000:
            continue
        filtered.append({"name": obj.get("name", "object"), "bbox": bbox})
    return filtered


@torch.inference_mode()
def annotate_batch(
    model,
    processor,
    image_paths: List[str],
    captions: List[str],
    device: str = "cuda",
    max_new_tokens: int = 256,
    max_image_size: int = 1024,  # é™åˆ¶å›¾ç‰‡æœ€å¤§å°ºå¯¸ï¼Œå‡å°‘æ˜¾å­˜å ç”¨
) -> List[Optional[List[Dict[str, Any]]]]:
    batch_images = []
    texts = []
    for path, cap in zip(image_paths, captions):
        try:
            img = Image.open(path).convert("RGB")
            # é™åˆ¶å›¾ç‰‡å°ºå¯¸ï¼Œå‡å°‘æ˜¾å­˜å ç”¨
            w, h = img.size
            if w > max_image_size or h > max_image_size:
                ratio = min(max_image_size / w, max_image_size / h)
                new_w, new_h = int(w * ratio), int(h * ratio)
                img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)
        except Exception:
            batch_images.append(None)
            texts.append(None)
            continue
        prompt = build_prompt(cap)
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image", "image": img},
                ],
            }
        ]
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        batch_images.append(img)
        texts.append(text)

    # è¿‡æ»¤æ‰æŸåçš„æ ·æœ¬
    valid_idx = [i for i, t in enumerate(texts) if t is not None]
    if not valid_idx:
        return [None] * len(image_paths)

    inputs = processor(
        text=[texts[i] for i in valid_idx],
        images=[batch_images[i] for i in valid_idx],
        return_tensors="pt",
        padding=True,
    ).to(device)

    # æ¸…ç†æ˜¾å­˜ç¼“å­˜
    torch.cuda.empty_cache()
    
    output_ids = None
    generated_ids = None
    text_outputs = None
    
    try:
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            num_beams=1,
            pad_token_id=processor.tokenizer.eos_token_id,
        )
        generated_ids = [
            out[len(inp) :] for inp, out in zip(inputs["input_ids"], output_ids)
        ]
        text_outputs = processor.batch_decode(
            generated_ids, skip_special_tokens=True
        )
    except torch.cuda.OutOfMemoryError as e:
        # æ¸…ç†æ˜¾å­˜åé‡æ–°æŠ›å‡ºå¼‚å¸¸
        if output_ids is not None:
            del output_ids
        if generated_ids is not None:
            del generated_ids
        if text_outputs is not None:
            del text_outputs
        del inputs
        torch.cuda.empty_cache()
        raise e
    finally:
        # æ¸…ç†è¾“å…¥å’Œä¸­é—´å˜é‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'inputs' in locals():
            del inputs
        if output_ids is not None:
            del output_ids
        if generated_ids is not None:
            del generated_ids
        torch.cuda.empty_cache()

    if text_outputs is None:
        # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ
        return [None] * len(image_paths)

    parsed_results = [None] * len(image_paths)
    for idx, out in zip(valid_idx, text_outputs):
        parsed_results[idx] = parse_bboxes(out)
    
    # æ¸…ç†æ–‡æœ¬è¾“å‡º
    del text_outputs
    torch.cuda.empty_cache()
    
    return parsed_results


def load_models_multi_gpu(model_path: str, num_gpus: int = 3):
    """åœ¨å¤šGPUä¸ŠåŠ è½½æ¨¡å‹"""
    models = []
    processors = []
    devices = []
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
    is_local = os.path.exists(model_path) and os.path.isdir(model_path)
    
    print(f"ğŸš€ åœ¨ {num_gpus} å¼  GPU ä¸ŠåŠ è½½æ¨¡å‹ {model_path} ...")
    if is_local:
        print(f"  ğŸ“¦ æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨ local_files_only=True")
    else:
        print(f"  ğŸ“¦ ä» HuggingFace åŠ è½½æ¨¡å‹")
    
    for i in range(num_gpus):
        device = f"cuda:{i}"
        print(f"  ğŸ“¦ åŠ è½½åˆ° {device} ...")
        
        # åŠ è½½æ¨¡å‹
        if is_local:
            model = Qwen2VLForConditionalGeneration.from_pretrained(
                model_path,
                device_map=device,
                torch_dtype="auto",
                local_files_only=True,
                trust_remote_code=True,
            )
            processor = AutoProcessor.from_pretrained(
                model_path,
                local_files_only=True,
                trust_remote_code=True,
            )
        else:
            model = Qwen2VLForConditionalGeneration.from_pretrained(
                model_path,
                device_map=device,
                torch_dtype="auto",
                trust_remote_code=True,
            )
            processor = AutoProcessor.from_pretrained(
                model_path,
                trust_remote_code=True,
            )
        
        models.append(model)
        processors.append(processor)
        devices.append(device)
        print(f"  âœ… {device} åŠ è½½å®Œæˆ")
    
    return models, processors, devices


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True, help="æœªæ ‡æ³¨çš„ JSONL è·¯å¾„")
    parser.add_argument("--output", required=True, help="è¾“å‡º JSONL è·¯å¾„")
    parser.add_argument("--image-root", required=True, help="å›¾ç‰‡æ ¹ç›®å½•")
    parser.add_argument(
        "--model", default="Qwen/Qwen2-VL-7B-Instruct", help="æ¨¡å‹åç§°æˆ–è·¯å¾„"
    )
    parser.add_argument("--device", default="cuda")
    parser.add_argument("--max-samples", type=int, default=None)
    parser.add_argument("--min-size", type=int, default=256)
    parser.add_argument("--batch-size", type=int, default=2, help="æ‰¹æ¬¡å¤§å°ï¼ˆå¤šGPUæ—¶å»ºè®®1-2ï¼Œå•GPUå¯æ›´å¤§ï¼‰")
    parser.add_argument("--num-gpus", type=int, default=1, help="ä½¿ç”¨çš„GPUæ•°é‡ï¼ˆé»˜è®¤1ï¼Œæ”¯æŒå¤šGPUå¹¶è¡Œï¼‰")
    parser.add_argument("--save-interval", type=int, default=50, help="æ¯å¤„ç†å¤šå°‘æ¡ä¿å­˜ä¸€æ¬¡è¿›åº¦ä¿¡æ¯")
    parser.add_argument("--max-image-size", type=int, default=1024, help="å›¾ç‰‡æœ€å¤§å°ºå¯¸ï¼ˆåƒç´ ï¼‰ï¼Œè¶…è¿‡ä¼šç¼©æ”¾ï¼Œå‡å°‘æ˜¾å­˜å ç”¨")
    args = parser.parse_args()

    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
    is_local = os.path.exists(args.model) and os.path.isdir(args.model)
    
    # å¤šGPUæ”¯æŒ
    if args.num_gpus > 1:
        models, processors, devices = load_models_multi_gpu(args.model, args.num_gpus)
        current_gpu = 0  # è½®è¯¢ä½¿ç”¨GPU
    else:
        device = args.device
        print(f"ğŸš€ åŠ è½½æ¨¡å‹ {args.model} åˆ° {device} ...")
        if is_local:
            print(f"  ğŸ“¦ æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨ local_files_only=True")
            model = Qwen2VLForConditionalGeneration.from_pretrained(
                args.model,
                device_map=device,
                torch_dtype="auto",
                local_files_only=True,
                trust_remote_code=True,
            )
            processor = AutoProcessor.from_pretrained(
                args.model,
                local_files_only=True,
                trust_remote_code=True,
            )
        else:
            model = Qwen2VLForConditionalGeneration.from_pretrained(
                args.model,
                device_map=device,
                torch_dtype="auto",
                trust_remote_code=True,
            )
            processor = AutoProcessor.from_pretrained(
                args.model,
                trust_remote_code=True,
            )
        models = [model]
        processors = [processor]
        devices = [device]

    # æ–­ç‚¹ç»­ä¼ ï¼šè¯»å–å·²æœ‰è¾“å‡ºçš„ image_path é›†åˆ
    processed: Set[str] = set()
    initial_success = 0
    if os.path.exists(args.output):
        print(f"ğŸ“– è¯»å–å·²æœ‰è¾“å‡ºæ–‡ä»¶: {args.output}")
        with open(args.output, "r", encoding="utf-8") as fexist:
            for line in fexist:
                try:
                    obj = json.loads(line)
                    p = obj.get("image_path") or obj.get("image")
                    if p:
                        processed.add(p)
                        initial_success += 1
                except Exception:
                    continue
        fout_mode = "a"
        print(f"ğŸ§© æ£€æµ‹åˆ°å·²å­˜åœ¨è¾“å‡ºï¼Œå·²æ ‡æ³¨ {initial_success} æ¡ï¼Œç»§ç»­è¿½åŠ å†™å…¥ã€‚")
    else:
        fout_mode = "w"
        print(f"ğŸ“ åˆ›å»ºæ–°è¾“å‡ºæ–‡ä»¶: {args.output}")

    # ç»Ÿè®¡æ€»è¡Œæ•°ï¼ˆç”¨äºè¿›åº¦æ¡ï¼‰
    print("ğŸ“Š ç»Ÿè®¡è¾“å…¥æ–‡ä»¶æ€»è¡Œæ•°...")
    total_lines = 0
    with open(args.input, "r", encoding="utf-8") as fin:
        for _ in fin:
            total_lines += 1
    print(f"   æ€»è¡Œæ•°: {total_lines}")

    total, success = 0, initial_success
    os.makedirs(os.path.dirname(args.output) if os.path.dirname(args.output) else ".", exist_ok=True)
    buffer_items = []
    
    # è¿›åº¦ç»Ÿè®¡
    start_time = time.time()
    speed_history = deque(maxlen=100)  # è®°å½•æœ€è¿‘100ä¸ªæ‰¹æ¬¡çš„é€Ÿåº¦
    
    # æ‰“å¼€æ–‡ä»¶
    fin = open(args.input, "r", encoding="utf-8")
    fout = open(args.output, fout_mode, encoding="utf-8")
    
    try:
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        pbar = tqdm(total=total_lines, desc="æ ‡æ³¨è¿›åº¦", unit="è¡Œ", initial=len(processed))
        
        for line_num, line in enumerate(fin, 1):
            if not line.strip():
                pbar.update(1)
                continue
            try:
                item = json.loads(line)
            except Exception:
                pbar.update(1)
                continue

            rel_path = item.get("image_path") or item.get("image")
            caption = item.get("caption") or ""
            if not rel_path:
                pbar.update(1)
                continue
            if rel_path in processed:
                pbar.update(1)
                continue

            full_path = rel_path
            if not os.path.isabs(full_path):
                full_path = os.path.join(args.image_root, rel_path)
            if not is_valid_image(full_path, min_size=args.min_size):
                pbar.update(1)
                continue

            buffer_items.append((item, full_path, rel_path, caption))
            
            # æ»¡æ‰¹å¤„ç†
            if len(buffer_items) >= args.batch_size:
                batch_start_time = time.time()
                total += len(buffer_items)
                
                # é€‰æ‹©GPUï¼ˆå¤šGPUæ—¶è½®è¯¢ï¼‰
                if args.num_gpus > 1:
                    gpu_idx = current_gpu % args.num_gpus
                    current_gpu += 1
                    model = models[gpu_idx]
                    processor = processors[gpu_idx]
                    device = devices[gpu_idx]
                else:
                    model = models[0]
                    processor = processors[0]
                    device = devices[0]
                
                imgs = [x[1] for x in buffer_items]
                caps = [x[3] for x in buffer_items]
                
                try:
                    results = annotate_batch(
                        model, processor, imgs, caps, device=device,
                        max_image_size=args.max_image_size
                    )
                except torch.cuda.OutOfMemoryError as e:
                    print(f"\nâŒ GPUæ˜¾å­˜ä¸è¶³ï¼å»ºè®®ï¼š")
                    print(f"   1. å‡å°‘æ‰¹æ¬¡å¤§å°ï¼š--batch-size 1")
                    print(f"   2. ä½¿ç”¨æ›´å°‘çš„GPUï¼š--num-gpus 2 æˆ– --num-gpus 1")
                    print(f"   3. å‡å°å›¾ç‰‡å°ºå¯¸ï¼š--max-image-size 512")
                    print(f"   4. æ¸…ç†å…¶ä»–å ç”¨æ˜¾å­˜çš„è¿›ç¨‹")
                    raise e
                
                batch_count = 0
                for (itm, _, relp, _), bboxes in zip(buffer_items, results):
                    if not bboxes:
                        continue
                    
                    filtered = filter_bboxes(bboxes)
                    if not filtered:
                        continue
                    
                    itm["objects"] = filtered
                    fout.write(json.dumps(itm, ensure_ascii=False) + "\n")
                    fout.flush()  # ç«‹å³åˆ·æ–°åˆ°ç£ç›˜
                    processed.add(relp)
                    success += 1
                    batch_count += 1
                
                # æ¸…ç†æ‰¹æ¬¡æ•°æ®
                del results
                del imgs
                del caps
                torch.cuda.empty_cache()
                
                # æ›´æ–°è¿›åº¦ç»Ÿè®¡
                batch_time = time.time() - batch_start_time
                batch_speed = len(buffer_items) / batch_time if batch_time > 0 else 0
                speed_history.append(batch_speed)
                avg_speed = sum(speed_history) / len(speed_history) if speed_history else 0
                
                buffer_items = []
                pbar.update(1)
                
                # å®šæœŸè¾“å‡ºè¯¦ç»†è¿›åº¦
                if success % args.save_interval == 0:
                    elapsed = time.time() - start_time
                    remaining = (total_lines - len(processed)) / avg_speed if avg_speed > 0 else 0
                    
                    progress_info = (
                        f"âœ… å·²æ ‡æ³¨: {success} | "
                        f"å‰©ä½™: {total_lines - len(processed)} | "
                        f"é€Ÿåº¦: {avg_speed:.2f} æ ·æœ¬/ç§’ | "
                        f"å·²ç”¨: {elapsed/3600:.1f}h | "
                        f"é¢„è®¡å‰©ä½™: {remaining/3600:.1f}h"
                    )
                    if args.num_gpus > 1:
                        progress_info += f" | GPU: {args.num_gpus}å¼ å¹¶è¡Œ"
                    print(f"\n{progress_info}")
                    pbar.set_postfix({
                        "å·²æ ‡æ³¨": success,
                        "é€Ÿåº¦": f"{avg_speed:.2f}/s",
                        "å‰©ä½™": f"{(remaining/3600):.1f}h"
                    })
                
                if args.max_samples and success >= args.max_samples:
                    break

        # å¤„ç†æ®‹ç•™ä¸è¶³ä¸€æ‰¹çš„æ ·æœ¬
        if buffer_items and (not args.max_samples or success < args.max_samples):
            batch_start_time = time.time()
            total += len(buffer_items)
            
            if args.num_gpus > 1:
                gpu_idx = current_gpu % args.num_gpus
                model = models[gpu_idx]
                processor = processors[gpu_idx]
                device = devices[gpu_idx]
            else:
                model = models[0]
                processor = processors[0]
                device = devices[0]
            
            imgs = [x[1] for x in buffer_items]
            caps = [x[3] for x in buffer_items]
            
            try:
                results = annotate_batch(
                    model, processor, imgs, caps, device=device,
                    max_image_size=args.max_image_size
                )
            except torch.cuda.OutOfMemoryError as e:
                print(f"\nâŒ GPUæ˜¾å­˜ä¸è¶³ï¼å»ºè®®ï¼š")
                print(f"   1. å‡å°‘æ‰¹æ¬¡å¤§å°ï¼š--batch-size 1")
                print(f"   2. ä½¿ç”¨æ›´å°‘çš„GPUï¼š--num-gpus 2 æˆ– --num-gpus 1")
                print(f"   3. å‡å°å›¾ç‰‡å°ºå¯¸ï¼š--max-image-size 512")
                print(f"   4. æ¸…ç†å…¶ä»–å ç”¨æ˜¾å­˜çš„è¿›ç¨‹")
                raise e
            
            for (itm, _, relp, _), bboxes in zip(buffer_items, results):
                if not bboxes:
                    continue
                
                filtered = filter_bboxes(bboxes)
                if not filtered:
                    continue
                
                itm["objects"] = filtered
                fout.write(json.dumps(itm, ensure_ascii=False) + "\n")
                fout.flush()
                processed.add(relp)
                success += 1
            
            pbar.update(1)
        
        pbar.close()
        
    finally:
        fin.close()
        fout.close()

    elapsed = time.time() - start_time
    print(f"\nâœ… æ ‡æ³¨å®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"   æˆåŠŸæ ‡æ³¨: {success} æ¡")
    print(f"   æ€»å¤„ç†: {total} æ¡")
    print(f"   æˆåŠŸç‡: {success/total*100:.1f}%" if total > 0 else "   æˆåŠŸç‡: N/A")
    print(f"   æ€»è€—æ—¶: {elapsed/3600:.2f} å°æ—¶")
    print(f"   å¹³å‡é€Ÿåº¦: {success/elapsed:.2f} æ ·æœ¬/ç§’" if elapsed > 0 else "   å¹³å‡é€Ÿåº¦: N/A")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {args.output}")


if __name__ == "__main__":
    main()
