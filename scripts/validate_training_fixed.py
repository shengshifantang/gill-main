#!/usr/bin/env python3
"""
ç¬¬ä¸€é˜¶æ®µè®­ç»ƒéªŒè¯è„šæœ¬ - ä¿®å¤ç‰ˆ

ä¿®å¤å†…å®¹ï¼š
1. âœ… ä¿®å¤ negative_prompt é—®é¢˜
2. âœ… æ·»åŠ å¼‚å¸¸å¤„ç†ï¼Œé¿å…å•ä¸ªæ ·æœ¬å¤±è´¥å¯¼è‡´æ•´ä½“ä¸­æ–­
3. âœ… æ·»åŠ å›¾ç‰‡æœ‰æ•ˆæ€§æ£€æŸ¥
4. âœ… é™ä½åˆ†è¾¨ç‡ä»¥èŠ‚çœæ˜¾å­˜ï¼ˆ512x512ï¼‰
"""

import sys
import os
import torch
import json
import numpy as np
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
from collections import defaultdict
import types

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from gill.spatial_adapter_fixed import (
    load_spatial_adapter_state_dict,
    inject_spatial_control_to_unet,
    remove_spatial_control_from_unet,
)
from diffusers import KolorsPipeline


def analyze_gate_values(state_dict):
    """åˆ†æ Gate å‚æ•°çš„ç»Ÿè®¡ä¿¡æ¯"""
    gate_values = []
    gate_info = []
    
    for name, param in state_dict.items():
        if "gate" in name.lower():
            val = param.detach().cpu().float()
            gate_values.append(val.flatten())
            gate_info.append({
                "name": name,
                "shape": list(param.shape),
                "mean": float(val.mean()),
                "std": float(val.std()),
                "min": float(val.min()),
                "max": float(val.max()),
                "tanh_mean": float(torch.tanh(val).mean()),
                "tanh_std": float(torch.tanh(val).std()),
            })
    
    if gate_values:
        all_gates = torch.cat(gate_values).numpy()
        all_gates_tanh = np.tanh(all_gates)
        
        return {
            "gate_info": gate_info,
            "statistics": {
                "total_gates": len(gate_values),
                "raw_mean": float(all_gates.mean()),
                "raw_std": float(all_gates.std()),
                "raw_min": float(all_gates.min()),
                "raw_max": float(all_gates.max()),
                "tanh_mean": float(all_gates_tanh.mean()),
                "tanh_std": float(all_gates_tanh.std()),
                "tanh_min": float(all_gates_tanh.min()),
                "tanh_max": float(all_gates_tanh.max()),
                "near_zero_ratio": float((np.abs(all_gates) < 0.01).sum() / len(all_gates)),
                "saturated_ratio": float((np.abs(all_gates_tanh) > 0.9).sum() / len(all_gates_tanh)),
            }
        }
    return None


def visualize_gate_distribution(gate_analysis, output_path):
    """å¯è§†åŒ– Gate åˆ†å¸ƒ"""
    if gate_analysis is None:
        return
    
    stats = gate_analysis["statistics"]
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle("Gate Parameter Analysis", fontsize=16, fontweight="bold")
    
    # 1. ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬
    ax = axes[0, 0]
    ax.axis("off")
    info_text = f"""
Gate Statistics:

Raw Values:
  Mean: {stats['raw_mean']:.4f}
  Std:  {stats['raw_std']:.4f}
  Range: [{stats['raw_min']:.4f}, {stats['raw_max']:.4f}]

After Tanh:
  Mean: {stats['tanh_mean']:.4f}
  Std:  {stats['tanh_std']:.4f}
  Range: [{stats['tanh_min']:.4f}, {stats['tanh_max']:.4f}]

Health Metrics:
  Near-zero ratio: {stats['near_zero_ratio']*100:.2f}%
  Saturated ratio: {stats['saturated_ratio']*100:.2f}%
  Total gates: {stats['total_gates']}
"""
    ax.text(0.1, 0.5, info_text, fontsize=11, family="monospace", va="center")
    
    # 2. å„å±‚ Gate å€¼å¯¹æ¯”
    ax = axes[0, 1]
    gate_info = gate_analysis["gate_info"]
    layer_names = [g["name"].split(".")[-2] if "." in g["name"] else g["name"] for g in gate_info]
    gate_means = [g["tanh_mean"] for g in gate_info]
    
    ax.barh(range(len(layer_names)), gate_means, color="steelblue")
    ax.set_yticks(range(len(layer_names)))
    ax.set_yticklabels(layer_names, fontsize=8)
    ax.set_xlabel("Tanh(Gate) Mean")
    ax.set_title("Gate Values by Layer")
    ax.axvline(0, color="red", linestyle="--", linewidth=1)
    ax.grid(axis="x", alpha=0.3)
    
    # 3. Gate åŸå§‹å€¼åˆ†å¸ƒ
    ax = axes[1, 0]
    raw_values = [g["mean"] for g in gate_info]
    ax.hist(raw_values, bins=20, color="coral", edgecolor="black", alpha=0.7)
    ax.set_xlabel("Raw Gate Value")
    ax.set_ylabel("Frequency")
    ax.set_title("Raw Gate Distribution")
    ax.axvline(0, color="red", linestyle="--", linewidth=2, label="Initial=0")
    ax.legend()
    
    # 4. å¥åº·åº¦è¯„ä¼°
    ax = axes[1, 1]
    categories = ["Near Zero\n(Not Learned)", "Normal\n(Healthy)", "Saturated\n(Overfitting)"]
    values = [
        stats['near_zero_ratio'] * 100,
        (1 - stats['near_zero_ratio'] - stats['saturated_ratio']) * 100,
        stats['saturated_ratio'] * 100,
    ]
    colors = ["red", "green", "orange"]
    
    ax.pie(values, labels=categories, colors=colors, autopct="%1.1f%%", startangle=90)
    ax.set_title("Gate Health Distribution")
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches="tight")
    plt.close()
    
    print(f"âœ“ Gate åˆ†æå›¾ä¿å­˜åˆ°: {output_path}")


def draw_bboxes_on_image(image, bboxes, labels=None, color=(255, 0, 0), width=3):
    """åœ¨å›¾åƒä¸Šç»˜åˆ¶ bbox"""
    draw = ImageDraw.Draw(image)
    W, H = image.size
    
    for i, bbox in enumerate(bboxes):
        x1, y1, x2, y2 = bbox
        # è½¬æ¢ä¸ºåƒç´ åæ ‡
        x1, y1, x2, y2 = int(x1 * W), int(y1 * H), int(x2 * W), int(y2 * H)
        
        # ç»˜åˆ¶çŸ©å½¢
        draw.rectangle([x1, y1, x2, y2], outline=color, width=width)
        
        # ç»˜åˆ¶æ ‡ç­¾
        if labels and i < len(labels):
            label = labels[i]
            draw.text((x1 + 5, y1 + 5), label, fill=color)
    
    return image


def check_image_valid(image):
    """æ£€æŸ¥å›¾ç‰‡æ˜¯å¦æœ‰æ•ˆï¼ˆéå…¨é»‘ï¼‰"""
    img_array = np.array(image)
    mean_val = img_array.mean()
    return mean_val > 1.0  # å¦‚æœå‡å€¼å°äº1ï¼Œè¯´æ˜å‡ ä¹å…¨é»‘


def generate_comparison(pipeline, caption, bboxes, obj_names, device, adapter_container=None, resolution=512):
    """ç”Ÿæˆå¯¹æ¯”å›¾ï¼šæ—  Adapter vs æœ‰ Adapter"""
    results = {}
    
    # 1. æ—  Adapter ç”Ÿæˆï¼ˆBaselineï¼‰
    print("  ç”Ÿæˆ Baselineï¼ˆæ—  Adapterï¼‰...")
    try:
        with torch.cuda.amp.autocast(dtype=torch.float16):
            image_baseline = pipeline(
                prompt=caption,
                negative_prompt="ä½è´¨é‡ï¼Œæ¨¡ç³Šï¼Œå˜å½¢ï¼Œä¸‘é™‹",
                num_inference_steps=30,
                guidance_scale=5.0,
                height=resolution,
                width=resolution,
            ).images[0]
        
        if check_image_valid(image_baseline):
            results["baseline"] = image_baseline
            print("    âœ“ Baseline ç”ŸæˆæˆåŠŸ")
        else:
            print("    âš ï¸  Baseline å›¾ç‰‡å¼‚å¸¸ï¼ˆå‡ ä¹å…¨é»‘ï¼‰")
            results["baseline"] = image_baseline  # ä»ç„¶ä¿å­˜ï¼Œç”¨äºè°ƒè¯•
            
    except Exception as e:
        print(f"    âŒ Baseline ç”Ÿæˆå¤±è´¥: {e}")
        return results
    
    # 2. æœ‰ Adapter ç”Ÿæˆ
    if adapter_container is not None:
        print("  ç”Ÿæˆ Adapter æ§åˆ¶å›¾...")
        try:
            bboxes_tensor = torch.tensor([bboxes], device=device, dtype=torch.float32)
            
            orig_procs, _, _ = inject_spatial_control_to_unet(
                pipeline.unet,
                adapter_dict=adapter_container,
                bboxes=bboxes_tensor,
                phrase_embeddings=None,
                masks=None,
                adapter_dtype=torch.float16,
            )
            
            with torch.cuda.amp.autocast(dtype=torch.float16):
                image_adapter = pipeline(
                    prompt=caption,
                    negative_prompt="ä½è´¨é‡ï¼Œæ¨¡ç³Šï¼Œå˜å½¢ï¼Œä¸‘é™‹",
                    num_inference_steps=30,
                    guidance_scale=5.0,
                    height=resolution,
                    width=resolution,
                ).images[0]
            
            remove_spatial_control_from_unet(pipeline.unet, orig_procs)
            
            if check_image_valid(image_adapter):
                results["adapter"] = image_adapter
                print("    âœ“ Adapter ç”ŸæˆæˆåŠŸ")
            else:
                print("    âš ï¸  Adapter å›¾ç‰‡å¼‚å¸¸ï¼ˆå‡ ä¹å…¨é»‘ï¼‰")
                results["adapter"] = image_adapter
            
            # 3. ç»˜åˆ¶ bbox æ ‡æ³¨
            image_annotated = image_adapter.copy()
            image_annotated = draw_bboxes_on_image(image_annotated, bboxes, obj_names)
            results["annotated"] = image_annotated
            
        except Exception as e:
            print(f"    âŒ Adapter ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    return results


def validate_checkpoint(
    checkpoint_path,
    output_dir,
    kolors_path="./model/Kolors",
    test_data_path="./data/coco2014_cn_val_clean.jsonl",
    num_samples=5,
    device="cuda:0",
    resolution=512,
):
    """å®Œæ•´éªŒè¯æµç¨‹"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("ç¬¬ä¸€é˜¶æ®µè®­ç»ƒéªŒè¯ï¼ˆä¿®å¤ç‰ˆï¼‰")
    print("=" * 60)
    print(f"Checkpoint: {checkpoint_path}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"åˆ†è¾¨ç‡: {resolution}x{resolution}")
    print()
    
    # ==================== 1. åŠ è½½ Checkpoint ====================
    print("ğŸ”§ åŠ è½½ Checkpoint...")
    state_dict = torch.load(checkpoint_path, map_location="cpu")
    if "adapter" in state_dict:
        state_dict = state_dict["adapter"]
    
    print(f"âœ“ Checkpoint åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(state_dict)} ä¸ªå‚æ•°")
    print()
    
    # ==================== 2. Gate åˆ†æ ====================
    print("ğŸ“Š åˆ†æ Gate å‚æ•°...")
    gate_analysis = analyze_gate_values(state_dict)
    
    if gate_analysis:
        stats = gate_analysis["statistics"]
        print(f"  æ€» Gate æ•°: {stats['total_gates']}")
        print(f"  Tanh(Gate) å‡å€¼: {stats['tanh_mean']:.4f}")
        print(f"  Tanh(Gate) æ ‡å‡†å·®: {stats['tanh_std']:.4f}")
        print(f"  æ¥è¿‘é›¶æ¯”ä¾‹: {stats['near_zero_ratio']*100:.2f}%")
        print(f"  é¥±å’Œæ¯”ä¾‹: {stats['saturated_ratio']*100:.2f}%")
        
        # å¥åº·åº¦åˆ¤æ–­
        if stats['near_zero_ratio'] > 0.8:
            print("  âš ï¸  è­¦å‘Šï¼šè¶…è¿‡ 80% çš„ Gate æ¥è¿‘é›¶ï¼Œå¯èƒ½æœªå……åˆ†è®­ç»ƒï¼")
        elif stats['saturated_ratio'] > 0.5:
            print("  âš ï¸  è­¦å‘Šï¼šè¶…è¿‡ 50% çš„ Gate é¥±å’Œï¼Œå¯èƒ½è¿‡æ‹Ÿåˆï¼")
        else:
            print("  âœ… Gate å‚æ•°å¥åº·")
        
        # ä¿å­˜è¯¦ç»†ä¿¡æ¯
        gate_json_path = output_dir / "gate_analysis.json"
        with open(gate_json_path, "w", encoding="utf-8") as f:
            json.dump(gate_analysis, f, indent=2, ensure_ascii=False)
        print(f"  âœ“ è¯¦ç»†åˆ†æä¿å­˜åˆ°: {gate_json_path}")
        
        # å¯è§†åŒ–
        visualize_gate_distribution(gate_analysis, output_dir / "gate_distribution.png")
    else:
        print("  âš ï¸  æœªæ‰¾åˆ° Gate å‚æ•°")
    print()
    
    # ==================== 3. ç”Ÿæˆæ•ˆæœæµ‹è¯• ====================
    print("ğŸ¨ æµ‹è¯•ç”Ÿæˆæ•ˆæœ...")
    print("ğŸ”§ åŠ è½½ Kolors Pipeline...")
    
    pipeline = KolorsPipeline.from_pretrained(
        kolors_path,
        torch_dtype=torch.float16,
        trust_remote_code=True,
    ).to(device)
    
    print("âœ“ Pipeline åŠ è½½æˆåŠŸ")
    
    # åŠ è½½ Adapter
    print("ğŸ”§ åŠ è½½ Adapter...")
    adapter_container = load_spatial_adapter_state_dict(
        state_dict,
        device=device,
        dtype=torch.float16
    )
    print("âœ“ Adapter åŠ è½½æˆåŠŸ")
    print()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print(f"ğŸ”§ åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")
    with open(test_data_path, "r", encoding="utf-8") as f:
        test_data = [json.loads(line) for line in f][:num_samples]
    print(f"âœ“ åŠ è½½ {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    print()
    
    # ç”Ÿæˆå¯¹æ¯”å›¾
    success_count = 0
    for i, sample in enumerate(test_data):
        try:
            caption = sample.get("caption", "")
            
            # æå– bbox
            bboxes_list = []
            obj_names = []
            if "bboxes" in sample and sample["bboxes"]:
                bboxes_list = sample["bboxes"][:5]  # é™åˆ¶æœ€å¤š5ä¸ªbbox
                if "objects" in sample:
                    obj_names = [obj.get("name", "") for obj in sample["objects"]][:5]
            elif "objects" in sample and sample["objects"]:
                for obj in sample["objects"][:5]:
                    if "bbox" in obj:
                        bboxes_list.append(obj["bbox"])
                        obj_names.append(obj.get("name", ""))
            
            if not bboxes_list:
                print(f"[{i+1}/{len(test_data)}] è·³è¿‡ï¼ˆæ—  bboxï¼‰")
                continue
            
            print(f"[{i+1}/{len(test_data)}] {caption[:60]}...")
            print(f"  BBox æ•°é‡: {len(bboxes_list)}")
            
            # ç”Ÿæˆå¯¹æ¯”
            results = generate_comparison(
                pipeline, caption, bboxes_list, obj_names, device, adapter_container, resolution
            )
            
            if not results:
                print(f"  âš ï¸  ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡")
                continue
            
            # ä¿å­˜ç»“æœ
            sample_dir = output_dir / f"sample_{i:02d}"
            sample_dir.mkdir(exist_ok=True)
            
            # ä¿å­˜å…ƒæ•°æ®
            meta = {
                "caption": caption,
                "bboxes": bboxes_list,
                "obj_names": obj_names,
            }
            with open(sample_dir / "metadata.json", "w", encoding="utf-8") as f:
                json.dump(meta, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜å›¾åƒ
            if "baseline" in results:
                results["baseline"].save(sample_dir / "baseline.png")
            if "adapter" in results:
                results["adapter"].save(sample_dir / "adapter.png")
            if "annotated" in results:
                results["annotated"].save(sample_dir / "annotated.png")
            
            # åˆ›å»ºå¯¹æ¯”å›¾
            if "baseline" in results and "adapter" in results:
                fig, axes = plt.subplots(1, 3, figsize=(18, 6))
                fig.suptitle(caption[:80], fontsize=12, fontweight="bold")
                
                axes[0].imshow(results["baseline"])
                axes[0].set_title("Baseline (No Adapter)")
                axes[0].axis("off")
                
                axes[1].imshow(results["adapter"])
                axes[1].set_title("With Adapter")
                axes[1].axis("off")
                
                axes[2].imshow(results["annotated"])
                axes[2].set_title("Annotated (Target Positions)")
                axes[2].axis("off")
                
                plt.tight_layout()
                plt.savefig(sample_dir / "comparison.png", dpi=150, bbox_inches="tight")
                plt.close()
            
            print(f"  âœ“ ä¿å­˜åˆ°: {sample_dir}")
            success_count += 1
            
        except Exception as e:
            print(f"  âŒ å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print()
    print("=" * 60)
    print("éªŒè¯å®Œæˆï¼")
    print("=" * 60)
    print(f"æˆåŠŸæ ·æœ¬: {success_count}/{len(test_data)}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="éªŒè¯ç¬¬ä¸€é˜¶æ®µè®­ç»ƒæ•ˆæœï¼ˆä¿®å¤ç‰ˆï¼‰")
    parser.add_argument("--checkpoint", type=str, required=True, help="Checkpoint è·¯å¾„")
    parser.add_argument("--output-dir", type=str, required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--kolors-path", type=str, default="./model/Kolors", help="Kolors æ¨¡å‹è·¯å¾„")
    parser.add_argument("--test-data", type=str, default="./data/coco2014_cn_val_clean.jsonl", help="æµ‹è¯•æ•°æ®è·¯å¾„")
    parser.add_argument("--num-samples", type=int, default=5, help="æµ‹è¯•æ ·æœ¬æ•°")
    parser.add_argument("--device", type=str, default="cuda:0", help="è®¾å¤‡")
    parser.add_argument("--resolution", type=int, default=512, help="ç”Ÿæˆåˆ†è¾¨ç‡")
    
    args = parser.parse_args()
    
    validate_checkpoint(
        checkpoint_path=args.checkpoint,
        output_dir=args.output_dir,
        kolors_path=args.kolors_path,
        test_data_path=args.test_data,
        num_samples=args.num_samples,
        device=args.device,
        resolution=args.resolution,
    )

