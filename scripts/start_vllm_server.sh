#!/bin/bash
# vLLM API Server å¯åŠ¨è„šæœ¬
# ç”¨äº Qwen2.5-VL-32B + 3x4090 ç¯å¢ƒ

# è®¾ç½® GPU
export CUDA_VISIBLE_DEVICES=0,1,2

# æ¨¡å‹è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
MODEL_PATH="${MODEL_PATH:-/root/models/Qwen2.5-VL-32B-Instruct-AWQ}"

# æ£€æŸ¥æ¨¡å‹è·¯å¾„
if [ ! -d "$MODEL_PATH" ]; then
    echo "âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: $MODEL_PATH"
    echo "è¯·è®¾ç½® MODEL_PATH ç¯å¢ƒå˜é‡æˆ–ä¿®æ”¹è„šæœ¬ä¸­çš„è·¯å¾„"
    exit 1
fi

echo "ğŸš€ å¯åŠ¨ vLLM API Server"
echo "æ¨¡å‹è·¯å¾„: $MODEL_PATH"
echo "GPU: $CUDA_VISIBLE_DEVICES"
echo ""

# å¯åŠ¨ vLLM API Server
python -m vllm.entrypoints.openai.api_server \
    --model "$MODEL_PATH" \
    --quantization awq \
    --tensor-parallel-size 3 \
    --trust-remote-code \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.95 \
    --port 8000 \
    --disable-log-requests

