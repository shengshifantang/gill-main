#!/usr/bin/env python3
"""
Spatial Adapter æ¨ç† Demo

åŠ è½½è®­ç»ƒå¥½çš„ Adapterï¼ŒéªŒè¯å¸ƒå±€æ§åˆ¶æ•ˆæœã€‚

Usage:
    python scripts/inference_demo.py \
        --kolors-model ./model/Kolors \
        --adapter-path ./checkpoints/spatial_adapter_wukong_v2/spatial_adapter_final.pt \
        --prompt "ä¸€åªçŒ«åœ¨å·¦è¾¹ï¼Œä¸€åªç‹—åœ¨å³è¾¹ï¼Œè‰åœ°èƒŒæ™¯" \
        --output demo_result.png
"""

import argparse
import os
import sys
import torch
from PIL import Image, ImageDraw
from diffusers import KolorsPipeline

# ç¡®ä¿å¯ä»¥å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from gill.spatial_adapter import (
    inject_spatial_control_to_unet,
    remove_spatial_control_from_unet,
    create_spatial_adapter_for_kolors,
)


def fix_kolors_tokenizer(pipeline):
    """ä¿®å¤ Kolors Tokenizer çš„ padding_side å…¼å®¹æ€§é—®é¢˜"""
    original_pad = pipeline.tokenizer._pad
    
    def patched_pad(*args, **kwargs):
        # ç§»é™¤ padding_side å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        kwargs.pop('padding_side', None)
        return original_pad(*args, **kwargs)
    
    pipeline.tokenizer._pad = patched_pad
    print("âœ“ å·²ä¿®å¤ Kolors Tokenizer padding å…¼å®¹æ€§")


def draw_boxes(image: Image.Image, boxes: list, color: str = "red", width: int = 5):
    """
    åœ¨å›¾åƒä¸Šç»˜åˆ¶è¾¹ç•Œæ¡†
    
    Args:
        image: PIL Image
        boxes: List of [x1, y1, x2, y2] å½’ä¸€åŒ–åæ ‡ (0-1)
        color: æ¡†çš„é¢œè‰²
        width: æ¡†çš„å®½åº¦
    """
    draw = ImageDraw.Draw(image)
    W, H = image.size
    
    for box in boxes:
        # å½’ä¸€åŒ–åæ ‡è½¬æ¢ä¸ºåƒç´ åæ ‡
        x1, y1, x2, y2 = box
        x1_px = x1 * W
        y1_px = y1 * H
        x2_px = x2 * W
        y2_px = y2 * H
        
        # ç»˜åˆ¶çŸ©å½¢æ¡†
        draw.rectangle([x1_px, y1_px, x2_px, y2_px], outline=color, width=width)
    
    return image


def main():
    parser = argparse.ArgumentParser(description="Spatial Adapter æ¨ç† Demo")
    parser.add_argument(
        "--kolors-model",
        type=str,
        default="./model/Kolors",
        help="Kolors æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--adapter-path",
        type=str,
        required=True,
        help="è®­ç»ƒå¥½çš„ Adapter æƒé‡è·¯å¾„"
    )
    parser.add_argument(
        "--prompt",
        type=str,
        default="ä¸€åªçŒ«åœ¨å·¦è¾¹ï¼Œä¸€åªç‹—åœ¨å³è¾¹ï¼Œè‰åœ°èƒŒæ™¯",
        help="ç”Ÿæˆæç¤ºè¯"
    )
    parser.add_argument(
        "--boxes",
        type=float,
        nargs="+",
        default=[0.0, 0.2, 0.4, 0.8, 0.6, 0.2, 1.0, 0.8],
        help="è¾¹ç•Œæ¡†åæ ‡ [x1, y1, x2, y2, ...] (å½’ä¸€åŒ– 0-1)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="demo_result.png",
        help="è¾“å‡ºå›¾åƒè·¯å¾„"
    )
    parser.add_argument(
        "--height",
        type=int,
        default=1024,
        help="ç”Ÿæˆå›¾åƒé«˜åº¦"
    )
    parser.add_argument(
        "--width",
        type=int,
        default=1024,
        help="ç”Ÿæˆå›¾åƒå®½åº¦"
    )
    parser.add_argument(
        "--num-inference-steps",
        type=int,
        default=50,
        help="æ¨ç†æ­¥æ•°"
    )
    parser.add_argument(
        "--guidance-scale",
        type=float,
        default=7.0,
        help="Guidance scale"
    )
    parser.add_argument(
        "--negative-prompt",
        type=str,
        default="",
        help="Classifier-Free Guidance çš„è´Ÿæç¤ºè¯ï¼Œç•™ç©ºåˆ™ä½¿ç”¨ç©ºä¸²"
    )
    parser.add_argument(
        "--gate-scale",
        type=float,
        default=1.0,
        help="æ¨ç†æ—¶æ”¾å¤§ gateï¼ˆ>1 å¢å¼ºç©ºé—´çº¦æŸï¼Œè°¨æ…ä½¿ç”¨ï¼Œé»˜è®¤ 1.0ï¼‰"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éšæœºç§å­ï¼Œä¾¿äºå¤ç°ï¼›è®¾ä¸º -1 åˆ™ä½¿ç”¨éšæœºç§å­"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="è®¾å¤‡ (cuda/cpu)"
    )
    
    args = parser.parse_args()
    
    # 1. åŠ è½½ Kolors Pipeline
    print(f"ğŸš€ åŠ è½½ Kolors Pipeline: {args.kolors_model}")
    device = args.device if torch.cuda.is_available() else "cpu"
    
    pipeline = KolorsPipeline.from_pretrained(
        args.kolors_model,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        # éƒ¨åˆ† diffusers ç‰ˆæœ¬åœ¨æœ¬åœ°æƒé‡ç¼ºå°‘ fp16 ç›®å½•æ—¶ä¼šå›  variant æŠ¥é”™ï¼Œç›´æ¥çœç•¥
        trust_remote_code=True
    ).to(device)
    
    # ä¿®å¤ Tokenizer å…¼å®¹æ€§
    fix_kolors_tokenizer(pipeline)
    
    # 2. åŠ è½½ Adapter
    print(f"ğŸ“¦ åŠ è½½ Adapter: {args.adapter_path}")
    if not os.path.exists(args.adapter_path):
        raise FileNotFoundError(f"Adapter æ–‡ä»¶ä¸å­˜åœ¨: {args.adapter_path}")
    
    # åˆ›å»º Adapter å®¹å™¨ï¼ˆåŠ¨æ€ç»´åº¦ç®¡ç†ï¼‰
    adapter_container = create_spatial_adapter_for_kolors()
    
    # åŠ è½½æƒé‡
    state_dict = torch.load(args.adapter_path, map_location=device)
    
    # åŠ¨æ€è¡¥é½å®¹å™¨é‡Œéœ€è¦çš„ç»´åº¦ï¼Œå†åŠ è½½ï¼ˆstrict=False å¿½ç•¥å†—ä½™é”®ï¼‰
    if isinstance(state_dict, dict) and any(k.startswith("dim_") for k in state_dict.keys()):
        # è§£æ state_dict ä¸­åŒ…å«çš„ç»´åº¦
        dims_in_ckpt = set()
        for k in state_dict.keys():
            if k.startswith("dim_"):
                try:
                    dim_val = int(k.split("_")[1].split(".")[0])
                    dims_in_ckpt.add(dim_val)
                except Exception:
                    continue
        # ä¸ºæ¯ä¸ªç»´åº¦ç¡®ä¿å®¹å™¨é‡Œæœ‰å¯¹åº” Adapter
        for d in sorted(dims_in_ckpt):
            key = f"dim_{d}"
            if key not in adapter_container:
                # Kolors çš„ UNet å…¸å‹ç»´åº¦æœ‰ 320/640/1280/2048 ç­‰ï¼Œç»Ÿä¸€åˆ›å»º
                adapter_container[key] = adapter_container["dim_2048"].__class__(hidden_dim=d, num_heads=8)
        adapter_container.load_state_dict(state_dict, strict=False)
    else:
        # å¦‚æœæ˜¯å•ä¸ª Adapter çš„æƒé‡ï¼Œéœ€è¦é€‚é…åˆ°å®¹å™¨æ ¼å¼
        # å‡è®¾æ˜¯é»˜è®¤ç»´åº¦ 2048
        if "dim_2048" in adapter_container.state_dict():
            adapter_container["dim_2048"].load_state_dict(state_dict)
        else:
            # å°è¯•ç›´æ¥åŠ è½½åˆ°å®¹å™¨
            adapter_container.load_state_dict(state_dict, strict=False)
    
    # Adapter ä¿æŒ FP32ï¼ˆè®­ç»ƒæ—¶å°±æ˜¯ FP32ï¼‰
    adapter_container = adapter_container.to(device=device, dtype=torch.float32)
    print("âœ“ Adapter åŠ è½½å®Œæˆ")
    
    # å¯é€‰ï¼šæ”¾å¤§ gateï¼ŒåŠ å¼ºç©ºé—´çº¦æŸï¼ˆä¸´æ—¶ hackï¼Œè¿‡å¤§ä¼šå½±å“ç”»è´¨ï¼‰
    if args.gate_scale != 1.0:
        with torch.no_grad():
            scaled_cnt = 0
            for m in adapter_container.modules():
                if hasattr(m, "gate"):
                    m.gate.mul_(args.gate_scale)
                    scaled_cnt += 1
        print(f"âœ“ gate æ”¾å¤§ç³»æ•° {args.gate_scale} å·²åº”ç”¨ï¼Œä½œç”¨å±‚æ•°: {scaled_cnt}")
    
    # è®¾ç½®éšæœºç§å­ï¼Œä¿è¯å¤ç°ï¼›seed=-1 æ—¶ä¸å›ºå®š
    generator = None
    if args.seed is not None and args.seed >= 0:
        torch.manual_seed(args.seed)
        if device == "cuda":
            torch.cuda.manual_seed_all(args.seed)
        generator = torch.Generator(device=device).manual_seed(args.seed)
        print(f"âœ“ å·²è®¾ç½®éšæœºç§å­: {args.seed}")
    
    # 3. å‡†å¤‡ BBoxes
    boxes_list = args.boxes
    if len(boxes_list) % 4 != 0:
        raise ValueError(f"BBoxes åæ ‡æ•°é‡å¿…é¡»æ˜¯ 4 çš„å€æ•°ï¼Œå½“å‰: {len(boxes_list)}")
    
    # è½¬æ¢ä¸º (B, N, 4) æ ¼å¼
    num_boxes = len(boxes_list) // 4
    boxes = [[boxes_list[i*4], boxes_list[i*4+1], boxes_list[i*4+2], boxes_list[i*4+3]] 
             for i in range(num_boxes)]
    
    bboxes_tensor = torch.tensor([boxes], device=device, dtype=torch.float32)
    print(f"âœ“ å‡†å¤‡ {num_boxes} ä¸ªè¾¹ç•Œæ¡†: {boxes}")
    
    # 4. æ³¨å…¥ Spatial Control
    print("ğŸ”§ æ³¨å…¥ Spatial Control åˆ° UNet...")
    orig_procs, spatial_procs, adapter_container = inject_spatial_control_to_unet(
        pipeline.unet,
        adapter_dict=adapter_container,
        bboxes=bboxes_tensor
    )
    print("âœ“ Spatial Control å·²æ³¨å…¥")
    
    try:
        # 5. ç”Ÿæˆå›¾åƒ
        print(f"ğŸ¨ ç”Ÿæˆå›¾åƒ: {args.prompt}")
        print(f"   å°ºå¯¸: {args.width}x{args.height}")
        print(f"   æ­¥æ•°: {args.num_inference_steps}, Guidance: {args.guidance_scale}")
        
        image = pipeline(
            prompt=args.prompt,
            negative_prompt=args.negative_prompt,
            height=args.height,
            width=args.width,
            num_inference_steps=args.num_inference_steps,
            guidance_scale=args.guidance_scale,
            generator=generator,
        ).images[0]
        
        print("âœ“ å›¾åƒç”Ÿæˆå®Œæˆ")
        
        # 6. ç»˜åˆ¶è¾¹ç•Œæ¡†
        image_with_boxes = draw_boxes(image.copy(), boxes)
        
        # 7. ä¿å­˜ç»“æœ
        image_with_boxes.save(args.output)
        print(f"âœ… ç»“æœå·²ä¿å­˜è‡³: {args.output}")
        
        # åŒæ—¶ä¿å­˜ä¸å¸¦æ¡†çš„åŸå§‹å›¾åƒ
        if args.output.endswith('.png'):
            raw_output = args.output.replace('.png', '_raw.png')
        else:
            raw_output = args.output + '_raw.png'
        image.save(raw_output)
        print(f"âœ… åŸå§‹å›¾åƒå·²ä¿å­˜è‡³: {raw_output}")
        
    finally:
        # 8. ç§»é™¤ Spatial Controlï¼ˆæ¢å¤åŸå§‹ UNetï¼‰
        print("ğŸ”§ ç§»é™¤ Spatial Control...")
        remove_spatial_control_from_unet(pipeline.unet, orig_procs)
        print("âœ“ Spatial Control å·²ç§»é™¤")


if __name__ == "__main__":
    main()

