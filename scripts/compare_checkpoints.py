#!/usr/bin/env python3
"""
æ¯”è¾ƒä¸åŒ checkpoint çš„è¾“å‡ºæ ¼å¼
ç”¨äºéªŒè¯"æœ€åä¸€ä¸ª epoch"æ˜¯å¦æ ¼å¼æ›´å¥½
"""

import os
import sys
import argparse
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def load_model_and_tokenizer(base_path, adapter_path, device_str):
    """åŠ è½½æ¨¡å‹å’Œ tokenizer"""
    print(f"ğŸ“¦ åŠ è½½: {adapter_path}")
    
    # æ£€æŸ¥å®é™…å¯ç”¨çš„ GPU æ•°é‡
    if not torch.cuda.is_available():
        print("  âš ï¸  CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
        device_obj = torch.device("cpu")
        device_map_str = "cpu"
    else:
        available_gpus = torch.cuda.device_count()
        print(f"  ğŸ“ æ£€æµ‹åˆ° {available_gpus} ä¸ªå¯ç”¨ GPU")
        
        # è§£æè®¾å¤‡å­—ç¬¦ä¸²
        if device_str.startswith("cuda:"):
            requested_idx = int(device_str.split(":")[1])
            # å¦‚æœä½¿ç”¨äº† CUDA_VISIBLE_DEVICESï¼Œå®é™…ç´¢å¼•ä¼šè¢«é‡æ–°æ˜ å°„
            # ä¾‹å¦‚ CUDA_VISIBLE_DEVICES=2 æ—¶ï¼ŒPyTorch åªèƒ½çœ‹åˆ° 1 ä¸ª GPUï¼ˆç´¢å¼•ä¸º 0ï¼‰
            # æ‰€ä»¥å¦‚æœè¯·æ±‚çš„æ˜¯ cuda:2ï¼Œä½†åªæœ‰ 1 ä¸ªå¯è§ GPUï¼Œåº”è¯¥ä½¿ç”¨ cuda:0
            if requested_idx >= available_gpus:
                print(f"  âš ï¸  è¯·æ±‚çš„è®¾å¤‡ {requested_idx} è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨ cuda:0")
                device_idx = 0
            else:
                device_idx = requested_idx
            device_obj = torch.device(f"cuda:{device_idx}")
            device_map_str = f"cuda:{device_idx}"
        elif device_str == "cuda":
            device_obj = torch.device("cuda:0")
            device_map_str = "cuda:0"
        else:
            device_obj = torch.device(device_str)
            device_map_str = device_str
    
    # 1. ä¼˜å…ˆä» Adapter è·¯å¾„åŠ è½½ Tokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)
        print("  âœ“ ä» Adapter åŠ è½½ Tokenizer")
    except:
        tokenizer = AutoTokenizer.from_pretrained(base_path, trust_remote_code=True)
        tokenizer.add_special_tokens({"additional_special_tokens": ["<obj>", "</obj>", "<box>", "</box>"]})
        print("  âš ï¸  ä»åŸºåº§åŠ è½½ Tokenizer")
    
    # 2. åŠ è½½åŸºåº§æ¨¡å‹ï¼ˆä½¿ç”¨æ˜ç¡®çš„è®¾å¤‡æ˜ å°„ï¼Œé¿å… auto æ¨¡å¼ï¼‰
    print(f"  ğŸ“ ä½¿ç”¨è®¾å¤‡: {device_map_str}")
    # å¯¹äºå• GPUï¼Œä½¿ç”¨å­—å…¸æ ¼å¼çš„ device_map æ›´å®‰å…¨
    # {"": device_str} è¡¨ç¤ºæ‰€æœ‰å±‚éƒ½æ”¾åœ¨æŒ‡å®šè®¾å¤‡ä¸Š
    model = AutoModelForCausalLM.from_pretrained(
        base_path,
        torch_dtype=torch.float16,
        device_map={"": device_map_str},  # ä½¿ç”¨å­—å…¸æ ¼å¼ï¼Œç¡®ä¿æ‰€æœ‰å±‚éƒ½åœ¨åŒä¸€è®¾å¤‡
        trust_remote_code=True
    )
    model.resize_token_embeddings(len(tokenizer))
    
    # 3. åŠ è½½ LoRAï¼ˆPeftModel ä¼šè‡ªåŠ¨å¤„ç†è®¾å¤‡ï¼‰
    print("  ğŸ“¦ åŠ è½½ LoRA Adapter...")
    model = PeftModel.from_pretrained(model, adapter_path)
    model.eval()
    print("  âœ“ æ¨¡å‹åŠ è½½å®Œæˆ\n")
    
    return model, tokenizer

def test_checkpoint(base_path, adapter_path, device_str, test_prompts):
    """æµ‹è¯•å•ä¸ª checkpoint"""
    model, tokenizer = load_model_and_tokenizer(base_path, adapter_path, device_str)
    
    # è§£æè®¾å¤‡å­—ç¬¦ä¸²ä¸º torch.device å¯¹è±¡ï¼ˆä¸ load_model_and_tokenizer ä¸­çš„é€»è¾‘ä¿æŒä¸€è‡´ï¼‰
    if not torch.cuda.is_available():
        device_obj = torch.device("cpu")
    elif device_str.startswith("cuda:"):
        requested_idx = int(device_str.split(":")[1])
        available_gpus = torch.cuda.device_count()
        # å¦‚æœä½¿ç”¨äº† CUDA_VISIBLE_DEVICESï¼Œå®é™…ç´¢å¼•ä¼šè¢«é‡æ–°æ˜ å°„
        device_idx = 0 if requested_idx >= available_gpus else requested_idx
        device_obj = torch.device(f"cuda:{device_idx}")
    elif device_str == "cuda":
        device_obj = torch.device("cuda:0")
    else:
        device_obj = torch.device(device_str)
    
    results = []
    for prompt in test_prompts:
        messages = [{"role": "user", "content": prompt}]
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer(text, return_tensors="pt").to(device_obj)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=128,
                temperature=0.2,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        generated = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=False)
        
        # åˆ†ææ ¼å¼
        has_obj = '<obj>' in generated
        has_box = '<box>' in generated
        has_tool_call = '</tool_call>' in generated or 'useRal' in generated
        has_garbage = any(x in generated for x in ['<|im_start|>', '<|im_end|>', '<|endoftext|>'])
        
        results.append({
            'prompt': prompt,
            'output': generated[:200],
            'has_obj': has_obj,
            'has_box': has_box,
            'has_tool_call': has_tool_call,
            'has_garbage': has_garbage,
            'format_ok': has_obj and has_box and not has_tool_call and not has_garbage
        })
    
    # æ¸…ç†
    del model
    torch.cuda.empty_cache()
    
    return results

def main():
    parser = argparse.ArgumentParser(description="æ¯”è¾ƒä¸åŒ checkpoint çš„è¾“å‡ºæ ¼å¼")
    parser.add_argument("--base-model", type=str, default="./model/qwen2.5-7B-Instruct")
    parser.add_argument("--device", type=str, default="cuda:2")
    parser.add_argument("--checkpoints", type=str, nargs="+", 
                       default=["checkpoints/layout_planner/final", 
                               "checkpoints/layout_planner/checkpoint-22500"])
    args = parser.parse_args()
    
    test_prompts = [
        "ç”»ä¸€åªåœ¨æ¡Œå­å·¦è¾¹çš„çŒ«",
        "å¤©ç©ºä¸­æœ‰é£é¸Ÿï¼Œä¸‹é¢æ˜¯å¹¿é˜”çš„è‰åœ°",
        "å·¦è¾¹æ˜¯ä¸€ä¸ªçº¢è‰²çš„è‹¹æœï¼Œå³è¾¹æ˜¯ä¸€ä¸ªé»„è‰²çš„é¦™è•‰"
    ]
    
    print("=" * 70)
    print("ğŸ” æ¯”è¾ƒä¸åŒ Checkpoint çš„è¾“å‡ºæ ¼å¼")
    print("=" * 70)
    print()
    
    all_results = {}
    for checkpoint_path in args.checkpoints:
        checkpoint_name = os.path.basename(checkpoint_path)
        print(f"\n{'='*70}")
        print(f"ğŸ“Š æµ‹è¯•: {checkpoint_name}")
        print(f"{'='*70}\n")
        
        try:
            results = test_checkpoint(args.base_model, checkpoint_path, args.device, test_prompts)
            all_results[checkpoint_name] = results
            
            # æ‰“å°ç»“æœ
            for r in results:
                print(f"ğŸ“ Prompt: {r['prompt']}")
                print(f"ğŸ¤– è¾“å‡º: {r['output']}")
                print(f"   æ ¼å¼æ£€æŸ¥:")
                print(f"     <obj>: {'âœ…' if r['has_obj'] else 'âŒ'}")
                print(f"     <box>: {'âœ…' if r['has_box'] else 'âŒ'}")
                print(f"     ä¹±ç : {'âŒ æœ‰' if r['has_tool_call'] or r['has_garbage'] else 'âœ… æ— '}")
                print(f"   æ€»ä½“: {'âœ… æ ¼å¼æ­£ç¡®' if r['format_ok'] else 'âš ï¸  æ ¼å¼æœ‰é—®é¢˜'}")
                print()
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            print()
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ“Š æ€»ç»“")
    print("=" * 70)
    for name, results in all_results.items():
        format_ok_count = sum(1 for r in results if r['format_ok'])
        print(f"{name}: {format_ok_count}/{len(results)} ä¸ªæ ·æœ¬æ ¼å¼æ­£ç¡®")
    
    print("\nğŸ’¡ å»ºè®®:")
    if len(all_results) >= 2:
        names = list(all_results.keys())
        r1 = all_results[names[0]]
        r2 = all_results[names[1]]
        ok1 = sum(1 for r in r1 if r['format_ok'])
        ok2 = sum(1 for r in r2 if r['format_ok'])
        
        if ok2 > ok1:
            print(f"   âœ… {names[1]} æ ¼å¼æ›´å¥½ï¼Œå»ºè®®ä½¿ç”¨è¯¥ checkpoint")
        elif ok1 > ok2:
            print(f"   âœ… {names[0]} æ ¼å¼æ›´å¥½ï¼Œå»ºè®®ä½¿ç”¨è¯¥ checkpoint")
        else:
            print(f"   âš ï¸  ä¸¤ä¸ª checkpoint æ ¼å¼ç›¸ä¼¼ï¼Œå»ºè®®ä½¿ç”¨æœ€åä¸€ä¸ª epoch çš„æ¨¡å‹")

if __name__ == "__main__":
    main()
