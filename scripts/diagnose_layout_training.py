#!/usr/bin/env python3
"""
è¯Šæ–­ Layout Planner è®­ç»ƒé—®é¢˜

æ£€æŸ¥ï¼š
1. è®­ç»ƒæ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®
2. Chat Template æ ¼å¼åŒ–åçš„æ–‡æœ¬æ ¼å¼
3. Label Masking æ˜¯å¦æ­£ç¡®
4. ç‰¹æ®Š Token æ˜¯å¦è¢«æ­£ç¡®å­¦ä¹ 
"""

import os
import sys
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def diagnose_training_data(data_path: str):
    """è¯Šæ–­è®­ç»ƒæ•°æ®æ ¼å¼"""
    print("=" * 60)
    print("1. æ£€æŸ¥è®­ç»ƒæ•°æ®æ ¼å¼")
    print("=" * 60)
    
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return False
    
    count = 0
    valid_input_output_count = 0
    valid_caption_objects_count = 0
    
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip():
                continue
            count += 1
            try:
                data = json.loads(line)
                
                # æƒ…å†µ1: æ£€æŸ¥ input/output æ ¼å¼ï¼ˆé¢„å¤„ç†å¥½çš„æ ¼å¼ï¼‰
                if 'input' in data and 'output' in data:
                    output = data['output']
                    if '<obj>' in output and '<box>' in output:
                        valid_input_output_count += 1
                    if count <= 3:
                        print(f"\næ ·æœ¬ {count} (input/output æ ¼å¼):")
                        print(f"  Input: {data['input'][:80]}")
                        print(f"  Output: {data['output'][:150]}")
                        print(f"  åŒ…å« <obj>: {'<obj>' in output}")
                        print(f"  åŒ…å« <box>: {'<box>' in output}")
                
                # æƒ…å†µ2: æ£€æŸ¥ caption + objects æ ¼å¼ï¼ˆåŸå§‹æ ¼å¼ï¼Œä¼šè¢« LayoutJsonlDataset è½¬æ¢ï¼‰
                elif 'caption' in data and 'objects' in data:
                    caption = data.get('caption', '').strip()
                    objects = data.get('objects', [])
                    if caption and objects and len(objects) > 0:
                        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ bbox
                        has_valid_bbox = False
                        for obj in objects:
                            bbox = obj.get('bbox', [])
                            bbox_1000 = obj.get('bbox_1000', [])
                            if (bbox and len(bbox) == 4) or (bbox_1000 and len(bbox_1000) == 4):
                                has_valid_bbox = True
                                break
                        
                        if has_valid_bbox:
                            valid_caption_objects_count += 1
                        
                        if count <= 3:
                            print(f"\næ ·æœ¬ {count} (caption/objects æ ¼å¼):")
                            print(f"  Caption: {caption[:80]}")
                            print(f"  Objects æ•°é‡: {len(objects)}")
                            print(f"  æœ‰æœ‰æ•ˆ bbox: {has_valid_bbox}")
                            if objects:
                                first_obj = objects[0]
                                print(f"  ç¬¬ä¸€ä¸ªå¯¹è±¡: name={first_obj.get('name', 'N/A')}, bbox={first_obj.get('bbox', 'N/A')}")
            except Exception as e:
                if count <= 3:
                    print(f"\næ ·æœ¬ {count}: è§£æé”™è¯¯ - {e}")
                continue
    
    total_valid = valid_input_output_count + valid_caption_objects_count
    print(f"\næ€»è®¡: {count} æ¡æ ·æœ¬")
    print(f"input/output æ ¼å¼: {valid_input_output_count} æ¡")
    print(f"caption/objects æ ¼å¼: {valid_caption_objects_count} æ¡")
    print(f"æœ‰æ•ˆæ ¼å¼æ€»è®¡: {total_valid} æ¡ ({total_valid/count*100:.1f}%)")
    
    return total_valid > 0


def diagnose_chat_template(base_model_path: str):
    """è¯Šæ–­ Chat Template æ ¼å¼åŒ–"""
    print("\n" + "=" * 60)
    print("2. æ£€æŸ¥ Chat Template æ ¼å¼åŒ–")
    print("=" * 60)
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        tokenizer.add_special_tokens({"additional_special_tokens": ["<obj>", "</obj>", "<box>", "</box>"]})
        
        # æµ‹è¯•æ ·æœ¬
        sample_input = "ç”»ä¸€åªåœ¨æ¡Œå­å·¦è¾¹çš„çŒ«"
        sample_output = "<obj>çŒ«</obj><box>[0.10,0.20,0.40,0.50]</box><obj>æ¡Œå­</obj><box>[0.50,0.30,0.90,0.80]</box>"
        
        # æ„å»ºå®Œæ•´å¯¹è¯
        messages = [
            {"role": "user", "content": sample_input},
            {"role": "assistant", "content": sample_output}
        ]
        full_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        
        # æ„å»º Prompt éƒ¨åˆ†
        user_msg = [{"role": "user", "content": sample_input}]
        prompt_text = tokenizer.apply_chat_template(user_msg, tokenize=False, add_generation_prompt=True)
        
        print(f"è¾“å…¥: {sample_input}")
        print(f"\nå®Œæ•´å¯¹è¯æ–‡æœ¬ï¼ˆæœ€å 200 å­—ç¬¦ï¼‰:")
        print(full_text[-200:])
        print(f"\nPrompt æ–‡æœ¬ï¼ˆæœ€å 100 å­—ç¬¦ï¼‰:")
        print(prompt_text[-100:])
        
        # Tokenize æ£€æŸ¥
        full_ids = tokenizer(full_text, add_special_tokens=False).input_ids
        prompt_ids = tokenizer(prompt_text, add_special_tokens=False).input_ids
        
        assistant_ids = full_ids[len(prompt_ids):]
        assistant_text = tokenizer.decode(assistant_ids, skip_special_tokens=False)
        
        print(f"\nAssistant éƒ¨åˆ† Token æ•°é‡: {len(assistant_ids)}")
        print(f"Assistant éƒ¨åˆ†æ–‡æœ¬: {assistant_text[:200]}")
        print(f"åŒ…å« <obj>: {'<obj>' in assistant_text}")
        print(f"åŒ…å« <box>: {'<box>' in assistant_text}")
        
        # æ£€æŸ¥ç‰¹æ®Š Token ID
        obj_id = tokenizer.convert_tokens_to_ids("<obj>")
        box_id = tokenizer.convert_tokens_to_ids("<box>")
        print(f"\nç‰¹æ®Š Token ID:")
        print(f"  <obj>: {obj_id}")
        print(f"  <box>: {box_id}")
        print(f"  <obj> åœ¨ Assistant éƒ¨åˆ†: {obj_id in assistant_ids}")
        print(f"  <box> åœ¨ Assistant éƒ¨åˆ†: {box_id in assistant_ids}")
        
        return True
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def diagnose_label_masking(base_model_path: str, data_path: str):
    """è¯Šæ–­ Label Masking"""
    print("\n" + "=" * 60)
    print("3. æ£€æŸ¥ Label Masking")
    print("=" * 60)
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        tokenizer.add_special_tokens({"additional_special_tokens": ["<obj>", "</obj>", "<box>", "</box>"]})
        
        # åŠ è½½ä¸€ä¸ªæ ·æœ¬å¹¶è½¬æ¢ä¸º input/output æ ¼å¼
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data = json.loads(line)
                    break
        
        # å¤„ç†ä¸¤ç§æ•°æ®æ ¼å¼
        if 'input' in data and 'output' in data:
            sample_input = data['input']
            sample_output = data['output']
        elif 'caption' in data and 'objects' in data:
            # æ¨¡æ‹Ÿ LayoutJsonlDataset çš„è½¬æ¢é€»è¾‘
            sample_input = str(data.get('caption', '')).strip()
            objs = data.get('objects', [])
            
            # è·å–å›¾åƒå°ºå¯¸
            width = float(data.get('width', 0))
            height = float(data.get('height', 0))
            has_dim = width > 0 and height > 0
            
            out_parts = []
            for obj in objs:
                # åç§°å¤„ç†
                name = str(obj.get("name", "")).strip()
                if not name:
                    # å°è¯•ä» category_id è½¬æ¢ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…ä»£ç ä¸­æœ‰å®Œæ•´æ˜ å°„ï¼‰
                    category_id = obj.get("category_id")
                    if category_id:
                        name = f"ç‰©ä½“{category_id}"
                    else:
                        name = "ç‰©ä½“"
                
                # åæ ‡å¤„ç†
                bbox = obj.get("bbox", [])
                bbox_1000 = obj.get("bbox_1000", [])
                bbox_final = None
                
                if bbox_1000 and len(bbox_1000) == 4:
                    bbox_final = [float(v) / 1000.0 for v in bbox_1000]
                elif bbox and len(bbox) == 4:
                    bbox_raw = [float(v) for v in bbox]
                    max_val = max(bbox_raw)
                    
                    if max_val <= 1.05:
                        bbox_final = bbox_raw
                    elif has_dim:
                        bbox_final = [
                            bbox_raw[0] / width,
                            bbox_raw[1] / height,
                            bbox_raw[2] / width,
                            bbox_raw[3] / height
                        ]
                    elif max_val <= 1000:
                        bbox_final = [v / 1000.0 for v in bbox_raw]
                
                if bbox_final:
                    bbox_final = [max(0.0, min(1.0, v)) for v in bbox_final]
                    bbox_str = ",".join(f"{v:.2f}" for v in bbox_final)
                    out_parts.append(f"<obj>{name}</obj><box>[{bbox_str}]</box>")
            
            sample_output = "".join(out_parts)
            
            if not sample_input or not sample_output:
                print("âŒ æ— æ³•ä» caption/objects æ ¼å¼ç”Ÿæˆæœ‰æ•ˆçš„ input/output")
                return False
        else:
            print("âŒ æ•°æ®æ ¼å¼ä¸æ”¯æŒï¼šæ—¢æ²¡æœ‰ input/outputï¼Œä¹Ÿæ²¡æœ‰ caption/objects")
            return False
        
        # æ„å»ºæ–‡æœ¬
        messages = [
            {"role": "user", "content": sample_input},
            {"role": "assistant", "content": sample_output}
        ]
        full_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        
        user_msg = [{"role": "user", "content": sample_input}]
        prompt_text = tokenizer.apply_chat_template(user_msg, tokenize=False, add_generation_prompt=True)
        
        # Tokenize
        full_ids = tokenizer(full_text, add_special_tokens=False, max_length=512, truncation=True).input_ids
        prompt_ids = tokenizer(prompt_text, add_special_tokens=False, max_length=512, truncation=True).input_ids
        
        # Label Maskingï¼ˆæ¨¡æ‹Ÿè®­ç»ƒä»£ç é€»è¾‘ï¼‰
        labels = full_ids.copy()
        prompt_len = len(prompt_ids)
        
        if prompt_len < len(labels):
            labels[:prompt_len] = [-100] * prompt_len
        else:
            labels = [-100] * len(labels)
        
        # æ£€æŸ¥
        print(f"Full IDs é•¿åº¦: {len(full_ids)}")
        print(f"Prompt IDs é•¿åº¦: {prompt_len}")
        print(f"Label ä¸­é -100 çš„æ•°é‡: {sum(1 for x in labels if x != -100)}")
        print(f"Label ä¸­ -100 çš„æ•°é‡: {sum(1 for x in labels if x == -100)}")
        
        # æ£€æŸ¥ Assistant éƒ¨åˆ†çš„æ ‡ç­¾
        assistant_labels = labels[prompt_len:]
        assistant_tokens = full_ids[prompt_len:]
        assistant_text = tokenizer.decode(assistant_tokens, skip_special_tokens=False)
        
        print(f"\nAssistant éƒ¨åˆ†:")
        print(f"  Token æ•°é‡: {len(assistant_tokens)}")
        print(f"  Label æ•°é‡: {len(assistant_labels)}")
        print(f"  éœ€è¦è®¡ç®— Loss çš„ Token æ•°: {sum(1 for x in assistant_labels if x != -100)}")
        print(f"  æ–‡æœ¬: {assistant_text[:200]}")
        print(f"  åŒ…å« <obj>: {'<obj>' in assistant_text}")
        print(f"  åŒ…å« <box>: {'<box>' in assistant_text}")
        
        # æ£€æŸ¥ç‰¹æ®Š Token çš„ Label
        obj_id = tokenizer.convert_tokens_to_ids("<obj>")
        box_id = tokenizer.convert_tokens_to_ids("<box>")
        
        obj_indices = [i for i, tid in enumerate(assistant_tokens) if tid == obj_id]
        box_indices = [i for i, tid in enumerate(assistant_tokens) if tid == box_id]
        
        print(f"\nç‰¹æ®Š Token ä½ç½®æ£€æŸ¥:")
        print(f"  <obj> å‡ºç°åœ¨ä½ç½®: {obj_indices[:5]}... (å…± {len(obj_indices)} ä¸ª)")
        print(f"  <box> å‡ºç°åœ¨ä½ç½®: {box_indices[:5]}... (å…± {len(box_indices)} ä¸ª)")
        
        if obj_indices:
            obj_label = assistant_labels[obj_indices[0]]
            print(f"  ç¬¬ä¸€ä¸ª <obj> çš„ Label: {obj_label} (åº”ä¸ºé -100)")
        
        if box_indices:
            box_label = assistant_labels[box_indices[0]]
            print(f"  ç¬¬ä¸€ä¸ª <box> çš„ Label: {box_label} (åº”ä¸ºé -100)")
        
        return True
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--data-path", type=str, default="data/layout_planner_train.jsonl")
    parser.add_argument("--base-model", type=str, default="./model/qwen2.5-7B-Instruct")
    args = parser.parse_args()
    
    print("ğŸ” Layout Planner è®­ç»ƒé—®é¢˜è¯Šæ–­")
    print("=" * 60)
    
    # 1. æ£€æŸ¥è®­ç»ƒæ•°æ®
    data_ok = diagnose_training_data(args.data_path)
    
    # 2. æ£€æŸ¥ Chat Template
    if data_ok:
        template_ok = diagnose_chat_template(args.base_model)
        
        # 3. æ£€æŸ¥ Label Masking
        if template_ok:
            masking_ok = diagnose_label_masking(args.base_model, args.data_path)
            
            # æ€»ç»“
            print("\n" + "=" * 60)
            print("è¯Šæ–­æ€»ç»“")
            print("=" * 60)
            print(f"è®­ç»ƒæ•°æ®æ ¼å¼: {'âœ…' if data_ok else 'âŒ'}")
            print(f"Chat Template: {'âœ…' if template_ok else 'âŒ'}")
            print(f"Label Masking: {'âœ…' if masking_ok else 'âŒ'}")


if __name__ == "__main__":
    main()
