#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
[SOTA Standard] å¼‚æ­¥é«˜å¹¶å‘å›¾ç‰‡ä¸‹è½½è„šæœ¬ï¼ˆç§‘ç ”çº§ä¼˜åŒ–ç‰ˆï¼‰

æ”¹è¿›ç‚¹ï¼š
1. æ¶æ„ï¼šaiohttp å¼‚æ­¥æ›¿ä»£å¤šçº¿ç¨‹ï¼Œé™ä½ CPU å¼€é”€ï¼Œæ”¯æŒæ›´é«˜å¹¶å‘ (100+)
2. æ ¡éªŒï¼šå¢åŠ  PIL å›¾ç‰‡å®Œæ•´æ€§æ ¡éªŒï¼Œå‰”é™¤ä¼ªè£…æˆ JPG çš„ HTML æˆ–æˆªæ–­å›¾ç‰‡
3. å®¡è®¡ï¼šä¿ç•™å®Œæ•´å…ƒæ•°æ®ï¼Œä¾¿äºè®ºæ–‡ Reproducibility
4. æ–­ç‚¹ç»­ä¼ ï¼šè‡ªåŠ¨è·³è¿‡å·²ä¸‹è½½çš„å›¾ç‰‡ï¼Œæ”¯æŒä¸­æ–­æ¢å¤

ç”¨æ³•ç¤ºä¾‹ï¼š
python scripts/download_wukong_robust.py \
    --csv_dir /mnt/disk/lxh/gill_data/wukong_filtered_spatial_500k.csv \
    --save_dir /mnt/disk/lxh/gill_data/wukong_images \
    --output_jsonl /mnt/disk/lxh/gill_data/wukong_downloaded_robust.jsonl \
    --concurrency 100
"""

import os
import json
import asyncio
import argparse
import hashlib
from pathlib import Path
from io import BytesIO
from typing import Set

try:
    import aiohttp
    import aiofiles
except ImportError:
    print("âŒ éœ€è¦å®‰è£… aiohttp å’Œ aiofiles: pip install aiohttp aiofiles")
    exit(1)

try:
    from PIL import Image
except ImportError:
    print("âŒ éœ€è¦å®‰è£… Pillow: pip install Pillow")
    exit(1)

try:
    import pandas as pd
except ImportError:
    print("âŒ éœ€è¦å®‰è£… pandas: pip install pandas")
    exit(1)

try:
    from tqdm.asyncio import tqdm
except ImportError:
    from tqdm import tqdm

# å…¨å±€ç»Ÿè®¡
STATS = {
    "total": 0,
    "success": 0,
    "failed": 0,
    "corrupt": 0,
    "exist": 0,
    "invalid_url": 0
}


async def validate_image(content: bytes) -> bool:
    """
    ä¸¥æ ¼æ ¡éªŒå›¾ç‰‡å®Œæ•´æ€§ï¼ˆé˜²æ­¢æˆªæ–­/æŸåï¼‰
    
    ä½¿ç”¨ PIL çš„ verify() æ–¹æ³•æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶ç»“æ„æ˜¯å¦å®Œæ•´ã€‚
    è¿™å¯¹äºå¤§è§„æ¨¡æ•°æ®é›†éå¸¸é‡è¦ï¼Œå› ä¸ºæŸåçš„å›¾ç‰‡ä¼šå¯¼è‡´è®­ç»ƒæ—¶ DataLoader å´©æºƒã€‚
    """
    try:
        with Image.open(BytesIO(content)) as img:
            img.verify()  # æ ¡éªŒæ–‡ä»¶ç»“æ„ï¼Œä¸åŠ è½½åƒç´ æ•°æ®ï¼ˆæ›´å¿«ï¼‰
        return True
    except Exception:
        return False


async def download_worker(
    session: aiohttp.ClientSession,
    row: dict,
    save_dir: str,
    semaphore: asyncio.Semaphore,
    processed_urls: Set[str]
) -> dict:
    """
    å¼‚æ­¥ä¸‹è½½å•å¼ å›¾ç‰‡
    
    Args:
        session: aiohttp ä¼šè¯
        row: åŒ…å« url å’Œ caption çš„å­—å…¸
        save_dir: ä¿å­˜ç›®å½•
        semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        processed_urls: å·²å¤„ç†çš„ URL é›†åˆï¼ˆç”¨äºå»é‡ï¼‰
    
    Returns:
        æˆåŠŸæ—¶è¿”å›åŒ…å« image_path, caption, url çš„å­—å…¸ï¼Œå¤±è´¥è¿”å› None
    """
    async with semaphore:  # é™åˆ¶å¹¶å‘æ•°
        url = row.get('url')
        caption = row.get('caption', '')
        
        # URL æœ‰æ•ˆæ€§æ£€æŸ¥
        if not isinstance(url, str) or len(url) < 5:
            STATS["invalid_url"] += 1
            return None

        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
        if url in processed_urls:
            return None

        # 1. è·¯å¾„è®¡ç®—ï¼ˆä½¿ç”¨ URL hash ç¡®ä¿å”¯ä¸€æ€§ï¼‰
        try:
            img_hash = hashlib.md5(url.encode('utf-8')).hexdigest()
            filename = f"{img_hash}.jpg"
            save_path = os.path.join(save_dir, filename)
            
            # 2. å­˜åœ¨æ€§æ£€æŸ¥ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
            if os.path.exists(save_path):
                file_size = os.path.getsize(save_path)
                # ç®€å•æ ¡éªŒå¤§å°ï¼Œé¿å…ç©ºæ–‡ä»¶
                if file_size > 1024:
                    # [å…³é”®] å¦‚æœå¯ç”¨éªŒè¯ï¼Œæ£€æŸ¥å·²å­˜åœ¨å›¾ç‰‡çš„å®Œæ•´æ€§
                    if args.validate_existing:
                        try:
                            with open(save_path, 'rb') as f:
                                content = f.read()
                            if not await validate_image(content):
                                # å›¾ç‰‡æŸåï¼Œåˆ é™¤å¹¶é‡æ–°ä¸‹è½½
                                os.remove(save_path)
                                STATS["corrupt"] += 1
                                # ç»§ç»­æ‰§è¡Œä¸‹è½½æµç¨‹ï¼ˆä¸è¿”å›ï¼‰
                            else:
                                # å›¾ç‰‡å®Œæ•´ï¼Œè·³è¿‡ä¸‹è½½
                                STATS["exist"] += 1
                                processed_urls.add(url)
                                return {
                                    "image_path": os.path.abspath(save_path),
                                    "caption": caption,
                                    "url": url
                                }
                        except Exception:
                            # è¯»å–å¤±è´¥ï¼Œåˆ é™¤å¹¶é‡æ–°ä¸‹è½½
                            if os.path.exists(save_path):
                                os.remove(save_path)
                            # ç»§ç»­æ‰§è¡Œä¸‹è½½æµç¨‹ï¼ˆä¸è¿”å›ï¼‰
                    else:
                        # ä¸éªŒè¯å·²å­˜åœ¨å›¾ç‰‡ï¼Œç›´æ¥è·³è¿‡ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
                        STATS["exist"] += 1
                        processed_urls.add(url)
                        return {
                            "image_path": os.path.abspath(save_path),
                            "caption": caption,
                            "url": url
                        }
            
            # 3. ç½‘ç»œè¯·æ±‚ï¼ˆå¸¦è¶…æ—¶å’Œé‡è¯•ï¼‰
            timeout = aiohttp.ClientTimeout(total=10, connect=3)
            try:
                async with session.get(url, timeout=timeout, allow_redirects=True) as response:
                    if response.status == 200:
                        content = await response.read()
                        
                        # 4. [å…³é”®] å›¾ç‰‡æœ‰æ•ˆæ€§æ·±åº¦æ ¡éªŒ
                        if len(content) < 1024:  # æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ˜¯é”™è¯¯é¡µé¢
                            STATS["corrupt"] += 1
                            return None
                        
                        if not await validate_image(content):
                            STATS["corrupt"] += 1
                            return None
                        
                        # 5. å¼‚æ­¥å†™å…¥
                        async with aiofiles.open(save_path, 'wb') as f:
                            await f.write(content)
                        
                        STATS["success"] += 1
                        processed_urls.add(url)
                        return {
                            "image_path": os.path.abspath(save_path),
                            "caption": caption,
                            "url": url
                        }
                    else:
                        STATS["failed"] += 1
                        return None
                        
            except asyncio.TimeoutError:
                STATS["failed"] += 1
                return None
            except aiohttp.ClientError:
                STATS["failed"] += 1
                return None
                
        except Exception as e:
            # å…¶ä»–å¼‚å¸¸ï¼ˆæ–‡ä»¶ç³»ç»Ÿé”™è¯¯ç­‰ï¼‰
            STATS["failed"] += 1
            return None


def _fix_newline(filepath: str):
    """ç¡®ä¿æ–‡ä»¶ä»¥æ¢è¡Œç¬¦ç»“å°¾ï¼ˆé˜²æ­¢è¿½åŠ æ—¶æ ¼å¼é”™è¯¯ï¼‰"""
    try:
        with open(filepath, 'rb+') as f:
            f.seek(0, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
            if f.tell() > 0:  # å¦‚æœæ–‡ä»¶ä¸ä¸ºç©º
                f.seek(-1, 2)  # ç§»åŠ¨åˆ°å€’æ•°ç¬¬ä¸€ä¸ªå­—èŠ‚
                if f.read(1) != b'\n':
                    f.write(b'\n')
    except Exception:
        pass


async def main(args):
    """ä¸»å‡½æ•°"""
    # å‡†å¤‡ç¯å¢ƒ
    os.makedirs(args.save_dir, exist_ok=True)
    
    # è¯»å–è¿›åº¦ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    processed_urls: Set[str] = set()
    if os.path.exists(args.output_jsonl):
        print(f"ğŸ“– è¯»å–æ–­ç‚¹ç»­ä¼ æ–‡ä»¶: {args.output_jsonl}")
        _fix_newline(args.output_jsonl)  # ä¿®å¤æœ«å°¾æ¢è¡Œç¬¦
        try:
            with open(args.output_jsonl, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        url = data.get('url')
                        if url:
                            processed_urls.add(url)
                    except json.JSONDecodeError:
                        continue
        except Exception as e:
            print(f"âš ï¸ è¯»å–è¿›åº¦æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    print(f"âœ… å·²åŒ…å« {len(processed_urls)} æ¡è®°å½•")
    STATS["exist"] = len(processed_urls)

    # è¯»å– CSV æ–‡ä»¶åˆ—è¡¨
    if os.path.isfile(args.csv_dir):
        csv_files = [args.csv_dir]
        print(f"ğŸ“¦ ä½¿ç”¨å•ä¸ª CSV æ–‡ä»¶: {os.path.basename(args.csv_dir)}")
    else:
        csv_files = sorted([str(p) for p in Path(args.csv_dir).rglob("*.csv")])
        print(f"ğŸ“¦ æ‰¾åˆ° {len(csv_files)} ä¸ª CSV æ–‡ä»¶")
    
    if args.limit_csvs:
        csv_files = csv_files[:args.limit_csvs]
        print(f"ğŸ“¦ é™åˆ¶å¤„ç†å‰ {args.limit_csvs} ä¸ªæ–‡ä»¶")

    # å¼‚æ­¥å¹¶å‘æ§åˆ¶
    connector = aiohttp.TCPConnector(limit=500, limit_per_host=50)  # è¿æ¥æ± 
    semaphore = asyncio.Semaphore(args.concurrency)
    
    # æ–‡ä»¶å†™å…¥é”
    write_lock = asyncio.Lock()
    
    async with aiohttp.ClientSession(
        connector=connector,
        headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    ) as session:
        
        # ä½¿ç”¨è¿½åŠ æ¨¡å¼æ‰“å¼€è¾“å‡ºæ–‡ä»¶
        file_mode = 'a' if os.path.exists(args.output_jsonl) else 'w'
        async with aiofiles.open(args.output_jsonl, file_mode, encoding='utf-8') as f_out:
            
            for csv_file in csv_files:
                print(f"\nğŸš€ å¤„ç† {os.path.basename(csv_file)}...")
                try:
                    # åˆ†å—è¯»å– CSVï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
                    chunk_iter = pd.read_csv(
                        csv_file,
                        on_bad_lines='skip',
                        chunksize=5000
                    )
                    
                    for chunk_idx, chunk in enumerate(chunk_iter):
                        # é¢„å¤„ç†åˆ—å
                        if 'url' not in chunk.columns:
                            if len(chunk.columns) >= 2:
                                chunk.rename(
                                    columns={
                                        chunk.columns[0]: 'url',
                                        chunk.columns[1]: 'caption'
                                    },
                                    inplace=True
                                )
                        
                        if 'text' in chunk.columns and 'caption' not in chunk.columns:
                            chunk.rename(columns={'text': 'caption'}, inplace=True)
                        
                        # è¿‡æ»¤å·²ä¸‹è½½çš„ URL
                        original_size = len(chunk)
                        chunk = chunk[~chunk['url'].isin(processed_urls)]
                        filtered_count = original_size - len(chunk)
                        
                        if filtered_count > 0:
                            print(f"  â­ï¸  è·³è¿‡ {filtered_count} ä¸ªå·²å¤„ç†çš„ URL")
                        
                        if chunk.empty:
                            continue
                        
                        # åˆ›å»ºä¸‹è½½ä»»åŠ¡
                        tasks = [
                            download_worker(session, row.to_dict(), args.save_dir, semaphore, processed_urls)
                            for _, row in chunk.iterrows()
                        ]
                        
                        STATS["total"] += len(tasks)
                        
                        # å¹¶å‘æ‰§è¡Œå¹¶æ˜¾ç¤ºè¿›åº¦
                        if hasattr(tqdm, 'asyncio'):
                            progress_bar = tqdm.asyncio.tqdm(
                                asyncio.as_completed(tasks),
                                total=len(tasks),
                                desc=f"  Chunk {chunk_idx+1}",
                                leave=False,
                                ncols=80
                            )
                        else:
                            progress_bar = tqdm(
                                total=len(tasks),
                                desc=f"  Chunk {chunk_idx+1}",
                                leave=False,
                                ncols=80
                            )
                        
                        completed_count = 0
                        for coro in asyncio.as_completed(tasks):
                            res = await coro
                            if res:
                                # å¼‚æ­¥å†™å…¥ç»“æœï¼ˆåŠ é”ä¿è¯çº¿ç¨‹å®‰å…¨ï¼‰
                                async with write_lock:
                                    await f_out.write(
                                        json.dumps(res, ensure_ascii=False) + "\n"
                                    )
                                    await f_out.flush()
                            
                            completed_count += 1
                            progress_bar.update(1)
                            
                            # æ¯ 100 ä¸ªæ˜¾ç¤ºä¸€æ¬¡ç»Ÿè®¡
                            if completed_count % 100 == 0:
                                success_rate = (
                                    STATS["success"] / STATS["total"] * 100
                                    if STATS["total"] > 0 else 0
                                )
                                progress_bar.set_postfix({
                                    "âœ…": STATS["success"],
                                    "ğŸ“Š": STATS["total"],
                                    "Rate": f"{success_rate:.1f}%"
                                })
                        
                        progress_bar.close()
                        
                except Exception as e:
                    print(f"âš ï¸ CSV å¤„ç†é”™è¯¯ {csv_file}: {e}")
                    continue

    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ä¸‹è½½å®Œæˆç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"æ€»ä»»åŠ¡æ•°: {STATS['total']}")
    print(f"æˆåŠŸä¸‹è½½: {STATS['success']}")
    print(f"å·²å­˜åœ¨: {STATS['exist']}")
    print(f"æŸåå›¾ç‰‡: {STATS['corrupt']}")
    print(f"ä¸‹è½½å¤±è´¥: {STATS['failed']}")
    print(f"æ— æ•ˆ URL: {STATS['invalid_url']}")
    if STATS['total'] > 0:
        success_rate = (STATS['success'] / STATS['total']) * 100
        print(f"æˆåŠŸç‡: {success_rate:.2f}%")
    print(f"{'='*60}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="[SOTA Standard] å¼‚æ­¥é«˜å¹¶å‘å›¾ç‰‡ä¸‹è½½è„šæœ¬ï¼ˆç§‘ç ”çº§ä¼˜åŒ–ç‰ˆï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "--csv_dir",
        type=str,
        required=True,
        help="CSV æ–‡ä»¶è·¯å¾„æˆ–åŒ…å« CSV æ–‡ä»¶çš„ç›®å½•"
    )
    parser.add_argument(
        "--save_dir",
        type=str,
        required=True,
        help="å›¾ç‰‡ä¿å­˜ç›®å½•"
    )
    parser.add_argument(
        "--output_jsonl",
        type=str,
        required=True,
        help="è¾“å‡º JSONL æ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«ä¸‹è½½æˆåŠŸçš„å›¾ç‰‡å…ƒæ•°æ®ï¼‰"
    )
    parser.add_argument(
        "--concurrency",
        type=int,
        default=100,
        help="å¹¶å‘æ•°ï¼ˆé»˜è®¤: 100ï¼Œå¯æ ¹æ®ç½‘ç»œè°ƒæ•´ï¼‰"
    )
    parser.add_argument(
        "--limit_csvs",
        type=int,
        default=None,
        help="å¦‚æœ csv_dir æ˜¯ç›®å½•ï¼Œåªå¤„ç†å‰ N ä¸ª CSV æ–‡ä»¶"
    )
    parser.add_argument(
        "--validate-existing",
        action="store_true",
        help="éªŒè¯å·²å­˜åœ¨çš„å›¾ç‰‡å®Œæ•´æ€§ï¼ˆä¼šåˆ é™¤æŸåçš„å›¾ç‰‡å¹¶é‡æ–°ä¸‹è½½ï¼Œé€Ÿåº¦è¾ƒæ…¢ä½†æ›´å®‰å…¨ï¼‰"
    )
    
    args = parser.parse_args()
    
    # éªŒè¯è¾“å…¥
    if not os.path.exists(args.csv_dir):
        print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {args.csv_dir}")
        exit(1)
    
    # è¿è¡Œä¸»å‡½æ•°
    asyncio.run(main(args))

