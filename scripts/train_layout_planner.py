#!/usr/bin/env python3
"""
Layout Planner è®­ç»ƒè„šæœ¬

ä½¿ç”¨ layout_dataset_qwen2vl_heuristic_4k.jsonl å¯¹å¸ƒå±€è§„åˆ’å™¨è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚

æ•°æ®æ ¼å¼ï¼ˆJSONL æ¯è¡Œï¼‰ï¼š
{
  "caption": "ä¸Šæ–¹æ˜¯è“å¤©ï¼Œä¸‹æ–¹æ˜¯è‰åœ°",
  "image_path": "xxxxx.jpg",
  "objects": [
    {"name": "å¤©ç©º", "bbox": [0.0, 0.0, 1.0, 0.4]},
    {"name": "è‰åœ°", "bbox": [0.0, 0.6, 1.0, 1.0]}
  ]
}

ä¼šè¢«è½¬æˆè®­ç»ƒæ ·æœ¬ï¼š
input : caption
output: <obj>å¤©ç©º</obj><box>[0.00,0.00,1.00,0.40]</box><obj>è‰åœ°</obj><box>[0.00,0.60,1.00,1.00]</box>

ç„¶åä½¿ç”¨ gill.layout_planner.train_layout_planner è¿›è¡Œ CAUSAL LM è®­ç»ƒã€‚
"""

import os
import sys
import json
import argparse
from typing import List, Dict

import torch
from torch.utils.data import Dataset, DataLoader

# ä¿è¯å¯ä»¥ import gill
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from gill.layout_planner import LayoutPlanner, train_layout_planner  # type: ignore


class LayoutJsonlDataset(Dataset):
    """ä» JSONL å¸ƒå±€æ•°æ®é›†ä¸­æ„é€  Layout Planner æŒ‡ä»¤æ ·æœ¬ã€‚"""

    def __init__(self, jsonl_path: str, max_samples: int = -1):
        self.samples: List[Dict] = []
        assert os.path.exists(jsonl_path), f"JSONL ä¸å­˜åœ¨: {jsonl_path}"

        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    item = json.loads(line)
                except Exception:
                    continue

                # æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
                # 1. CoT æ ¼å¼ï¼šç›´æ¥æœ‰ input/output å­—æ®µï¼ˆæ¥è‡ª prepare_cot_training_data.pyï¼‰
                # 2. ä¼ ç»Ÿæ ¼å¼ï¼šä» caption å’Œ objects æ„é€ 
                if "input" in item and "output" in item:
                    # CoT æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨
                    input_text = str(item.get("input", "")).strip()
                    output_text = str(item.get("output", "")).strip()
                    if input_text and output_text:
                        self.samples.append({"input": input_text, "output": output_text})
                        if max_samples > 0 and len(self.samples) >= max_samples:
                            break
                        continue
                
                # ä¼ ç»Ÿæ ¼å¼ï¼šä» caption å’Œ objects æ„é€ 
                caption = str(item.get("caption", "")).strip()
                objects = item.get("objects", []) or []
                if not caption or not objects:
                    continue

                parts: List[str] = []
                for obj in objects:
                    name = str(obj.get("name", "")).strip() or "ç‰©ä½“"
                    # æ”¯æŒä¸¤ç§ bbox æ ¼å¼ï¼š0-1 æµ®ç‚¹æ•° æˆ– 0-1000 æ•´æ•°
                    bbox = obj.get("bbox", [])
                    bbox_1000 = obj.get("bbox_1000", [])
                    
                    if bbox_1000 and len(bbox_1000) == 4:
                        # ä½¿ç”¨ 0-1000 æ•´æ•°æ ¼å¼
                        bbox_str = ",".join(f"{int(v)}" for v in bbox_1000)
                    elif bbox and len(bbox) == 4:
                        # ä½¿ç”¨ 0-1 æµ®ç‚¹æ•°æ ¼å¼
                        try:
                            bbox_f = [float(v) for v in bbox]
                            bbox_str = ",".join(f"{v:.2f}" for v in bbox_f)
                        except Exception:
                            continue
                    else:
                        continue
                    
                    parts.append(f"<obj>{name}</obj><box>[{bbox_str}]</box>")

                if not parts:
                    continue

                output_text = "".join(parts)
                self.samples.append({"input": caption, "output": output_text})

                if max_samples > 0 and len(self.samples) >= max_samples:
                    break

        print(f"âœ“ ä» {jsonl_path} è¯»å–åˆ° {len(self.samples)} æ¡è®­ç»ƒæ ·æœ¬")

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> Dict:
        return self.samples[idx]


def collate_fn(batch: List[Dict]) -> List[Dict]:
    """ä¿æŒ batch ä¸º list[dict]ï¼Œæ–¹ä¾¿ train_layout_planner ç›´æ¥ä½¿ç”¨ã€‚"""
    return batch


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Layout Planner è®­ç»ƒè„šæœ¬")
    parser.add_argument(
        "--layout-json",
        type=str,
        default="data/layout_dataset_qwen2vl_heuristic_4k.jsonl",
        help="å¸ƒå±€æ•°æ®é›† JSONL è·¯å¾„",
    )
    parser.add_argument(
        "--base-model",
        type=str,
        default="./model/deepseek-llm-7b-base",
        help="Layout Planner åŸºåº§æ¨¡å‹è·¯å¾„",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="è®¾å¤‡ (cuda/cpu/cuda:0,1 ç­‰å¤š GPUï¼Œæˆ– 'auto' è‡ªåŠ¨åˆ†é…)",
    )
    parser.add_argument(
        "--use-lora",
        nargs='?',
        const=True,
        default=True,
        type=lambda x: str(x).lower() == "true",
        help="æ˜¯å¦ä½¿ç”¨ LoRAï¼ˆé»˜è®¤ Trueï¼›ä¼  --use-lora False å…³é—­ï¼Œè‹¥ç¯å¢ƒæœªå®‰è£… peft ä¼šè‡ªåŠ¨é€€å›å…¨å‚æ•°è®­ç»ƒï¼‰",
    )
    parser.add_argument("--batch-size", type=int, default=2, help="batch size")
    parser.add_argument("--epochs", type=int, default=1, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--lr", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument(
        "--max-samples",
        type=int,
        default=-1,
        help="æœ€å¤šä½¿ç”¨å¤šå°‘æ¡æ ·æœ¬ï¼ˆ-1 è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ï¼‰",
    )
    parser.add_argument(
        "--save-dir",
        type=str,
        default="./checkpoints/layout_planner",
        help="ä¿å­˜ LoRA/æ¨¡å‹æƒé‡çš„ç›®å½•",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    os.makedirs(args.save_dir, exist_ok=True)
    
    # å¦‚æœæŒ‡å®šäº†å¤š GPUï¼Œè®¾ç½® CUDA_VISIBLE_DEVICES
    if "," in args.device or args.device == "auto":
        # æå– GPU ç¼–å·ï¼ˆå¦‚ "cuda:0,1" -> ["0", "1"]ï¼‰
        if "," in args.device:
            gpu_ids = args.device.replace("cuda:", "").split(",")
        else:
            # é»˜è®¤ä½¿ç”¨ GPU 0 å’Œ 1
            gpu_ids = ["0", "1"]
        os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(gpu_ids)
        print(f"âœ“ è®¾ç½® CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']}")
        # é‡ç½® device ä¸º "auto"ï¼Œè®© transformers è‡ªåŠ¨åˆ†é…
        actual_device = "auto"
    else:
        actual_device = args.device

    print("=" * 60)
    print("ğŸš€ Layout Planner è®­ç»ƒ")
    print("=" * 60)
    print(f"æ•°æ®é›†: {args.layout_json}")
    print(f"åŸºåº§æ¨¡å‹: {args.base_model}")
    print(f"è®¾å¤‡: {args.device}")
    print(f"batch size: {args.batch_size}, epochs: {args.epochs}, lr: {args.lr}")

    # 1) æ„é€ æ•°æ®é›†ä¸ DataLoader
    dataset = LayoutJsonlDataset(args.layout_json, max_samples=args.max_samples)
    if len(dataset) == 0:
        print("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œé€€å‡º")
        return

    loader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
    )

    # 2) åˆ›å»º LayoutPlannerï¼ˆå¤š GPU æ”¯æŒï¼‰
    planner = LayoutPlanner(args.base_model, device=actual_device, use_lora=args.use_lora)

    # 3) ä¼˜åŒ–å™¨ï¼ˆå…¨å‚æ•°å¾®è°ƒæ—¶ä¼˜å…ˆä½¿ç”¨ 8-bit ä¼˜åŒ–å™¨èŠ‚çœæ˜¾å­˜ï¼‰
    if not args.use_lora:
        try:
            import bitsandbytes as bnb  # type: ignore
            print("âœ“ ä½¿ç”¨ 8-bit AdamW ä¼˜åŒ–å™¨ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
            optimizer = bnb.optim.AdamW8bit(planner.model.parameters(), lr=args.lr)
        except ImportError:
            print("âš ï¸ bitsandbytes æœªå®‰è£…ï¼Œå›é€€åˆ°æ ‡å‡† AdamWï¼ˆæ˜¾å­˜å ç”¨è¾ƒé«˜ï¼‰")
            optimizer = torch.optim.AdamW(planner.model.parameters(), lr=args.lr)
    else:
        optimizer = torch.optim.AdamW(planner.model.parameters(), lr=args.lr)

    # 4) è®­ç»ƒ
    planner = train_layout_planner(
        planner,
        train_loader=loader,
        optimizer=optimizer,
        num_epochs=args.epochs,
        device=actual_device,
    )

    # 5) ä¿å­˜æƒé‡
    try:
        if hasattr(planner.model, "save_pretrained"):
            out_dir = os.path.join(args.save_dir, "final")
            os.makedirs(out_dir, exist_ok=True)
            planner.model.save_pretrained(out_dir)
            if hasattr(planner, "tokenizer") and hasattr(planner.tokenizer, "save_pretrained"):
                planner.tokenizer.save_pretrained(out_dir)
            print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {out_dir}")
        else:
            # å¯¹äº LoRAï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ peft çš„ save_pretrainedï¼Œè¿™é‡Œå…ˆç®€å•ä¿å­˜ state_dict
            out_path = os.path.join(args.save_dir, "planner_model.pt")
            torch.save(planner.model.state_dict(), out_path)
            print(f"âœ“ æ¨¡å‹ state_dict å·²ä¿å­˜åˆ°: {out_path}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")


if __name__ == "__main__":
    main()
