#!/usr/bin/env python3
"""
Layout Planner è®­ç»ƒè„šæœ¬ (Refactored)

ä½¿ç”¨ Hugging Face Trainer å’Œ DataCollatorï¼Œå®ç°æ­£ç¡®çš„ Label Maskingã€‚
"""

import os
import sys
import json
import argparse
import random
from typing import List, Dict
from dataclasses import dataclass
import inspect

import torch
from torch.utils.data import Dataset
from transformers import (
    Trainer, 
    TrainingArguments,
    AutoTokenizer,
    AutoModelForCausalLM,
    EarlyStoppingCallback,
    set_seed
)

# ä¿è¯å¯ä»¥ import gill
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# COCO 80 ç±»åˆ« ID åˆ°ä¸­æ–‡åç§°çš„æ˜ å°„
COCO_CATEGORY_ID_TO_CHINESE = {
    1: "äºº", 2: "è‡ªè¡Œè½¦", 3: "æ±½è½¦", 4: "æ‘©æ‰˜è½¦", 5: "é£æœº", 6: "å…¬äº¤è½¦", 7: "ç«è½¦", 8: "å¡è½¦",
    9: "èˆ¹", 10: "äº¤é€šç¯", 11: "æ¶ˆé˜²æ “", 12: "åœæ­¢æ ‡å¿—", 13: "åœè½¦è®¡æ—¶å™¨", 14: "é•¿æ¤…", 15: "é¸Ÿ",
    16: "çŒ«", 17: "ç‹—", 18: "é©¬", 19: "ç¾Š", 20: "ç‰›", 21: "å¤§è±¡", 22: "ç†Š", 23: "æ–‘é©¬",
    24: "é•¿é¢ˆé¹¿", 25: "èƒŒåŒ…", 26: "é›¨ä¼", 27: "æ‰‹æåŒ…", 28: "é¢†å¸¦", 29: "è¡Œæç®±", 30: "é£ç›˜",
    31: "æ»‘é›ªæ¿", 32: "æ»‘é›ªæ¿", 33: "è¿åŠ¨çƒ", 34: "é£ç­", 35: "æ£’çƒæ£’", 36: "æ£’çƒæ‰‹å¥—",
    37: "æ»‘æ¿", 38: "å†²æµªæ¿", 39: "ç½‘çƒæ‹", 40: "ç“¶å­", 41: "é…’æ¯", 42: "æ¯å­", 43: "å‰å­",
    44: "åˆ€", 45: "å‹ºå­", 46: "ç¢—", 47: "é¦™è•‰", 48: "è‹¹æœ", 49: "ä¸‰æ˜æ²»", 50: "æ©™å­",
    51: "è¥¿å…°èŠ±", 52: "èƒ¡èåœ", 53: "çƒ­ç‹—", 54: "æŠ«è¨", 55: "ç”œç”œåœˆ", 56: "è›‹ç³•", 57: "æ¤…å­",
    58: "æ²™å‘", 59: "ç›†æ ½", 60: "åºŠ", 61: "é¤æ¡Œ", 62: "å•æ‰€", 63: "ç”µè§†", 64: "ç¬”è®°æœ¬ç”µè„‘",
    65: "é¼ æ ‡", 66: "é¥æ§å™¨", 67: "é”®ç›˜", 68: "æ‰‹æœº", 69: "å¾®æ³¢ç‚‰", 70: "çƒ¤ç®±", 71: "çƒ¤é¢åŒ…æœº",
    72: "æ°´æ§½", 73: "å†°ç®±", 74: "ä¹¦", 75: "æ—¶é’Ÿ", 76: "èŠ±ç“¶", 77: "å‰ªåˆ€", 78: "æ³°è¿ªç†Š",
    79: "å¹é£æœº", 80: "ç‰™åˆ·"
}

FORMAT_INSTRUCTION = (
    "è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š<obj>åç§°</obj><box>[x1,y1,x2,y2]</box>... "
    "å¦‚æœæ— æ³•ç»™å‡ºå¸ƒå±€ï¼Œè¯·è¾“å‡º <no_layout>ã€‚åªè¾“å‡ºæ ¼å¼ï¼Œä¸è¦è§£é‡Šã€‚"
)

class LayoutJsonlDataset(Dataset):
    """ä» JSONL å¸ƒå±€æ•°æ®é›†ä¸­æ„é€  Layout Planner æŒ‡ä»¤æ ·æœ¬ã€‚"""
    
    def __init__(self, jsonl_path: str, tokenizer, max_samples: int = -1):
        self.samples = []
        if not os.path.exists(jsonl_path):
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {jsonl_path}")
            return

        print(f"ğŸ“– æ­£åœ¨åŠ è½½: {jsonl_path}")
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                if not line.strip(): continue
                try:
                    item = json.loads(line)
                    
                    # 1. ä¼˜å…ˆå¤„ç† CoT æ ¼å¼ (ç›´æ¥æœ‰ input/output)
                    if "input" in item and "output" in item:
                        inp, out = str(item["input"]).strip(), str(item["output"]).strip()
                        if inp and out:
                            self.samples.append({"input": inp, "output": out})
                        continue

                    # 2. å¤„ç†æ ‡å‡† caption + objects æ ¼å¼
                    inp = str(item.get("caption", "")).strip()
                    objs = item.get("objects", [])
                    if not inp:
                        continue
                    if not isinstance(objs, list):
                        objs = []
                    if not objs:
                        self.samples.append({"input": inp, "output": "<no_layout>"})
                        continue

                    # è·å–å›¾åƒå°ºå¯¸
                    width = float(item.get("width", 0))
                    height = float(item.get("height", 0))
                    has_dim = width > 0 and height > 0

                    out_parts = []
                    for obj in objs:
                        # --- åç§°å¤„ç† ---
                        name = str(obj.get("name", "")).strip()
                        if not name:
                            category_id = obj.get("category_id")
                            if category_id and category_id in COCO_CATEGORY_ID_TO_CHINESE:
                                name = COCO_CATEGORY_ID_TO_CHINESE[category_id]
                            else:
                                name = "ç‰©ä½“"

                        # --- åæ ‡å¤„ç† ---
                        bbox = obj.get("bbox", [])
                        bbox_1000 = obj.get("bbox_1000", [])
                        bbox_final = None 

                        # ä¼˜å…ˆçº§ 1: æ˜ç¡®çš„ 0-1000 æ ¼å¼ï¼ˆæœ€å¯é ï¼‰
                        if bbox_1000 and len(bbox_1000) == 4:
                            bbox_final = [float(v) / 1000.0 for v in bbox_1000]

                        # ä¼˜å…ˆçº§ 2: é€šç”¨ bbox å¤„ç†
                        elif bbox and len(bbox) == 4:
                            bbox_raw = [float(v) for v in bbox]
                            max_val = max(bbox_raw)

                            # æƒ…å†µ A: å·²ç»æ˜¯ 0-1 æ ¼å¼ï¼ˆæœ€ä¼˜å…ˆåˆ¤æ–­ï¼Œé¿å…è¯¯åˆ¤ï¼‰
                            if max_val <= 1.05:
                                bbox_final = bbox_raw
                            
                            # æƒ…å†µ B: æœ‰å®½é«˜ä¿¡æ¯ -> å¼ºåˆ¶æŒ‰åƒç´ å½’ä¸€åŒ–ï¼ˆé’ˆå¯¹ COCO-CN ç­‰åƒç´ åæ ‡ï¼‰
                            elif has_dim:
                                bbox_final = [
                                    bbox_raw[0] / width, 
                                    bbox_raw[1] / height,
                                    bbox_raw[2] / width, 
                                    bbox_raw[3] / height
                                ]
                            
                            # æƒ…å†µ C: æ— å®½é«˜ä¿¡æ¯ï¼Œä½†å€¼ <= 1000 -> åªèƒ½å‡è®¾æ˜¯ 0-1000 æ ¼å¼ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
                            elif max_val <= 1000:
                                bbox_final = [v / 1000.0 for v in bbox_raw]
                            
                            # æƒ…å†µ D: æ— æ³•å¤„ç†ï¼Œè·³è¿‡
                            else:
                                continue

                        if bbox_final:
                            # æˆªæ–­åˆ° 0-1 èŒƒå›´
                            bbox_final = [max(0.0, min(1.0, v)) for v in bbox_final]
                            bbox_str = ",".join(f"{v:.2f}" for v in bbox_final)
                            out_parts.append(f"<obj>{name}</obj><box>[{bbox_str}]</box>")
                    
                    if out_parts:
                        out = "".join(out_parts)
                        self.samples.append({"input": inp, "output": out})

                    if max_samples > 0 and len(self.samples) >= max_samples:
                        break

                except Exception:
                    continue
        
        print(f"âœ“ åŠ è½½ {len(self.samples)} æ¡æ ·æœ¬")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

@dataclass
class DataCollatorForLayoutPlanner:
    """
    å…³é”®ç»„ä»¶ï¼šæ­£ç¡®å¤„ç† Chat Template å’Œ Label Masking
    """
    tokenizer: AutoTokenizer
    max_length: int = 512
    
    def __call__(self, examples: List[Dict]) -> Dict[str, torch.Tensor]:
        input_ids_list = []
        labels_list = []
        
        for example in examples:
            user_input = example["input"]
            if "<obj>" not in user_input and "åªè¾“å‡ºæ ¼å¼" not in user_input:
                user_input = f"{user_input}\n\n{FORMAT_INSTRUCTION}"
            # 1. æ„å»ºå®Œæ•´å¯¹è¯
            messages = [
                {"role": "user", "content": user_input},
                {"role": "assistant", "content": example["output"]}
            ]
            full_text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
            
            # 2. æ„å»º Prompt éƒ¨åˆ†
            user_msg = [{"role": "user", "content": user_input}]
            prompt_text = self.tokenizer.apply_chat_template(user_msg, tokenize=False, add_generation_prompt=True)
            
            # 3. Tokenize
            full_ids = self.tokenizer(full_text, add_special_tokens=False, max_length=self.max_length, truncation=True).input_ids
            prompt_ids = self.tokenizer(prompt_text, add_special_tokens=False, max_length=self.max_length, truncation=True).input_ids
            
            input_ids = torch.tensor(full_ids, dtype=torch.long)
            labels = input_ids.clone()
            
            # 4. Masking
            prompt_len = len(prompt_ids)
            if prompt_len < len(labels):
                labels[:prompt_len] = -100
            else:
                labels[:] = -100
                
            input_ids_list.append(input_ids)
            labels_list.append(labels)
            
        # 5. Padding
        max_len = min(max(len(ids) for ids in input_ids_list), self.max_length)
        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id
        
        input_ids_padded = []
        labels_padded = []
        attention_mask_list = []
        
        for input_ids, labels in zip(input_ids_list, labels_list):
            pad_len = max_len - len(input_ids)
            if pad_len > 0:
                input_ids_padded.append(torch.cat([input_ids, torch.full((pad_len,), pad_token_id, dtype=torch.long)]))
                labels_padded.append(torch.cat([labels, torch.full((pad_len,), -100, dtype=torch.long)]))
                attention_mask_list.append(torch.cat([torch.ones(len(input_ids), dtype=torch.long), torch.zeros(pad_len, dtype=torch.long)]))
            else:
                input_ids_padded.append(input_ids[:max_len])
                labels_padded.append(labels[:max_len])
                attention_mask_list.append(torch.ones(max_len, dtype=torch.long))
        
        return {
            "input_ids": torch.stack(input_ids_padded),
            "labels": torch.stack(labels_padded),
            "attention_mask": torch.stack(attention_mask_list)
        }

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Layout Planner è®­ç»ƒè„šæœ¬")
    parser.add_argument("--layout-json", type=str, default="data/layout_planner_train.jsonl")
    parser.add_argument("--base-model", type=str, default="./model/qwen2.5-7B-Instruct")
    parser.add_argument("--output-dir", type=str, default="./checkpoints/layout_planner")
    parser.add_argument("--epochs", type=int, default=5)
    parser.add_argument("--batch-size", type=int, default=2)
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--max-samples", type=int, default=-1)
    parser.add_argument("--use-lora", action="store_true", default=True)
    parser.add_argument("--gradient-accumulation-steps", type=int, default=4)
    parser.add_argument("--val-json", type=str, default="data/coco-cn/coco-cn_val.jsonl")
    parser.add_argument("--val-split-ratio", type=float, default=0.1)
    parser.add_argument("--early-stopping-patience", type=int, default=2)
    parser.add_argument("--load-best-model-at-end", action="store_true", default=False,
                       help="æ˜¯å¦åŠ è½½éªŒè¯é›†ä¸Šæœ€ä½³æ¨¡å‹ï¼ˆFalse=ä½¿ç”¨æœ€åä¸€ä¸ªepochçš„æ¨¡å‹ï¼Œæ¨èç”¨äºæ ¼å¼è¦æ±‚é«˜çš„ä»»åŠ¡ï¼‰")
    parser.add_argument("--save-total-limit", type=int, default=3,
                       help="ä¿å­˜çš„checkpointæ•°é‡é™åˆ¶")
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­ï¼ˆé»˜è®¤ 42ï¼‰")
    parser.add_argument(
        "--resume-from-checkpoint",
        type=str,
        default=None,
        help="Resume training from a checkpoint directory (e.g., output_dir/checkpoint-xxxx).",
    )
    parser.add_argument(
        "--adapter-path",
        type=str,
        default=None,
        help="Optional LoRA adapter path to warm-start training.",
    )
    parser.add_argument(
        "--optim",
        type=str,
        default="adamw_torch",
        help="Trainer optimizer name (e.g., adamw_torch, adafactor, adamw_hf)"
    )
    parser.add_argument(
        "--max-memory-gb",
        type=float,
        default=None,
        help="Per-GPU max memory in GiB for device_map auto (e.g., 23)."
    )
    parser.add_argument(
        "--device-map",
        type=str,
        default="auto",
        choices=["auto", "none"],
        help="Use model sharding with device_map=auto, or 'none' for DDP (torchrun)."
    )

    parser.add_argument(
        "--gradient-checkpointing",
        action="store_true",
        help="Enable gradient checkpointing to reduce activation memory."
    )
    parser.add_argument(
        "--num-workers",
        type=int,
        default=2,
        help="Dataloader worker processes (0 disables multiprocessing)."
    )
    return parser.parse_args()

def main():
    args = parse_args()
    os.makedirs(args.output_dir, exist_ok=True)
    set_seed(args.seed)
    random.seed(args.seed)
    
    # ... (åˆå§‹åŒ– Tokenizer å’Œ Model éƒ¨åˆ†ä¿æŒä¸å˜ï¼Œæ­¤å¤„çœç•¥ä»¥èŠ‚çœç¯‡å¹…ï¼Œè¯·ä¿ç•™åŸä»£ç ä¸­çš„åŠ è½½é€»è¾‘)
    # ç®€å†™å¦‚ä¸‹ï¼š
    print("\nğŸ“¦ åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(args.base_model, trust_remote_code=True)
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
    tokenizer.add_special_tokens({"additional_special_tokens": ["<obj>", "</obj>", "<box>", "</box>", "<no_layout>"]})
    
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
    local_rank = int(os.environ.get("LOCAL_RANK", "-1"))
    use_device_map = args.device_map != "none"
    if local_rank >= 0 and use_device_map:
        print("DDP detected; forcing device_map=none for torchrun.")
        use_device_map = False

    max_memory = None
    device_map = "auto" if use_device_map else None
    if use_device_map and args.max_memory_gb is not None and torch.cuda.is_available():
        max_memory = {i: f"{args.max_memory_gb}GiB" for i in range(torch.cuda.device_count())}

    model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype=torch.bfloat16,
        device_map=device_map,
        max_memory=max_memory,
        trust_remote_code=True,
    )
    if not use_device_map and torch.cuda.is_available():
        target_device = f"cuda:{local_rank}" if local_rank >= 0 else "cuda"
        model.to(target_device)
    model.resize_token_embeddings(len(tokenizer))
    if args.gradient_checkpointing and hasattr(model, "gradient_checkpointing_enable"):
        model.gradient_checkpointing_enable()
    
    if args.use_lora:
        from peft import LoraConfig, get_peft_model, TaskType, PeftModel
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=32,
            lora_alpha=64,
            lora_dropout=0.05,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            # Keep embeddings and lm_head trainable so new special tokens can be learned.
            modules_to_save=["embed_tokens", "lm_head"],
        )
        if args.adapter_path and os.path.exists(args.adapter_path):
            print("Loading LoRA adapter from: " + str(args.adapter_path))
            model = PeftModel.from_pretrained(model, args.adapter_path, is_trainable=True)
        else:
            model = get_peft_model(model, peft_config)
    
    # å‡†å¤‡æ•°æ®é›†
    print("\nğŸ“– å‡†å¤‡æ•°æ®é›†...")
    train_dataset = LayoutJsonlDataset(args.layout_json, tokenizer, max_samples=args.max_samples)
    
    # å‡†å¤‡éªŒè¯é›†
    if args.val_json and os.path.exists(args.val_json):
        print(f"ğŸ“Š ä½¿ç”¨ç‹¬ç«‹éªŒè¯é›†: {args.val_json}")
        val_dataset = LayoutJsonlDataset(args.val_json, tokenizer, max_samples=-1)
    else:
        # å›é€€é€»è¾‘
        val_size = int(len(train_dataset) * args.val_split_ratio)
        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [len(train_dataset)-val_size, val_size])
    
    collator = DataCollatorForLayoutPlanner(tokenizer=tokenizer, max_length=512)
    
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.lr,
        warmup_ratio=0.03,
        logging_steps=100,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=args.save_total_limit,
        load_best_model_at_end=args.load_best_model_at_end,
        metric_for_best_model="eval_loss", 
        greater_is_better=False,  # loss è¶Šå°è¶Šå¥½
        seed=args.seed,
        data_seed=args.seed,
        bf16=True,
        remove_unused_columns=False,
        dataloader_num_workers=args.num_workers,
        optim=args.optim
    )
    
    trainer_kwargs = dict(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=collator,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=args.early_stopping_patience)]
    )
    sig = inspect.signature(Trainer.__init__)
    if "processing_class" in sig.parameters:
        trainer_kwargs["processing_class"] = tokenizer
    elif "tokenizer" in sig.parameters:
        trainer_kwargs["tokenizer"] = tokenizer
    trainer = Trainer(**trainer_kwargs)
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    if args.resume_from_checkpoint:
        print("Resuming from checkpoint: " + str(args.resume_from_checkpoint))
        trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)
    else:
        trainer.train()
    
    save_path = os.path.join(args.output_dir, "final")
    model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)
    print(f"âœ… å®Œæˆ: {save_path}")

if __name__ == "__main__":
    main()
