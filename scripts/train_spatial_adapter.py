#!/usr/bin/env python3
"""
è®­ç»ƒ Spatial Adapterï¼ˆé€‚é… Kolors/SDXLï¼‰- é˜² NaN å¢å¼ºç‰ˆï¼ˆMixed Precision Trainingï¼‰

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. åŠ è½½ Kolors Pipelineï¼ˆå†»ç»“ UNet/VAE/TextEncoderï¼‰
2. åŠ è½½æ··åˆå¸ƒå±€æ•°æ®é›†ï¼ˆJSONLï¼‰
3. åŠ¨æ€æ³¨å…¥ Spatial Adapter åˆ° UNet çš„æ‰€æœ‰ Attention å±‚
4. æ··åˆç²¾åº¦è®­ç»ƒï¼šFP32 æƒé‡ + FP16 è®¡ç®— + GradScaler

Usage:
    python scripts/train_spatial_adapter.py \
        --mixed-data data/mixed_training_65k.jsonl \
        --kolors-model ./model/Kolors \
        --output-dir ./checkpoints/spatial_adapter_mixed \
        --batch-size 4 \
        --epochs 5
"""

import argparse
import os
import sys
import json
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from PIL import Image
import numpy as np

# å°è¯•å¯¼å…¥ diffusers ç»„ä»¶
try:
    from diffusers import KolorsPipeline, DDPMScheduler
    from diffusers.optimization import get_scheduler
except ImportError:
    print("âŒ æœªå®‰è£… diffusersï¼Œè¯·è¿è¡Œ: pip install diffusers accelerate")
    sys.exit(1)

# ç¡®ä¿å¯ä»¥å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from gill.spatial_adapter import (
    inject_spatial_control_to_unet, 
    remove_spatial_control_from_unet, 
    create_spatial_adapter_for_kolors,
    SpatialAdapterModuleDict
)


class MixedLayoutDataset(Dataset):
    """
    æ··åˆå¸ƒå±€æ•°æ®é›†
    
    æ”¯æŒæ ¼å¼ï¼š
    {
        "image_path": "path/to/img.jpg",
        "caption": "æè¿°æ–‡æœ¬",
        "objects": [{"name": "cat", "bbox": [0.1, 0.1, 0.5, 0.5]}, ...],
        "has_layout": true/false
    }
    """
    def __init__(self, jsonl_path: str, image_dir: str = None, resolution: int = 1024):
        self.samples = []
        self.image_dir = image_dir
        self.resolution = resolution
        
        if not os.path.exists(jsonl_path):
            raise FileNotFoundError(f"JSONL æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_path}")
            
        print(f"ğŸ“– è¯»å–æ•°æ®: {jsonl_path}")
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        item = json.loads(line)
                        if 'caption' in item:
                            self.samples.append(item)
                    except:
                        continue
        print(f"âœ“ åŠ è½½ {len(self.samples)} æ¡æ•°æ®")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        item = self.samples[idx]
        
        # 1. åŠ è½½å›¾åƒ
        image_path = item.get('image_path', '')
        if self.image_dir and not os.path.isabs(image_path):
            image_path = os.path.join(self.image_dir, image_path)
            
        try:
            if os.path.exists(image_path):
                image = Image.open(image_path).convert('RGB')
                image = image.resize((self.resolution, self.resolution))
                pixel_values = torch.from_numpy(np.array(image)).float() / 127.5 - 1.0
                pixel_values = pixel_values.permute(2, 0, 1) # [3, H, W]
                # å¼ºåˆ¶ clamp åˆ° [-1, 1]ï¼Œé˜²æ­¢æç«¯å€¼å¯¼è‡´ VAE NaN
                pixel_values = torch.clamp(pixel_values, -1.0, 1.0)
            else:
                # Dummy image for testing
                pixel_values = torch.randn(3, self.resolution, self.resolution)
                pixel_values = torch.clamp(pixel_values, -1.0, 1.0)
        except Exception as e:
            # å›¾ç‰‡åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å®‰å…¨çš„ dummy æ•°æ®
            pixel_values = torch.randn(3, self.resolution, self.resolution)
            pixel_values = torch.clamp(pixel_values, -1.0, 1.0)

        # 2. å¤„ç† BBoxï¼ˆç»Ÿä¸€å½’ä¸€åŒ–åˆ° 0-1ï¼Œå¹¶è¿‡æ»¤å‡ ä½•æç«¯æ ·æœ¬ï¼‰ï¼ŒåŒæ—¶æ”¶é›†å¯¹è±¡åç§°
        objects = item.get('objects', [])
        bboxes = []
        obj_names = []
        for obj in objects:
            bbox = obj.get('bbox', [])
            if len(bbox) == 4:
                # å…¼å®¹ 0-1000 å’Œ 0-1
                x1, y1, x2, y2 = bbox
                if max(x1, y1, x2, y2) > 1.5:
                    bbox = [x / 1000.0 for x in bbox]
                    x1, y1, x2, y2 = bbox
                
                # å‡ ä½•è¿‡æ»¤ï¼šè¿‡æ»¤æå°æ¡†å’Œå‡ ä¹å…¨å›¾çš„æ¡†
                w = max(0.0, x2 - x1)
                h = max(0.0, y2 - y1)
                area = w * h
                if 0.02 < area < 0.9 and w > 0.03 and h > 0.03:
                    bboxes.append([x1, y1, x2, y2])
                    obj_names.append(obj.get("name", ""))
        
        return {
            'pixel_values': pixel_values,
            'caption': item.get('caption', ''),
            'bboxes': bboxes,
            'obj_names': obj_names,
            'has_layout': len(bboxes) > 0
        }


def collate_mixed_batch(batch):
    # è¿‡æ»¤æ‰ None æ ·æœ¬
    batch = [b for b in batch if b is not None]
    if len(batch) == 0:
        return None

    pixel_values = torch.stack([item['pixel_values'] for item in batch])
    captions = [item['caption'] for item in batch]
    
    max_boxes = max(len(item['bboxes']) for item in batch)
    max_boxes = max(max_boxes, 1)
    
    bboxes_padded = []
    masks = []
    obj_names_batched = []
    
    for item in batch:
        boxes = item['bboxes']
        names = item.get('obj_names', [])
        num_boxes = len(boxes)
        padded = boxes + [[0.0]*4] * (max_boxes - num_boxes)
        bboxes_padded.append(padded)
        masks.append([1]*num_boxes + [0]*(max_boxes - num_boxes))
        # åç§°æŒ‰ç›¸åŒé•¿åº¦ paddingï¼Œç©ºå­—ç¬¦ä¸²è¡¨ç¤ºæ— å¯¹è±¡
        padded_names = names + [""] * (max_boxes - num_boxes)
        obj_names_batched.append(padded_names)
        
    bboxes_tensor = torch.tensor(bboxes_padded, dtype=torch.float32)
    masks_tensor = torch.tensor(masks, dtype=torch.float32)
    
    return {
        'pixel_values': pixel_values,
        'captions': captions,
        'bboxes': bboxes_tensor,
        'masks': masks_tensor,
        'obj_names': obj_names_batched
    }


def _get_add_time_ids(bs, device, original_size=(1024, 1024), target_size=(1024, 1024), crops_coords_top_left=(0, 0)):
    # SDXL/Kolors éœ€è¦çš„ time_ids
    add_time_ids = list(original_size + crops_coords_top_left + target_size)
    add_time_ids = torch.tensor([add_time_ids], dtype=torch.long, device=device)
    return add_time_ids.repeat(bs, 1)


def train_spatial_adapter(
    mixed_data_path: str, 
    kolors_model_path: str, 
    output_dir: str, 
    batch_size: int = 4, 
    epochs: int = 5,
    lr: float = 1e-4, 
    device: str = "cuda:0",
    image_dir: str = None
):
    print(f"ğŸš€ åˆå§‹åŒ– Kolors Spatial Adapter è®­ç»ƒ (Mixed Precision)...")
    print(f"   Model: {kolors_model_path}")
    print(f"   Data: {mixed_data_path}")
    
    # 1. åŠ è½½ç»„ä»¶ (FP16)
    try:
        # æ³¨æ„ï¼šä¸ä¼  variant="fp16"ï¼Œåªä¼  torch_dtypeï¼Œé¿å… IndexError
        pipeline = KolorsPipeline.from_pretrained(
            kolors_model_path, 
            torch_dtype=torch.float16,
            trust_remote_code=True
        ).to(device)

        # ä¿®å¤ Kolors Tokenizer ä¸æ”¯æŒ padding_side å‚æ•°çš„é—®é¢˜
        if hasattr(pipeline, "tokenizer") and pipeline.tokenizer is not None:
            if hasattr(pipeline.tokenizer, "_pad"):
                original_pad = pipeline.tokenizer._pad
                def compatible_pad(encoded_inputs, max_length=None, padding_strategy=None, pad_to_multiple_of=None, return_attention_mask=None, **kwargs):
                    kwargs.pop("padding_side", None)
                    return original_pad(encoded_inputs, max_length, padding_strategy, pad_to_multiple_of, return_attention_mask, **kwargs)
                pipeline.tokenizer._pad = compatible_pad
                print("âœ“ å·²ä¿®å¤ Kolors Tokenizer padding å…¼å®¹æ€§")
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return

    # æå–ç»„ä»¶å¹¶å†»ç»“
    vae = pipeline.vae
    text_encoder = pipeline.text_encoder
    unet = pipeline.unet
    scheduler = DDPMScheduler.from_config(pipeline.scheduler.config)
    
    # ã€ä¿®å¤ã€‘å¼ºåˆ¶ VAE ä½¿ç”¨ FP32ï¼Œé˜²æ­¢ NaNï¼ˆSDXL/Kolors VAE åœ¨ FP16 ä¸‹ä¸ç¨³å®šï¼‰
    vae.to(dtype=torch.float32)
    print("âœ“ VAE å·²åˆ‡æ¢åˆ° FP32 ç²¾åº¦ï¼ˆé˜²æ­¢æ•°å€¼æº¢å‡ºï¼‰")
    
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    unet.requires_grad_(False)
    
    # ã€æ˜¾å­˜ä¼˜åŒ–1ã€‘å¯ç”¨ UNet çš„ gradient checkpointingï¼ˆå¿…é¡»å¯ç”¨ï¼Œå¦åˆ™ OOMï¼‰
    # æ³¨æ„ï¼šcheckpointing å¿…é¡»åœ¨ train æ¨¡å¼ä¸‹æ‰èƒ½å·¥ä½œ
    unet.train()
    if hasattr(unet, 'enable_gradient_checkpointing'):
        unet.enable_gradient_checkpointing()
        print("âœ“ å·²å¯ç”¨ UNet gradient checkpointingï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
    
    # ã€æ˜¾å­˜ä¼˜åŒ–2ã€‘VAE CPU Offloadï¼›Text Encoder å¸¸é©» GPUï¼ˆæ­¤å‰åå¤ .to è¿‡æ…¢å¹¶æ›¾è§¦å‘ä¸­æ–­ï¼‰
    # UNet å¿…é¡»ä¸€ç›´åœ¨ GPUï¼ˆå› ä¸ºè®­ç»ƒå¾ªç¯ä¸­å¤šæ¬¡è°ƒç”¨ï¼‰
    vae.to("cpu", dtype=torch.float32)
    text_encoder.to(device, dtype=torch.float16)
    unet.to(device, dtype=torch.float16)
    print("âœ“ å·²å¯ç”¨ VAE CPU Offloadï¼›Text Encoder/UNet å¸¸é©» GPU")
    
    # 2. åˆå§‹åŒ– Adapter (ä¿æŒ FP32 ä»¥ç¨³å®šè®­ç»ƒ)
    print("ğŸ“¦ åˆå§‹åŒ– Adapter å®¹å™¨ (FP32)...")
    adapter_container = create_spatial_adapter_for_kolors() 
    adapter_container.to(device, dtype=torch.float32)  # æ˜ç¡®æŒ‡å®š FP32
    
    # 3. ä¼˜åŒ–å™¨ï¼ˆä¸ä½¿ç”¨ GradScalerï¼Œå› ä¸º Adapter æ˜¯ FP32ï¼Œä¸éœ€è¦æ··åˆç²¾åº¦ï¼‰
    # æ³¨æ„ï¼šUNet æ˜¯ FP16 ä½†è¢«å†»ç»“ï¼Œåªæœ‰ Adapter (FP32) éœ€è¦æ¢¯åº¦
    optimizer = None 
    
    # 4. æ•°æ®åŠ è½½
    dataset = MixedLayoutDataset(mixed_data_path, image_dir=image_dir)
    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        collate_fn=collate_mixed_batch,
        num_workers=0
    )
    
    os.makedirs(output_dir, exist_ok=True)
    global_step = 0
    
    for epoch in range(epochs):
        unet.train()
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}")
        
        for batch in progress_bar:
            if batch is None:
                continue
                
            # --- A. å‡†å¤‡ Latents (VAE CPU Offload) ---
            with torch.no_grad():
                # ã€æ˜¾å­˜ä¼˜åŒ–ã€‘ä¸´æ—¶å°† VAE ç§»åˆ° GPU
                vae.to(device)
                torch.cuda.empty_cache()  # æ¸…ç†ç¢ç‰‡
                
                # ã€ä¿®å¤ã€‘å›¾åƒè½¬ä¸º FP32 è¿›å…¥ VAEï¼ˆVAE å¿…é¡»ç”¨ FP32ï¼‰
                pixel_values = batch['pixel_values'].to(device, dtype=torch.float32)
                # å†æ¬¡ clampï¼Œç¡®ä¿è¾“å…¥ VAE çš„å€¼åœ¨åˆæ³•èŒƒå›´
                pixel_values = torch.clamp(pixel_values, -1.0, 1.0)
                
                try:
                    latents = vae.encode(pixel_values).latent_dist.sample()
                    latents = latents * vae.config.scaling_factor
                    
                    # ã€é‡è¦ã€‘ç¼–ç å®Œåå†è½¬å› FP16 ç»™ UNet ç”¨ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
                    latents = latents.to(dtype=torch.float16)
                    
                    # æ•°æ®æ£€æŸ¥ï¼šé˜²æ­¢åæ•°æ®å¯¼è‡´çš„ NaN
                    if torch.isnan(latents).any() or torch.isinf(latents).any():
                        print(f"âš ï¸ è­¦å‘Š: æ£€æµ‹åˆ° VAE è¾“å‡º NaN/Infï¼Œè·³è¿‡æ­¤ Batch (step {global_step})")
                        vae.to("cpu")  # å‡ºé”™ä¹Ÿè¦ç§»å› CPU
                        continue
                except Exception as e:
                    print(f"âš ï¸ è­¦å‘Š: VAE ç¼–ç å¤±è´¥: {e}ï¼Œè·³è¿‡æ­¤ Batch (step {global_step})")
                    vae.to("cpu")  # å‡ºé”™ä¹Ÿè¦ç§»å› CPU
                    continue
                finally:
                    # ã€æ˜¾å­˜ä¼˜åŒ–ã€‘VAE ç”¨å®Œç«‹å³ç§»å› CPUï¼Œé‡Šæ”¾æ˜¾å­˜
                    vae.to("cpu")
                    torch.cuda.empty_cache()

                noise = torch.randn_like(latents)
                bs = latents.shape[0]
                timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (bs,), device=device).long()
                noisy_latents = scheduler.add_noise(latents, noise, timesteps)
                
                # --- B. å‡†å¤‡ Text Embeddings (Text Encoder å¸¸é©» GPU) ---
                try:
                    encoded = pipeline.encode_prompt(
                        prompt=batch['captions'], 
                        device=device,
                        num_images_per_prompt=1,
                        do_classifier_free_guidance=False 
                    )
                    
                    if isinstance(encoded, tuple) and len(encoded) >= 3:
                        prompt_embeds = encoded[0]
                        pooled_embeds = encoded[2]
                    else:
                        continue
                except Exception as e:
                    print(f"âš ï¸ è­¦å‘Š: Text Encoder ç¼–ç å¤±è´¥: {e}ï¼Œè·³è¿‡æ­¤ Batch (step {global_step})")
                    continue
                
                # === é¢å¤–ï¼šç¼–ç ç‰©ä½“åç§°ä¸º phrase_embeddings (ç”¨äºè¯­ä¹‰ç»‘å®š) ===
                obj_names_batch = batch.get('obj_names', [])
                max_boxes = batch['bboxes'].shape[1]
                text_hidden = getattr(text_encoder.config, "hidden_size", 4096)
                
                # å±•å¹³åç§°åˆ—è¡¨ï¼Œç©ºå­—ç¬¦ä¸²ä¿ç•™ä¸ºå ä½
                flat_names = [name for names in obj_names_batch for name in names]
                phrase_emb_batch = torch.zeros((len(flat_names), text_hidden), device=device, dtype=torch.float32)
                
                valid_indices = [i for i, n in enumerate(flat_names) if isinstance(n, str) and len(n.strip()) > 0]
                if len(valid_indices) > 0:
                    valid_names = [flat_names[i] for i in valid_indices]
                    try:
                        tok_inputs = pipeline.tokenizer(
                            valid_names,
                            padding=True,
                            truncation=True,
                            return_tensors="pt"
                        ).to(device)
                        with torch.no_grad():
                            tok_outputs = text_encoder(**tok_inputs)
                            attn_mask = tok_inputs.attention_mask.unsqueeze(-1)
                            # mean pooling
                            embs = (tok_outputs.last_hidden_state * attn_mask).sum(dim=1) / attn_mask.sum(dim=1).clamp(min=1e-6)
                            phrase_emb_batch[valid_indices] = embs.to(dtype=torch.float32)
                    except Exception as e:
                        print(f"âš ï¸ è­¦å‘Š: Phrase embedding ç¼–ç å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é›¶å‘é‡ (step {global_step})")
                
                phrase_embeddings = phrase_emb_batch.view(bs, max_boxes, text_hidden).to(dtype=torch.float32)

            # --- C. æ³¨å…¥ Spatial Control ---
            bboxes = batch['bboxes'].to(device, dtype=torch.float32) # Adapter æœŸæœ› FP32 è®¡ç®—
            
            # åŠ¨æ€æ³¨å…¥ (Adapter ä¿æŒ FP32)
            orig_procs, spatial_procs, adapter_container = inject_spatial_control_to_unet(
                unet, 
                adapter_dict=adapter_container, 
                bboxes=bboxes,
                phrase_embeddings=phrase_embeddings
            )
            
            # --- å»¶è¿Ÿåˆå§‹åŒ–ä¼˜åŒ–å™¨ ---
            if optimizer is None:
                params_to_optimize = [p for p in adapter_container.parameters() if p.requires_grad]
                if len(params_to_optimize) == 0:
                    for p in adapter_container.parameters(): 
                        p.requires_grad = True
                    params_to_optimize = adapter_container.parameters()
                
                optimizer = torch.optim.AdamW(params_to_optimize, lr=lr, eps=1e-4, weight_decay=0.0)
                print(f"âœ“ ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆï¼Œå‚æ•°é‡: {sum(p.numel() for p in params_to_optimize)}")

            # --- D. Forward (Autocast for UNet only) ---
            added_cond_kwargs = {
                "text_embeds": pooled_embeds, 
                "time_ids": _get_add_time_ids(bs, device)
            }
            
            # å¼€å¯ Autocastï¼šUNet ä½¿ç”¨ FP16ï¼Œä½† Adapter ä¿æŒ FP32
            # æ³¨æ„ï¼šAdapter åœ¨ UNet å†…éƒ¨è¢«è°ƒç”¨ï¼Œä½†å‚æ•°æ˜¯ FP32ï¼Œè®¡ç®—ä¹Ÿä¼šä¿æŒ FP32
            with torch.amp.autocast('cuda', dtype=torch.float16):
                model_pred = unet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states=prompt_embeds,
                    added_cond_kwargs=added_cond_kwargs
                ).sample
                
                # è®¡ç®— Lossï¼ˆè½¬æ¢ä¸º FP32 ä»¥ç¡®ä¿ç²¾åº¦ï¼‰
                loss = F.mse_loss(model_pred.float(), noise.float(), reduction="mean")

            # loss æ•°å€¼æ£€æŸ¥ï¼Œé˜²æ­¢ NaN/Inf è¿›å…¥ backward
            if not torch.isfinite(loss):
                print(f"âš ï¸ è­¦å‘Š: loss éæœ‰é™ (step {global_step})ï¼Œè·³è¿‡æ­¤ Batch")
                optimizer.zero_grad()
                remove_spatial_control_from_unet(unet, orig_procs)
                continue
            
            # --- E. Backward (ç›´æ¥ backwardï¼Œä¸ä½¿ç”¨ Scaler) ---
            # å› ä¸º Adapter æ˜¯ FP32ï¼Œä¸éœ€è¦æ··åˆç²¾åº¦è®­ç»ƒ
            # UNet è¢«å†»ç»“ï¼Œåªæœ‰ Adapter éœ€è¦æ¢¯åº¦
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰å¹¶æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰æ•ˆ
            grad_norm = torch.nn.utils.clip_grad_norm_(adapter_container.parameters(), 0.5)
            if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                print(f"âš ï¸ è­¦å‘Š: æ£€æµ‹åˆ° NaN/Inf æ¢¯åº¦ï¼Œè·³è¿‡æ­¤ Batch (step {global_step})")
                optimizer.zero_grad()
                continue
            
            # æ›´æ–°å‚æ•°
            optimizer.step()
            optimizer.zero_grad()
            
            # --- F. æ¸…ç† ---
            remove_spatial_control_from_unet(unet, orig_procs)
            
            global_step += 1
            progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})
            
            if global_step % 500 == 0:
                save_path = os.path.join(output_dir, f"checkpoint-{global_step}.pt")
                torch.save(adapter_container.state_dict(), save_path)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(output_dir, "spatial_adapter_final.pt")
    torch.save(adapter_container.state_dict(), final_path)
    print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {final_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--mixed-data", type=str, required=True, help="æ··åˆæ•°æ®é›† JSONL è·¯å¾„")
    parser.add_argument("--kolors-model", type=str, default="./model/Kolors")
    parser.add_argument("--output-dir", type=str, default="./checkpoints/spatial_adapter_mixed")
    parser.add_argument("--batch-size", type=int, default=2)
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--image-dir", type=str, default=None)
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="æŒ‡å®š GPU è®¾å¤‡ (ä¾‹å¦‚: cuda:0, cuda:1, cuda:2)ã€‚é»˜è®¤è‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨ GPU"
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šè®¾å¤‡
    if args.device:
        device = args.device
    elif torch.cuda.is_available():
        device = "cuda:0"  # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ª GPU
    else:
        device = "cpu"
    
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    train_spatial_adapter(
        args.mixed_data,
        args.kolors_model,
        args.output_dir,
        args.batch_size,
        args.epochs,
        args.lr,
        device=device,
        image_dir=args.image_dir
    )
