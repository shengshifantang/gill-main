#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ··åˆç­›é€‰æ–¹æ¡ˆï¼šå…³é”®è¯å¿«é€Ÿé¢„ç­›é€‰ + Qwen ç²¾ç¡®ç­›é€‰
åŠŸèƒ½ï¼š
 1. ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨å…³é”®è¯å¿«é€Ÿè¿‡æ»¤ï¼Œå‡å°‘éœ€è¦ Qwen å¤„ç†çš„æ•°æ®é‡
 2. ç¬¬äºŒæ­¥ï¼šå¯¹é¢„ç­›é€‰ç»“æœä½¿ç”¨ Qwen æ¨¡å‹ç²¾ç¡®åˆ¤æ–­
 3. æé«˜æ•ˆç‡ï¼ŒèŠ‚çœè®¡ç®—èµ„æº

ç”¨æ³•ç¤ºä¾‹ï¼š
python scripts/filter_wukong_hybrid.py \
  --input_dir /mnt/disk/lxh/gill_data/wukong_release/wukong_release \
  --output_csv /mnt/disk/lxh/gill_data/wukong_filtered_spatial.csv \
  --model Qwen/Qwen2.5-7B-Instruct \
  --device cuda \
  --target_samples 20000
"""

import os
import pandas as pd
import argparse
from pathlib import Path
from tqdm import tqdm
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Optional, List

# æ‰©å±•çš„ä¸­æ–‡å…³é”®è¯åº“ï¼ˆæå‡å¬å›ç‡ï¼Œå®å¯å¤šé€‰ä¸å¯æ¼é€‰ï¼‰
# é’ˆå¯¹ä¸­æ–‡åœºæ™¯ä¼˜åŒ–ï¼ŒåŒ…å«ä»‹è¯ç»„åˆã€åŠ¨è¯æ–¹ä½ã€æ•°é‡è¯ç­‰

# å¼ºæ–¹ä½è¯ï¼ˆæ˜ç¡®çš„ç©ºé—´ä½ç½®ï¼‰
STRONG_KEYWORDS = [
    # åŸºç¡€æ–¹ä½
    "å·¦", "å³", "ä¸Š", "ä¸‹", "ä¸­é—´", "ä¸­å¤®", "é¡¶éƒ¨", "åº•éƒ¨",
    "å·¦ä¸Š", "å·¦ä¸‹", "å³ä¸Š", "å³ä¸‹", "ä¸Šæ–¹", "ä¸‹æ–¹", "å·¦ä¾§", "å³ä¾§",
    "æ­£ä¸­å¤®", "æ­£ä¸­é—´", "æœ€ä¸Šæ–¹", "æœ€ä¸‹æ–¹", "æœ€å·¦è¾¹", "æœ€å³è¾¹",
    
    # ä»‹è¯ç»„åˆï¼ˆæå‡å¬å›ç‡ï¼‰
    "åœ¨å·¦è¾¹", "åœ¨å³ä¾§", "åœ¨ä¸‹æ–¹", "åœ¨ä¸Šæ–¹", "åœ¨ä¸­é—´", "åœ¨ä¸­å¤®",
    "ä½äºå·¦ä¾§", "ä½äºå³ä¾§", "ä½äºä¸Šæ–¹", "ä½äºä¸‹æ–¹", "ä½äºä¸­é—´",
    "æ”¾åœ¨å·¦è¾¹", "æ”¾åœ¨å³è¾¹", "æ”¾åœ¨ä¸Šæ–¹", "æ”¾åœ¨ä¸‹æ–¹",
    "ç½®äºå·¦ä¾§", "ç½®äºå³ä¾§", "ç½®äºä¸­å¤®",
    
    # åŠ¨è¯æ–¹ä½ï¼ˆéšå«ä½ç½®å…³ç³»ï¼‰
    "åäº", "ä½äº", "ç½®äº", "æ”¾åœ¨", "æ‘†åœ¨", "æŒ‚åœ¨", "è´´åœ¨",
    "æ’åˆ—åœ¨", "åˆ†å¸ƒåœ¨", "åˆ†æ•£åœ¨", "é›†ä¸­åœ¨",
    
    # æ•°é‡è¯ç»„åˆï¼ˆå¤šç‰©ä½“é€šå¸¸éšå«ä½ç½®å…³ç³»ï¼‰
    "ä¸¤ä¸ª", "ä¸‰ä¸ª", "å››ä¸ª", "å¤šä¸ª", "ä¸€å¯¹", "ä¸¤å¯¹", "ä¸€ç»„", "ä¸¤ç»„",
    "ä¸¤åª", "ä¸‰åª", "å››åª", "å‡ ä¸ª", "æ•°ä¸ª",
    
    # ç›¸å¯¹ä½ç½®æè¿°
    "ä¹‹é—´", "ä¹‹ä¸­", "ä¹‹å†…", "ä¹‹å¤–", "ä¹‹å‰", "ä¹‹å",
    "ç›¸å¯¹", "ç›¸å¯¹ä½ç½®", "ç›¸å¯¹å…³ç³»",
    
    # å¸ƒå±€æè¿°è¯
    "å¸ƒå±€", "æ’åˆ—", "åˆ†å¸ƒ", "æ’åˆ—æ–¹å¼", "ç©ºé—´å¸ƒå±€",
    "æ¨ªå‘", "çºµå‘", "æ°´å¹³", "å‚ç›´", "å¯¹ç§°", "ä¸å¯¹ç§°"
]

# å¼±æ–¹ä½è¯ï¼ˆæ¨¡ç³Šçš„ç©ºé—´å…³ç³»ï¼‰
WEAK_KEYWORDS = [
    # æ¨¡ç³Šæ–¹ä½
    "æ—è¾¹", "å‘¨å›´", "å››å‘¨", "ç¯ç»•", "å¯¹è§’", "ä¾§é¢",
    "èƒŒæ™¯", "å‰æ™¯", "é™„è¿‘", "å‘¨è¾¹", "é‚»è¿‘",
    
    # ç¯å¢ƒæè¿°
    "ç¯å¢ƒ", "åœºæ™¯", "å‘¨å›´ç¯å¢ƒ", "èƒŒæ™¯ä¸­", "å‰æ™¯ä¸­",
    "å‘¨å›´æœ‰", "é™„è¿‘æœ‰", "å‘¨è¾¹æœ‰",
    
    # åˆ†å¸ƒæè¿°
    "åˆ†æ•£", "é›†ä¸­", "èšé›†", "å›´ç»•", "åŒ…å›´",
    "é›¶æ˜Ÿ", "å¯†é›†", "ç¨€ç–",
    
    # ç›¸å¯¹å…³ç³»ï¼ˆå¼±ï¼‰
    "ç›¸å¯¹", "ç›¸å¯¹äº", "å¯¹æ¯”", "å¯¹æ¯”äº"
]

# ç›´æ¥å®ç°ï¼Œé¿å…å¯¼å…¥é—®é¢˜
def load_model(model_name: str, device: str = "cuda"):
    """åŠ è½½ Qwen æ–‡æœ¬æ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰"""
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch
    import os
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„ï¼ˆæ”¯æŒå¤§å°å†™ä¸æ•æ„ŸåŒ¹é…ï¼‰
    is_local = False
    original_model_name = model_name
    if os.path.exists(model_name) and os.path.isdir(model_name):
        is_local = True
    else:
        # å°è¯•å¤§å°å†™ä¸æ•æ„ŸåŒ¹é…ï¼ˆå¦‚æœè·¯å¾„æ˜¯ç»å¯¹è·¯å¾„ä¸”åŒ…å« model ç›®å½•ï¼‰
        if os.path.isabs(model_name) and 'model' in model_name.lower():
            parent_dir = os.path.dirname(model_name)
            model_basename = os.path.basename(model_name)
            if os.path.exists(parent_dir):
                # åœ¨çˆ¶ç›®å½•ä¸­æŸ¥æ‰¾å¤§å°å†™ä¸åŒ¹é…çš„ç›®å½•
                for item in os.listdir(parent_dir):
                    if item.lower() == model_basename.lower() and os.path.isdir(os.path.join(parent_dir, item)):
                        model_name = os.path.join(parent_dir, item)
                        is_local = True
                        print(f"  ğŸ” æ£€æµ‹åˆ°å¤§å°å†™ä¸åŒ¹é…ï¼Œè‡ªåŠ¨ä¿®æ­£è·¯å¾„: {model_name}")
                        break
    
    if is_local:
        print(f"ğŸ“¦ ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: {model_name}")
        # æœ¬åœ°æ¨¡å‹ï¼šç¦ç”¨ç½‘ç»œè¯·æ±‚
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            local_files_only=True,  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            padding_side='left'  # decoder-onlyæ¨¡å‹ä½¿ç”¨left padding
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
            device_map=device if device == "cuda" else None,
            trust_remote_code=True,
            local_files_only=True  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )
    else:
        print(f"ğŸ“¦ ä» HuggingFace åŠ è½½æ¨¡å‹: {model_name}")
        # è¿œç¨‹æ¨¡å‹ï¼šä¸´æ—¶ç¦ç”¨ä»£ç†ï¼ˆå¦‚æœä»£ç†æœåŠ¡æœªè¿è¡Œï¼‰
        old_proxy = os.environ.get('HTTP_PROXY')
        old_https_proxy = os.environ.get('HTTPS_PROXY')
        old_http_proxy = os.environ.get('http_proxy')
        old_https_proxy_lower = os.environ.get('https_proxy')
        try:
            # ä¸´æ—¶ç¦ç”¨æ‰€æœ‰ä»£ç†ç¯å¢ƒå˜é‡ï¼ˆé¿å…ä»£ç†è¿æ¥å¤±è´¥ï¼‰
            if old_proxy:
                os.environ.pop('HTTP_PROXY', None)
            if old_https_proxy:
                os.environ.pop('HTTPS_PROXY', None)
            if old_http_proxy:
                os.environ.pop('http_proxy', None)
            if old_https_proxy_lower:
                os.environ.pop('https_proxy', None)
            
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                padding_side='left'  # decoder-onlyæ¨¡å‹ä½¿ç”¨left padding
            )
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
                device_map=device if device == "cuda" else None,
                trust_remote_code=True
            )
        finally:
            # æ¢å¤ä»£ç†è®¾ç½®
            if old_proxy:
                os.environ['HTTP_PROXY'] = old_proxy
            if old_https_proxy:
                os.environ['HTTPS_PROXY'] = old_https_proxy
            if old_http_proxy:
                os.environ['http_proxy'] = old_http_proxy
            if old_https_proxy_lower:
                os.environ['https_proxy'] = old_https_proxy_lower
    
    if device == "cpu":
        model = model.to(device)
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    return model, tokenizer

def check_caption_with_qwen(caption: str, model, tokenizer, device: str = "cuda") -> tuple:
    """
    ä½¿ç”¨ Qwen æ¨¡å‹åˆ¤æ–­ captionï¼ˆè®ºæ–‡çº§ CoT Promptï¼‰
    
    Returns:
        (type, reason): type ä¸º "strong"/"weak"/"none", reason ä¸ºåˆ¤æ–­ç†ç”±
    """
    if not isinstance(caption, str) or len(caption.strip()) < 3:
        return (None, None)
    
    # è®ºæ–‡çº§ Promptï¼ˆChain-of-Thoughtï¼Œè§†è§‰å¯¼å‘ï¼‰
    prompt = f"""ä½œä¸ºä¸€ä¸ªè§†è§‰æ•°æ®é›†ä¸“å®¶ï¼Œè¯·åˆ¤æ–­ä»¥ä¸‹å›¾åƒæè¿°æ˜¯å¦åŒ…å«**å…·ä½“çš„ã€å¯è§†è§‰åŒ–çš„**ç‰©ä½“ç©ºé—´å…³ç³»ã€‚

æè¿°ï¼š{caption}

åˆ¤åˆ«æ ‡å‡†ï¼ˆè¯·é€æ­¥æ€è€ƒï¼‰ï¼š
1. **å®ä½“è¦æ±‚**ï¼šå¿…é¡»åŒ…å«è‡³å°‘ä¸¤ä¸ªå…·ä½“çš„ç‰©ç†å®ä½“ï¼ˆå¦‚ï¼šäººã€ç‰©ä½“ã€åŠ¨ç‰©ç­‰ï¼‰ï¼Œè€Œä¸æ˜¯æŠ½è±¡æ¦‚å¿µã€‚
2. **ä½ç½®è¦æ±‚**ï¼šå¿…é¡»åŒ…å«æ˜ç¡®çš„ç›¸å¯¹ä½ç½®æè¿°ï¼ˆå¦‚ï¼šå·¦è¾¹ã€å³è¾¹ã€ä¸Šæ–¹ã€ä¸‹æ–¹ã€ä¸­é—´ã€å‘¨å›´ç­‰ï¼‰ã€‚
3. **æ’é™¤è§„åˆ™**ï¼š
   - æŠ½è±¡æ¦‚å¿µï¼ˆå¦‚"ç¤¾ä¼šåº•å±‚"ã€"å·¦ç¿¼æ€æƒ³"ã€"å¿ƒåº•"ï¼‰â†’ åˆ¤ä¸º none
   - å•ä¸€ç‰©ä½“å±…ä¸­ï¼ˆå¦‚"ä¸­é—´æ˜¯ä¸€æœµèŠ±"ï¼‰â†’ åˆ¤ä¸º strongï¼ˆæœ‰æ•ˆï¼‰
   - æ— ç©ºé—´å…³ç³»çš„å¹¶åˆ—æè¿°ï¼ˆå¦‚"æœ‰çŒ«å’Œç‹—"ï¼‰â†’ åˆ¤ä¸º none
4. **åˆ†ç±»è§„åˆ™**ï¼š
   - æ˜ç¡®æ–¹ä½è¯ï¼ˆå·¦ã€å³ã€ä¸Šã€ä¸‹ã€ä¸­é—´ã€é¡¶éƒ¨ã€åº•éƒ¨ã€å·¦ä¸Šã€å³ä¸‹ç­‰ï¼‰â†’ strong
   - æ¨¡ç³Šæ–¹ä½è¯ï¼ˆæ—è¾¹ã€å‘¨å›´ã€å››å‘¨ã€ç¯ç»•ã€èƒŒæ™¯ã€å‰æ™¯ç­‰ï¼‰â†’ weak
   - æ— ä½ç½®ä¿¡æ¯æˆ–ä¸ç¬¦åˆä¸Šè¿°è¦æ±‚ â†’ none

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
{{"type": "strong/weak/none", "reason": "ç®€çŸ­åˆ¤æ–­ç†ç”±ï¼ˆ1-2å¥è¯ï¼‰"}}"""
    
    try:
        messages = [{"role": "user", "content": prompt}]
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer(text, return_tensors="pt", padding=True).to(device)
        
        with torch.no_grad():
            # ä½¿ç”¨ generate çš„å‚æ•°ï¼Œæ˜ç¡®ç¦ç”¨é‡‡æ ·ç›¸å…³å‚æ•°ä»¥é¿å…è­¦å‘Š
            generation_config = model.generation_config if hasattr(model, 'generation_config') else None
            outputs = model.generate(
                **inputs,
                max_new_tokens=128,  # å¢åŠ ä»¥å®¹çº³ JSON å’Œ reason
                do_sample=False,  # ä½¿ç”¨è´ªå¿ƒè§£ç 
                temperature=None,  # æ˜ç¡®è®¾ç½®ä¸º None
                top_p=None,  # æ˜ç¡®è®¾ç½®ä¸º None
                top_k=None,  # æ˜ç¡®è®¾ç½®ä¸º None
            )
        
        response = tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        ).strip()
        
        # è§£æ JSON å“åº”
        import json
        import re
        
        # å°è¯•æå– JSON
        json_match = re.search(r'\{[^}]+\}', response, re.DOTALL)
        if json_match:
            try:
                result = json.loads(json_match.group())
                result_type = result.get('type', '').lower()
                reason = result.get('reason', '')
                
                if result_type in ['strong', 'weak', 'none']:
                    return (result_type, reason)
            except:
                pass
        
        # å›é€€åˆ°ç®€å•åŒ¹é…
        response_lower = response.lower()
        if "strong" in response_lower:
            return ("strong", "åŒ…å«æ˜ç¡®æ–¹ä½è¯")
        elif "weak" in response_lower:
            return ("weak", "åŒ…å«æ¨¡ç³Šæ–¹ä½è¯")
        else:
            return ("none", "æ— ç©ºé—´å…³ç³»")
            
    except Exception as e:
        # å¦‚æœæ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯åŒ¹é…
        caption_lower = caption.lower() if isinstance(caption, str) else ""
        has_strong = any(k in caption_lower for k in STRONG_KEYWORDS) if caption_lower else False
        has_weak = any(k in caption_lower for k in WEAK_KEYWORDS) if caption_lower else False
        
        if has_strong:
            return ("strong", "æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…")
        elif has_weak:
            return ("weak", "æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…")
        else:
            return ("none", "æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œæ— åŒ¹é…å…³é”®è¯")

def fallback_keyword_check(caption: str) -> Optional[str]:
    """å…³é”®è¯åŒ¹é…çš„å¤‡ç”¨æ–¹æ¡ˆ"""
    if not isinstance(caption, str):
        return None
    
    caption_lower = caption.lower()
    
    has_strong = any(k in caption_lower for k in STRONG_KEYWORDS)
    has_weak = any(k in caption_lower for k in WEAK_KEYWORDS)
    
    if has_strong:
        return "strong"
    elif has_weak:
        return "weak"
    return None

def process_batch_truly(captions: List[str], model, tokenizer, device: str = "cuda", batch_size: int = 32) -> List[tuple]:
    """
    çœŸæ­£çš„æ‰¹å¤„ç†ï¼šä¸€æ¬¡å¤„ç†å¤šä¸ªcaptionï¼Œå……åˆ†åˆ©ç”¨GPU
    4090 24GBæ˜¾å­˜å¯ä»¥æ”¯æŒæ›´å¤§çš„batch_size
    """
    import json
    import re
    
    if not captions:
        return []
    
    results = []
    total = len(captions)
    
    # æ„å»ºæ‰¹å¤„ç†prompt
    prompt_template = """ä½œä¸ºä¸€ä¸ªè§†è§‰æ•°æ®é›†ä¸“å®¶ï¼Œè¯·åˆ¤æ–­ä»¥ä¸‹å›¾åƒæè¿°æ˜¯å¦åŒ…å«**å…·ä½“çš„ã€å¯è§†è§‰åŒ–çš„**ç‰©ä½“ç©ºé—´å…³ç³»ã€‚

åˆ¤åˆ«æ ‡å‡†ï¼ˆè¯·é€æ­¥æ€è€ƒï¼‰ï¼š
1. **å®ä½“è¦æ±‚**ï¼šå¿…é¡»åŒ…å«è‡³å°‘ä¸¤ä¸ªå…·ä½“çš„ç‰©ç†å®ä½“ï¼ˆå¦‚ï¼šäººã€ç‰©ä½“ã€åŠ¨ç‰©ç­‰ï¼‰ï¼Œè€Œä¸æ˜¯æŠ½è±¡æ¦‚å¿µã€‚
2. **ä½ç½®è¦æ±‚**ï¼šå¿…é¡»åŒ…å«æ˜ç¡®çš„ç›¸å¯¹ä½ç½®æè¿°ï¼ˆå¦‚ï¼šå·¦è¾¹ã€å³è¾¹ã€ä¸Šæ–¹ã€ä¸‹æ–¹ã€ä¸­é—´ã€å‘¨å›´ç­‰ï¼‰ã€‚
3. **æ’é™¤è§„åˆ™**ï¼š
   - æŠ½è±¡æ¦‚å¿µï¼ˆå¦‚"ç¤¾ä¼šåº•å±‚"ã€"å·¦ç¿¼æ€æƒ³"ã€"å¿ƒåº•"ï¼‰â†’ åˆ¤ä¸º none
   - å•ä¸€ç‰©ä½“å±…ä¸­ï¼ˆå¦‚"ä¸­é—´æ˜¯ä¸€æœµèŠ±"ï¼‰â†’ åˆ¤ä¸º strongï¼ˆæœ‰æ•ˆï¼‰
   - æ— ç©ºé—´å…³ç³»çš„å¹¶åˆ—æè¿°ï¼ˆå¦‚"æœ‰çŒ«å’Œç‹—"ï¼‰â†’ åˆ¤ä¸º none
4. **åˆ†ç±»è§„åˆ™**ï¼š
   - æ˜ç¡®æ–¹ä½è¯ï¼ˆå·¦ã€å³ã€ä¸Šã€ä¸‹ã€ä¸­é—´ã€é¡¶éƒ¨ã€åº•éƒ¨ã€å·¦ä¸Šã€å³ä¸‹ç­‰ï¼‰â†’ strong
   - æ¨¡ç³Šæ–¹ä½è¯ï¼ˆæ—è¾¹ã€å‘¨å›´ã€å››å‘¨ã€ç¯ç»•ã€èƒŒæ™¯ã€å‰æ™¯ç­‰ï¼‰â†’ weak
   - æ— ä½ç½®ä¿¡æ¯æˆ–ä¸ç¬¦åˆä¸Šè¿°è¦æ±‚ â†’ none

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
{{"type": "strong/weak/none", "reason": "ç®€çŸ­åˆ¤æ–­ç†ç”±ï¼ˆ1-2å¥è¯ï¼‰"}}"""
    
    # åˆ†æ‰¹å¤„ç†
    for batch_start in range(0, total, batch_size):
        batch_end = min(batch_start + batch_size, total)
        batch_captions = captions[batch_start:batch_end]
        
        # ä¸ºæ¯ä¸ªcaptionæ„å»ºå®Œæ•´çš„prompt
        batch_prompts = []
        for caption in batch_captions:
            if not isinstance(caption, str) or len(caption.strip()) < 3:
                batch_prompts.append(None)
                continue
            full_prompt = f"""{prompt_template}

æè¿°ï¼š{caption}"""
            batch_prompts.append(full_prompt)
        
        # è¿‡æ»¤æ‰None
        valid_indices = [i for i, p in enumerate(batch_prompts) if p is not None]
        if not valid_indices:
            # å¦‚æœæ•´ä¸ªæ‰¹æ¬¡éƒ½æ— æ•ˆï¼Œè¿”å›Noneç»“æœ
            results.extend([(None, None)] * len(batch_captions))
            continue
        
        valid_prompts = [batch_prompts[i] for i in valid_indices]
        
        try:
            # æ„å»ºæ‰¹å¤„ç†æ¶ˆæ¯
            messages_list = [[{"role": "user", "content": prompt}] for prompt in valid_prompts]
            texts = [tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True) 
                     for msgs in messages_list]
            
            # æ‰¹å¤„ç†tokenize
            inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=128,
                    do_sample=False,
                    temperature=None,
                    top_p=None,
                    top_k=None,
                )
            
            # è§£ç è¾“å‡º
            generated_ids = [
                out[len(inp):] for inp, out in zip(inputs['input_ids'], outputs)
            ]
            responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
            
            # è§£ææ¯ä¸ªå“åº”
            batch_results = [None] * len(batch_captions)
            for idx, (valid_idx, response) in enumerate(zip(valid_indices, responses)):
                response = response.strip()
                
                # å°è¯•è§£æJSON
                json_match = re.search(r'\{[^}]+\}', response, re.DOTALL)
                if json_match:
                    try:
                        result = json.loads(json_match.group())
                        result_type = result.get('type', '').lower()
                        reason = result.get('reason', '')
                        
                        if result_type in ['strong', 'weak', 'none']:
                            batch_results[valid_idx] = (result_type, reason)
                            continue
                    except:
                        pass
                
                # å›é€€åˆ°ç®€å•åŒ¹é…
                response_lower = response.lower()
                if "strong" in response_lower:
                    batch_results[valid_idx] = ("strong", "åŒ…å«æ˜ç¡®æ–¹ä½è¯")
                elif "weak" in response_lower:
                    batch_results[valid_idx] = ("weak", "åŒ…å«æ¨¡ç³Šæ–¹ä½è¯")
                else:
                    batch_results[valid_idx] = ("none", "æ— ç©ºé—´å…³ç³»")
            
            # å¤„ç†æ— æ•ˆçš„captionï¼ˆè¿”å›Noneï¼‰
            for i in range(len(batch_captions)):
                if batch_results[i] is None:
                    batch_results[i] = (None, None)
            
            results.extend(batch_results)
            
            # æ¸…ç†æ˜¾å­˜
            del inputs, outputs, generated_ids, responses
            torch.cuda.empty_cache()
            
        except Exception as e:
            # å¦‚æœæ‰¹å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯åŒ¹é…
            for caption in batch_captions:
                result = fallback_keyword_check(caption)
                if result:
                    results.append((result, "æ‰¹å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…"))
                else:
                    results.append(("none", "æ‰¹å¤„ç†å¤±è´¥ï¼Œæ— åŒ¹é…å…³é”®è¯"))
        
        # æ˜¾ç¤ºè¿›åº¦
        print(f"\r  ğŸ¤– Qwen å¤„ç†è¿›åº¦: {batch_end}/{total} ({100*batch_end/total:.1f}%)", end="", flush=True)
    
    print()  # æ¢è¡Œ
    return results

def process_batch(captions: List[str], model, tokenizer, device: str = "cuda", batch_size: int = 32) -> List[tuple]:
    """æ‰¹é‡å¤„ç†ï¼Œè¿”å› (type, reason) å…ƒç»„åˆ—è¡¨ï¼ˆä½¿ç”¨çœŸæ­£çš„æ‰¹å¤„ç†ï¼‰"""
    return process_batch_truly(captions, model, tokenizer, device, batch_size)

def quick_keyword_filter(caption: str) -> Optional[str]:
    """
    å¿«é€Ÿå…³é”®è¯é¢„ç­›é€‰
    è¿”å›: "candidate" (å€™é€‰), None (ä¸åŒ…å«æ–¹ä½è¯)
    """
    if not isinstance(caption, str):
        return None
    
    caption_lower = caption.lower()
    
    has_strong = any(k in caption_lower for k in STRONG_KEYWORDS)
    has_weak = any(k in caption_lower for k in WEAK_KEYWORDS)
    
    if has_strong or has_weak:
        return "candidate"
    return None

def load_models_multi_gpu(model_name: str, num_gpus: int = 3):
    """åœ¨å¤šGPUä¸ŠåŠ è½½æ¨¡å‹"""
    models = []
    tokenizers = []
    devices = []
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„ï¼ˆæ”¯æŒå¤§å°å†™ä¸æ•æ„ŸåŒ¹é…ï¼‰
    is_local = False
    if os.path.exists(model_name) and os.path.isdir(model_name):
        is_local = True
    else:
        # å°è¯•å¤§å°å†™ä¸æ•æ„ŸåŒ¹é…ï¼ˆå¦‚æœè·¯å¾„æ˜¯ç»å¯¹è·¯å¾„ä¸”åŒ…å« model ç›®å½•ï¼‰
        if os.path.isabs(model_name) and 'model' in model_name.lower():
            parent_dir = os.path.dirname(model_name)
            model_basename = os.path.basename(model_name)
            if os.path.exists(parent_dir):
                # åœ¨çˆ¶ç›®å½•ä¸­æŸ¥æ‰¾å¤§å°å†™ä¸åŒ¹é…çš„ç›®å½•
                for item in os.listdir(parent_dir):
                    if item.lower() == model_basename.lower() and os.path.isdir(os.path.join(parent_dir, item)):
                        model_name = os.path.join(parent_dir, item)
                        is_local = True
                        print(f"  ğŸ” æ£€æµ‹åˆ°å¤§å°å†™ä¸åŒ¹é…ï¼Œè‡ªåŠ¨ä¿®æ­£è·¯å¾„: {model_name}")
                        break
    
    print(f"ğŸš€ åœ¨ {num_gpus} å¼  GPU ä¸ŠåŠ è½½æ¨¡å‹ {model_name} ...")
    if is_local:
        print(f"  ğŸ“¦ æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨ local_files_only=True")
    else:
        print(f"  ğŸ“¦ ä» HuggingFace åŠ è½½æ¨¡å‹")
    
    for i in range(num_gpus):
        device = f"cuda:{i}"
        print(f"  ğŸ“¦ åŠ è½½åˆ° {device} ...")
        
        if is_local:
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                local_files_only=True,
                padding_side='left'  # decoder-onlyæ¨¡å‹ä½¿ç”¨left padding
            )
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.bfloat16,
                device_map=device,
                trust_remote_code=True,
                local_files_only=True
            )
        else:
            # ä¸´æ—¶ç¦ç”¨ä»£ç†ï¼ˆå¦‚æœä»£ç†æœåŠ¡æœªè¿è¡Œï¼‰
            old_proxy = os.environ.get('HTTP_PROXY')
            old_https_proxy = os.environ.get('HTTPS_PROXY')
            old_http_proxy = os.environ.get('http_proxy')
            old_https_proxy_lower = os.environ.get('https_proxy')
            try:
                if old_proxy:
                    os.environ.pop('HTTP_PROXY', None)
                if old_https_proxy:
                    os.environ.pop('HTTPS_PROXY', None)
                if old_http_proxy:
                    os.environ.pop('http_proxy', None)
                if old_https_proxy_lower:
                    os.environ.pop('https_proxy', None)
                
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    trust_remote_code=True,
                    padding_side='left'  # decoder-onlyæ¨¡å‹ä½¿ç”¨left padding
                )
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=device,
                    trust_remote_code=True
                )
            finally:
                if old_proxy:
                    os.environ['HTTP_PROXY'] = old_proxy
                if old_https_proxy:
                    os.environ['HTTPS_PROXY'] = old_https_proxy
                if old_http_proxy:
                    os.environ['http_proxy'] = old_http_proxy
                if old_https_proxy_lower:
                    os.environ['https_proxy'] = old_https_proxy_lower
        
        model.eval()
        models.append(model)
        tokenizers.append(tokenizer)
        devices.append(device)
        print(f"  âœ… {device} åŠ è½½å®Œæˆ")
    
    return models, tokenizers, devices

def main(args):
    # å¤šGPUæ”¯æŒ
    if args.num_gpus > 1:
        models, tokenizers, devices = load_models_multi_gpu(args.model, args.num_gpus)
        current_gpu = 0  # è½®è¯¢ä½¿ç”¨GPU
    else:
        model, tokenizer = load_model(args.model, args.device)
        models = [model]
        tokenizers = [tokenizer]
        devices = [args.device]
    
    os.makedirs(os.path.dirname(args.output_csv) or ".", exist_ok=True)
    
    # ä½¿ç”¨è‡ªç„¶æ’åºï¼ˆæ•°å­—æ’åºï¼‰è€Œä¸æ˜¯å­—ç¬¦ä¸²æ’åº
    import re
    def natural_sort_key(s):
        """è‡ªç„¶æ’åºï¼šå°†æ•°å­—éƒ¨åˆ†æŒ‰æ•°å€¼å¤§å°æ’åºï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²æ’åº"""
        return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\d+)', s)]
    
    csv_files = sorted([str(p) for p in Path(args.input_dir).rglob("*.csv")], key=natural_sort_key)
    print(f"ğŸ“¦ æ‰¾åˆ° {len(csv_files)} ä¸ª CSV æ–‡ä»¶")
    
    # ä»æŒ‡å®šæ–‡ä»¶å¼€å§‹å¤„ç†
    if args.start_from:
        start_file = args.start_from
        # æ”¯æŒæ–‡ä»¶åæˆ–å®Œæ•´è·¯å¾„
        if not os.path.isabs(start_file):
            # å¦‚æœæ˜¯æ–‡ä»¶åï¼ŒæŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
            start_idx = None
            for i, csv_file in enumerate(csv_files):
                if os.path.basename(csv_file) == start_file or start_file in csv_file:
                    start_idx = i
                    break
            if start_idx is not None:
                csv_files = csv_files[start_idx:]
                print(f"ğŸ“ ä» {os.path.basename(csv_files[0])} å¼€å§‹å¤„ç†ï¼ˆè·³è¿‡å‰ {start_idx} ä¸ªæ–‡ä»¶ï¼‰")
            else:
                print(f"âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ°æ–‡ä»¶ '{start_file}'ï¼Œå°†ä»ç¬¬ä¸€ä¸ªæ–‡ä»¶å¼€å§‹å¤„ç†")
        else:
            # å¦‚æœæ˜¯å®Œæ•´è·¯å¾„ï¼Œç›´æ¥æŸ¥æ‰¾
            if start_file in csv_files:
                start_idx = csv_files.index(start_file)
                csv_files = csv_files[start_idx:]
                print(f"ğŸ“ ä» {os.path.basename(csv_files[0])} å¼€å§‹å¤„ç†ï¼ˆè·³è¿‡å‰ {start_idx} ä¸ªæ–‡ä»¶ï¼‰")
            else:
                print(f"âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ°æ–‡ä»¶ '{start_file}'ï¼Œå°†ä»ç¬¬ä¸€ä¸ªæ–‡ä»¶å¼€å§‹å¤„ç†")
    
    if args.limit_csvs:
        csv_files = csv_files[:args.limit_csvs]
    
    filtered_data = []
    total_processed = 0
    keyword_candidates = 0
    strong_count = 0
    weak_count = 0
    negative_count = 0  # è´Ÿæ ·æœ¬è®¡æ•°
    
    processed_urls = set()
    initial_filtered_count = 0  # è®°å½•å·²æœ‰è¾“å‡ºæ–‡ä»¶ä¸­çš„è®°å½•æ•°
    if os.path.exists(args.output_csv):
        try:
            existing_df = pd.read_csv(args.output_csv)
            processed_urls = set(existing_df['url'].astype(str))
            initial_filtered_count = len(existing_df)
            print(f"ğŸ“‚ æ£€æµ‹åˆ°å·²æœ‰è¾“å‡ºï¼Œå·²å¤„ç† {len(processed_urls)} æ¡ï¼ˆå·²æœ‰ {initial_filtered_count} æ¡è®°å½•ï¼‰")
        except Exception:
            pass
    
    for csv_file in csv_files:
        # ä¿®å¤ï¼šæ£€æŸ¥æ€»å¤„ç†æ•°ï¼ˆåŒ…æ‹¬å·²æœ‰è®°å½•ï¼‰ï¼Œè€Œä¸æ˜¯ç¼“å†²åŒºå¤§å°
        if args.target_samples:
            current_total = initial_filtered_count + total_processed
            if current_total >= args.target_samples:
                print(f"  âœ… å·²è¾¾åˆ°ç›®æ ‡æ ·æœ¬æ•° {args.target_samples}ï¼ˆå½“å‰: {current_total}ï¼‰ï¼Œåœæ­¢å¤„ç†")
                break
        
        print(f"\nğŸ” å¤„ç† {os.path.basename(csv_file)}...")
        
        try:
            chunk_iter = pd.read_csv(csv_file, on_bad_lines='skip', chunksize=2000)
            csv_processed_count = 0
            csv_total_count = 0
            csv_skipped_chunks = 0  # è®°å½•è·³è¿‡çš„ chunk æ•°é‡
            csv_total_chunks = 0  # è®°å½•æ€» chunk æ•°é‡
            
            for chunk in chunk_iter:
                csv_total_chunks += 1
                # åˆ—åé€‚é…
                if 'url' not in chunk.columns and len(chunk.columns) >= 2:
                    chunk.rename(columns={chunk.columns[0]: 'url', chunk.columns[1]: 'caption'}, inplace=True)
                if 'text' in chunk.columns:
                    chunk.rename(columns={'text': 'caption'}, inplace=True)
                
                if 'caption' not in chunk.columns or 'url' not in chunk.columns:
                    continue
                
                csv_total_count += len(chunk)
                
                # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœæ‰€æœ‰ URL éƒ½å·²å¤„ç†ï¼Œç›´æ¥è·³è¿‡ï¼ˆé¿å…ä¸å¿…è¦çš„å¤„ç†ï¼‰
                chunk_urls = set(chunk['url'].astype(str))
                already_processed = chunk_urls & processed_urls
                if len(already_processed) == len(chunk_urls):
                    # æ•´ä¸ª chunk éƒ½å·²å¤„ç†ï¼Œç›´æ¥è·³è¿‡
                    csv_processed_count += len(chunk)
                    csv_skipped_chunks += 1
                    continue
                
                # è¿‡æ»¤å·²å¤„ç†
                chunk = chunk[~chunk['url'].astype(str).isin(processed_urls)]
                if len(chunk) == 0:
                    csv_processed_count += len(chunk_urls) - len(chunk)
                    csv_skipped_chunks += 1
                    continue
                
                # ä¼˜åŒ–ï¼šå¦‚æœå·²è·³è¿‡æ¡æ•°è¶…è¿‡40ï¼Œè·³è¿‡æ•´ä¸ªchunkï¼ˆé¿å…å¤„ç†å¤§éƒ¨åˆ†å·²å¤„ç†çš„chunkï¼‰
                if len(already_processed) > 40:
                    csv_processed_count += len(chunk_urls)
                    csv_skipped_chunks += 1
                    print(f"  â­ï¸  è·³è¿‡æ•´ä¸ªchunkï¼šå·²è·³è¿‡ {len(already_processed)} æ¡ï¼ˆè¶…è¿‡40æ¡é˜ˆå€¼ï¼‰")
                    continue
                
                # ç¬¬ä¸€æ­¥ï¼šå…³é”®è¯å¿«é€Ÿé¢„ç­›é€‰
                candidates = []
                for idx, row in chunk.iterrows():
                    if quick_keyword_filter(row['caption']):
                        candidates.append((idx, row))
                
                keyword_candidates += len(candidates)
                print(f"  ğŸ“‹ å…³é”®è¯é¢„ç­›é€‰: {len(candidates)}/{len(chunk)} å€™é€‰ (å·²è·³è¿‡ {len(already_processed)} æ¡)")
                
                if len(candidates) == 0:
                    continue
                
                # ç¬¬äºŒæ­¥ï¼šQwen ç²¾ç¡®ç­›é€‰
                print(f"  ğŸ¤– å¼€å§‹ Qwen ç²¾ç¡®ç­›é€‰ {len(candidates)} ä¸ªå€™é€‰...")
                candidate_captions = [row['caption'] for _, row in candidates]
                
                # é€‰æ‹©GPUï¼ˆå¤šGPUæ—¶è½®è¯¢ï¼‰
                if args.num_gpus > 1:
                    gpu_idx = current_gpu % args.num_gpus
                    current_gpu += 1
                    model = models[gpu_idx]
                    tokenizer = tokenizers[gpu_idx]
                    device = devices[gpu_idx]
                else:
                    model = models[0]
                    tokenizer = tokenizers[0]
                    device = devices[0]
                
                results = process_batch(
                    candidate_captions,
                    model,
                    tokenizer,
                    device,
                    batch_size=args.batch_size
                )
                
                # æ”¶é›†æœ‰æ•ˆç»“æœï¼ˆåŒ…æ‹¬è´Ÿæ ·æœ¬æŒ–æ˜ï¼‰
                negative_candidates = []  # ç”¨äºè´Ÿæ ·æœ¬æŒ–æ˜
                
                for (idx, row), (result_type, reason) in zip(candidates, results):
                    if result_type in ["strong", "weak"]:
                        # æ­£æ ·æœ¬ï¼šåŒ…å«ç©ºé—´å…³ç³»
                        filtered_data.append({
                            'url': row['url'],
                            'caption': row['caption'],
                            'spatial_type': result_type,
                            'reason': reason
                        })
                        
                        if result_type == "strong":
                            strong_count += 1
                        else:
                            weak_count += 1
                        
                        processed_urls.add(str(row['url']))
                        total_processed += 1
                    elif result_type == "none":
                        # è´Ÿæ ·æœ¬å€™é€‰ï¼šå…³é”®è¯å¬å›ä½† LLM åˆ¤å®šä¸º noneï¼ˆä¼ªç©ºé—´å…³ç³»ï¼‰
                        negative_candidates.append((idx, row, reason))
                
                # è´Ÿæ ·æœ¬æŒ–æ˜ï¼šä¿ç•™ 10% çš„ none æ ·æœ¬ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
                if negative_candidates and args.negative_ratio > 0:
                    import random
                    num_negative = max(1, int(len(negative_candidates) * args.negative_ratio))
                    selected_negative = random.sample(negative_candidates, min(num_negative, len(negative_candidates)))
                    
                    for idx, row, reason in selected_negative:
                        filtered_data.append({
                            'url': row['url'],
                            'caption': row['caption'],
                            'spatial_type': 'negative',  # æ ‡è®°ä¸ºè´Ÿæ ·æœ¬
                            'reason': reason
                        })
                        negative_count += 1
                        processed_urls.add(str(row['url']))
                        total_processed += 1
                
                # æ›´æ–° CSV å¤„ç†ç»Ÿè®¡
                csv_processed_count += len(results)
                
                # å®šæœŸä¿å­˜ï¼ˆæ¯ç´¯ç§¯ 50 æ¡å°±ä¿å­˜ï¼Œæ›´é¢‘ç¹çš„ä¿å­˜ï¼‰
                if len(filtered_data) >= 50:  # æ”¹ä¸º >= 50ï¼Œæ›´é¢‘ç¹ä¿å­˜
                    df_temp = pd.DataFrame(filtered_data)
                    if os.path.exists(args.output_csv):
                        df_temp.to_csv(args.output_csv, mode='a', header=False, index=False)
                    else:
                        df_temp.to_csv(args.output_csv, index=False)
                    filtered_data = []  # æ¸…ç©ºç¼“å†²åŒº
                    current_total = initial_filtered_count + total_processed
                    print(f"  ğŸ’¾ å·²ä¿å­˜ {current_total} æ¡ (æœ¬æ¬¡æ–°å¢: {total_processed}, Strong: {strong_count}, Weak: {weak_count}, Negative: {negative_count})", flush=True)
                
                # ä¿®å¤ï¼šæ£€æŸ¥æ€»å¤„ç†æ•°ï¼ˆåŒ…æ‹¬å·²æœ‰è®°å½•ï¼‰
                if args.target_samples:
                    current_total = initial_filtered_count + total_processed
                    if current_total >= args.target_samples:
                        print(f"  âœ… å·²è¾¾åˆ°ç›®æ ‡æ ·æœ¬æ•° {args.target_samples}ï¼ˆå½“å‰: {current_total}ï¼‰ï¼Œåœæ­¢å¤„ç†")
                        break
            
            # æ˜¾ç¤ºè¯¥ CSV æ–‡ä»¶çš„å¤„ç†ç»Ÿè®¡
            if csv_total_count > 0:
                skip_ratio = csv_processed_count / csv_total_count * 100 if csv_total_count > 0 else 0
                if skip_ratio > 50:
                    print(f"  â­ï¸  {os.path.basename(csv_file)}: å·²è·³è¿‡ {csv_processed_count}/{csv_total_count} æ¡ ({skip_ratio:.1f}%)")
            elif csv_total_chunks > 0 and csv_skipped_chunks == csv_total_chunks:
                # å¦‚æœæ‰€æœ‰ chunk éƒ½è¢«è·³è¿‡ï¼Œæ˜¾ç¤ºæç¤ºä¿¡æ¯
                print(f"  â­ï¸  {os.path.basename(csv_file)}: æ‰€æœ‰ {csv_total_chunks} ä¸ª chunk éƒ½å·²è·³è¿‡ï¼ˆå·²å¤„ç†æˆ–è¶…è¿‡é˜ˆå€¼ï¼‰")
            
            # ä¿®å¤ï¼šæ£€æŸ¥æ€»å¤„ç†æ•°ï¼ˆåŒ…æ‹¬å·²æœ‰è®°å½•ï¼‰
            if args.target_samples:
                current_total = initial_filtered_count + total_processed
                if current_total >= args.target_samples:
                    print(f"  âœ… å·²è¾¾åˆ°ç›®æ ‡æ ·æœ¬æ•° {args.target_samples}ï¼ˆå½“å‰: {current_total}ï¼‰ï¼Œåœæ­¢å¤„ç†")
                    break
                
        except Exception as e:
            print(f"âš ï¸ å¤„ç† {csv_file} æ—¶å‡ºé”™: {e}")
            continue
    
    # ä¿å­˜å‰©ä½™æ•°æ®
    if filtered_data:
        df_temp = pd.DataFrame(filtered_data)
        if os.path.exists(args.output_csv):
            df_temp.to_csv(args.output_csv, mode='a', header=False, index=False)
        else:
            df_temp.to_csv(args.output_csv, index=False)
    
    # æœ€ç»ˆç»Ÿè®¡å’Œå¹³è¡¡
    if os.path.exists(args.output_csv):
        final_df = pd.read_csv(args.output_csv)
        final_strong = len(final_df[final_df['spatial_type'] == 'strong'])
        final_weak = len(final_df[final_df['spatial_type'] == 'weak'])
        final_negative = len(final_df[final_df['spatial_type'] == 'negative'])
        
        print(f"\nâœ… ç­›é€‰å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡:")
        print(f"   å…³é”®è¯å€™é€‰: {keyword_candidates}")
        print(f"   æœ€ç»ˆä¿ç•™: {len(final_df)} æ¡")
        print(f"   Strong (å¼ºæ–¹ä½): {final_strong} ({final_strong/len(final_df):.1%})")
        print(f"   Weak (å¼±æ–¹ä½): {final_weak} ({final_weak/len(final_df):.1%})")
        if final_negative > 0:
            print(f"   Negative (è´Ÿæ ·æœ¬): {final_negative} ({final_negative/len(final_df):.1%})")
        print(f"   ç­›é€‰ç‡: {len(final_df)/keyword_candidates:.1%}" if keyword_candidates > 0 else "")
        
        # å¦‚æœéœ€è¦è°ƒæ•´æ¯”ä¾‹ï¼Œè¿›è¡Œå¹³è¡¡é‡‡æ ·
        if args.strong_ratio and final_strong > 0 and final_weak > 0:
            positive_df = final_df[final_df['spatial_type'].isin(['strong', 'weak'])]
            negative_df = final_df[final_df['spatial_type'] == 'negative']
            
            # è®¡ç®—ç›®æ ‡æ•°é‡ï¼ˆåŸºäºæ­£æ ·æœ¬æ€»æ•°ï¼‰
            total_positive = len(positive_df)
            target_strong = int(total_positive * args.strong_ratio)
            target_weak = total_positive - target_strong
            
            # ä¸‹é‡‡æ ·
            strong_df = positive_df[positive_df['spatial_type'] == 'strong']
            weak_df = positive_df[positive_df['spatial_type'] == 'weak']
            
            # æ£€æŸ¥ strong æ ·æœ¬æ˜¯å¦è¶³å¤Ÿ
            if len(strong_df) < target_strong:
                print(f"\nâš ï¸  è­¦å‘Šï¼šStrong æ ·æœ¬ä¸è¶³ï¼")
                print(f"   éœ€è¦: {target_strong} ä¸ªï¼Œå®é™…åªæœ‰: {len(strong_df)} ä¸ª")
                print(f"   å»ºè®®ï¼šç­›é€‰æ›´å¤šåŸå§‹æ•°æ®ä»¥è·å¾—è¶³å¤Ÿçš„ strong æ ·æœ¬")
                print(f"   å½“å‰å°†ä½¿ç”¨æ‰€æœ‰ {len(strong_df)} ä¸ª strong æ ·æœ¬")
                # è°ƒæ•´ weak æ ·æœ¬æ•°é‡ä»¥åŒ¹é…å®é™… strong æ•°é‡
                actual_strong_ratio = len(strong_df) / total_positive if total_positive > 0 else 0
                target_weak = total_positive - len(strong_df)
            else:
                strong_df = strong_df.sample(n=target_strong, random_state=42)
            
            if len(weak_df) > target_weak:
                weak_df = weak_df.sample(n=target_weak, random_state=42)
            
            # å¤„ç† negative æ ·æœ¬ï¼ˆå¦‚æœè®¾ç½®äº† negative_ratioï¼‰
            if args.negative_ratio > 0 and len(negative_df) > 0:
                total_balanced = len(strong_df) + len(weak_df)
                target_negative = int(total_balanced * args.negative_ratio / (1 - args.negative_ratio))
                if len(negative_df) > target_negative:
                    negative_df = negative_df.sample(n=target_negative, random_state=42)
            
            # åˆå¹¶å¹¶æ‰“ä¹±
            balanced_df = pd.concat([strong_df, weak_df, negative_df]).sample(frac=1, random_state=42).reset_index(drop=True)
            balanced_path = args.output_csv.replace('.csv', '_balanced.csv')
            balanced_df.to_csv(balanced_path, index=False)
            
            print(f"\nğŸ“Š å·²ç”Ÿæˆå¹³è¡¡æ•°æ®é›†:")
            print(f"   Strong: {len(strong_df)} ({len(strong_df)/len(balanced_df):.1%})")
            print(f"   Weak: {len(weak_df)} ({len(weak_df)/len(balanced_df):.1%})")
            if len(negative_df) > 0:
                print(f"   Negative: {len(negative_df)} ({len(negative_df)/len(balanced_df):.1%})")
            print(f"ğŸ’¾ ä¿å­˜åˆ°: {balanced_path}")
        
        print(f"ğŸ’¾ åŸå§‹æ•°æ®ä¿å­˜åˆ°: {args.output_csv}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="æ··åˆç­›é€‰æ–¹æ¡ˆï¼šå…³é”®è¯é¢„ç­›é€‰ + Qwen ç²¾ç¡®ç­›é€‰ï¼ˆè®ºæ–‡çº§ä¼˜åŒ–ç‰ˆï¼‰"
    )
    parser.add_argument("--input_dir", type=str, required=True,
                       help="åŸå§‹ Wukong CSV æ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("--output_csv", type=str, required=True,
                       help="è¾“å‡ºç­›é€‰åçš„ CSV è·¯å¾„")
    parser.add_argument("--model", type=str, default="Qwen/Qwen2.5-7B-Instruct",
                       help="Qwen æ¨¡å‹è·¯å¾„æˆ– HuggingFace æ¨¡å‹å")
    parser.add_argument("--device", type=str, default="cuda",
                       help="è®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--target_samples", type=int, default=20000,
                       help="ç›®æ ‡æ ·æœ¬æ•°ï¼ˆå»ºè®® 20000-50000ï¼‰")
    parser.add_argument("--limit_csvs", type=int, default=None,
                       help="é™åˆ¶å¤„ç†çš„ CSV æ–‡ä»¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰")
    parser.add_argument("--start_from", type=str, default=None,
                       help="ä»æŒ‡å®šçš„ CSV æ–‡ä»¶å¼€å§‹å¤„ç†ï¼ˆå¯ä»¥æ˜¯æ–‡ä»¶åå¦‚ 'wukong_100m_1.csv' æˆ–å®Œæ•´è·¯å¾„ï¼‰")
    parser.add_argument("--batch_size", type=int, default=32,
                       help="æ‰¹å¤„ç†å¤§å°ï¼ˆ4090 24GBæ˜¾å­˜å»ºè®®32-64ï¼Œå¤šGPUæ—¶å¯æ›´å¤§ï¼‰")
    parser.add_argument("--num-gpus", type=int, default=1,
                       help="ä½¿ç”¨çš„GPUæ•°é‡ï¼ˆé»˜è®¤1ï¼Œæ”¯æŒå¤šGPUå¹¶è¡Œï¼Œå»ºè®®3å¼ 4090ï¼‰")
    parser.add_argument("--strong_ratio", type=float, default=0.8,
                       help="Strong æ ·æœ¬çš„ç›®æ ‡æ¯”ä¾‹ï¼ˆ0.8 è¡¨ç¤º 80%%ï¼‰")
    parser.add_argument("--negative_ratio", type=float, default=0.1,
                       help="è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼Œ0.1 è¡¨ç¤º 10%%ï¼‰")
    args = parser.parse_args()
    main(args)

