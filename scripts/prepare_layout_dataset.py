#!/usr/bin/env python3
"""
æ„å»ºå¸¦ç©ºé—´æ ‡æ³¨çš„ä¸­æ–‡æ•°æ®é›†

ä» WuKong æ•°æ®é›†ä¸­ç­›é€‰åŒ…å«ä½ç½®æè¿°çš„æ•°æ®ï¼Œå¹¶ä½¿ç”¨ GPT-4V æˆ– Qwen-VL è‡ªåŠ¨æ ‡æ³¨ bounding boxã€‚

æ•°æ®æ ¼å¼ï¼š
{
    "caption": "å·¦è¾¹æ”¾ä¸€ä¸ªé’èŠ±ç“·ç“¶ï¼Œå³è¾¹æ”¾ä¸€ç›˜é¥ºå­",
    "image_path": "xxx.jpg",
    "objects": [
        {"name": "é’èŠ±ç“·ç“¶", "bbox": [0.1, 0.3, 0.4, 0.7]},
        {"name": "é¥ºå­", "bbox": [0.6, 0.3, 0.9, 0.7]}
    ]
}

Usage:
    python scripts/prepare_layout_dataset.py \
        --input-tsv data/wukong_train.tsv \
        --output-jsonl data/layout_dataset.jsonl \
        --num-samples 10000 \
        --use-qwen-vl
"""

import argparse
import json
import os
import re
from typing import Dict, List, Optional
from tqdm import tqdm
import pandas as pd
from PIL import Image


# ä½ç½®å…³é”®è¯åˆ—è¡¨
POSITION_KEYWORDS = [
    "å·¦è¾¹", "å·¦ä¾§", "å·¦æ–¹", "å·¦é¢",
    "å³è¾¹", "å³ä¾§", "å³æ–¹", "å³é¢",
    "ä¸Šæ–¹", "ä¸Šè¾¹", "ä¸Šé¢", "ä¸Šä¾§",
    "ä¸‹æ–¹", "ä¸‹è¾¹", "ä¸‹é¢", "ä¸‹ä¾§",
    "ä¸­é—´", "ä¸­å¤®", "ä¸­å¿ƒ", "ä¸­éƒ¨",
    "å·¦ä¸Šè§’", "å·¦ä¸‹è§’", "å³ä¸Šè§’", "å³ä¸‹è§’",
    "å·¦ä¸Š", "å·¦ä¸‹", "å³ä¸Š", "å³ä¸‹"
]


def has_position_description(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä½ç½®æè¿°"""
    return any(keyword in text for keyword in POSITION_KEYWORDS)


def filter_captions_with_position(input_tsv: str, output_tsv: str, num_samples: int = 10000):
    """
    ä»è¾“å…¥ TSV æ–‡ä»¶ä¸­ç­›é€‰åŒ…å«ä½ç½®æè¿°çš„ caption
    
    Args:
        input_tsv: è¾“å…¥ TSV æ–‡ä»¶è·¯å¾„
        output_tsv: è¾“å‡º TSV æ–‡ä»¶è·¯å¾„
        num_samples: éœ€è¦ç­›é€‰çš„æ ·æœ¬æ•°é‡
    """
    print(f"ğŸ“– è¯»å–æ•°æ®: {input_tsv}")
    df = pd.read_csv(input_tsv, sep='\t', header=None, names=['caption', 'image'])
    
    print(f"ğŸ” ç­›é€‰åŒ…å«ä½ç½®æè¿°çš„æ ·æœ¬...")
    filtered_df = df[df['caption'].apply(has_position_description)]
    
    print(f"âœ“ æ‰¾åˆ° {len(filtered_df)} æ¡åŒ…å«ä½ç½®æè¿°çš„æ ·æœ¬ï¼ˆå…± {len(df)} æ¡ï¼‰")
    
    # éšæœºé‡‡æ ·
    if len(filtered_df) > num_samples:
        filtered_df = filtered_df.sample(n=num_samples, random_state=42)
        print(f"âœ“ éšæœºé‡‡æ · {num_samples} æ¡")
    
    # ä¿å­˜
    filtered_df.to_csv(output_tsv, sep='\t', header=False, index=False)
    print(f"âœ“ ä¿å­˜åˆ°: {output_tsv}")
    
    return filtered_df


def annotate_with_qwen_vl(image_path: str, caption: str, 
                          qwen_model=None, processor=None) -> List[Dict]:
    """
    ä½¿ç”¨ Qwen-VL è‡ªåŠ¨æ ‡æ³¨ bounding box
    
    Args:
        image_path: å›¾åƒè·¯å¾„
        caption: æ–‡æœ¬æè¿°
        qwen_model: Qwen-VL æ¨¡å‹ï¼ˆå¦‚æœä¸º Noneï¼Œä¼šè‡ªåŠ¨åŠ è½½ï¼‰
        processor: Qwen-VL processorï¼ˆå¦‚æœä¸º Noneï¼Œä¼šè‡ªåŠ¨åŠ è½½ï¼‰
    
    Returns:
        objects: [{"name": "...", "bbox": [x1, y1, x2, y2]}]
    """
    try:
        from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
        import torch
        
        if qwen_model is None or processor is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
            processor = AutoProcessor.from_pretrained(
                "Qwen/Qwen-VL", trust_remote_code=True
            )
            qwen_model = Qwen2VLForConditionalGeneration.from_pretrained(
                "Qwen/Qwen-VL",
                torch_dtype=torch.bfloat16,
                device_map=device,
                trust_remote_code=True
            )
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        # æ„å»º grounding prompt
        prompt = f"è¯·æ£€æµ‹å›¾åƒä¸­çš„ä»¥ä¸‹å¯¹è±¡å¹¶æ ‡æ³¨ä½ç½®ï¼š{caption}"
        
        # å¤„ç†è¾“å…¥
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
        
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = processor(
            text=[text],
            images=[image],
            padding=True,
            return_tensors="pt"
        ).to(qwen_model.device)
        
        # ç”Ÿæˆï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦è°ƒç”¨ grounding APIï¼‰
        # æ³¨æ„ï¼šQwen-VL çš„ grounding åŠŸèƒ½å¯èƒ½éœ€è¦ç‰¹æ®Šè°ƒç”¨
        
        # è¿”å›ç©ºç»“æœï¼ˆéœ€è¦æ ¹æ®å®é™… API å®ç°ï¼‰
        return []
        
    except Exception as e:
        print(f"âš ï¸ Qwen-VL æ ‡æ³¨å‡ºé”™: {e}")
        return []


def annotate_with_heuristic(caption: str) -> List[Dict]:
    """
    ä½¿ç”¨å¯å‘å¼è§„åˆ™æ ‡æ³¨ï¼ˆåŸºäºä½ç½®å…³é”®è¯ï¼‰
    
    Args:
        caption: æ–‡æœ¬æè¿°
    
    Returns:
        objects: [{"name": "...", "bbox": [x1, y1, x2, y2]}]
    """
    objects = []
    
    # é¢„å®šä¹‰æ§½ä½
    slots = {
        "left": [0.0, 0.1, 0.4, 0.9],
        "right": [0.6, 0.1, 1.0, 0.9],
        "top": [0.1, 0.0, 0.9, 0.4],
        "bottom": [0.1, 0.6, 0.9, 1.0],
        "center": [0.3, 0.3, 0.7, 0.7],
        "bottom_left": [0.0, 0.6, 0.4, 1.0],
        "bottom_right": [0.6, 0.6, 1.0, 1.0],
    }
    
    # ç®€å•åè¯æå–ï¼ˆä½¿ç”¨ jieba æˆ–ç®€å•è§„åˆ™ï¼‰
    try:
        import jieba.posseg as pseg
        words = pseg.cut(caption)
        nouns = [w for w, flag in words if flag.startswith("n")]
    except:
        # ç®€å•åˆ†è¯
        nouns = re.findall(r'[\u4e00-\u9fa5]{2,}', caption)
    
    # æ ¹æ®ä½ç½®å…³é”®è¯åˆ†é…æ§½ä½
    slot_mapping = {
        "left": ["å·¦è¾¹", "å·¦ä¾§", "å·¦æ–¹", "å·¦é¢"],
        "right": ["å³è¾¹", "å³ä¾§", "å³æ–¹", "å³é¢"],
        "top": ["ä¸Šæ–¹", "ä¸Šè¾¹", "ä¸Šé¢", "ä¸Šä¾§"],
        "bottom": ["ä¸‹æ–¹", "ä¸‹è¾¹", "ä¸‹é¢", "ä¸‹ä¾§"],
        "center": ["ä¸­é—´", "ä¸­å¤®", "ä¸­å¿ƒ", "ä¸­éƒ¨"],
        "bottom_left": ["å·¦ä¸‹è§’", "å·¦ä¸‹"],
        "bottom_right": ["å³ä¸‹è§’", "å³ä¸‹"],
    }
    
    used_slots = set()
    for noun in nouns[:3]:  # æœ€å¤šå¤„ç†3ä¸ªå¯¹è±¡
        # æ‰¾åˆ°å¯¹åº”çš„ä½ç½®å…³é”®è¯
        assigned_slot = None
        for slot_key, keywords in slot_mapping.items():
            if any(kw in caption for kw in keywords) and slot_key not in used_slots:
                assigned_slot = slot_key
                used_slots.add(slot_key)
                break
        
        if assigned_slot is None:
            assigned_slot = "center"  # é»˜è®¤å±…ä¸­
        
        objects.append({
            "name": noun,
            "bbox": slots[assigned_slot]
        })
    
    return objects


def create_layout_dataset(input_tsv: str,
                         output_jsonl: str,
                         image_dir: str,
                         num_samples: int = 10000,
                         use_qwen_vl: bool = False,
                         use_heuristic: bool = True):
    """
    åˆ›å»ºå¸ƒå±€æ•°æ®é›†
    
    Args:
        input_tsv: è¾“å…¥ TSV æ–‡ä»¶
        output_jsonl: è¾“å‡º JSONL æ–‡ä»¶
        image_dir: å›¾åƒç›®å½•
        num_samples: æ ·æœ¬æ•°é‡
        use_qwen_vl: æ˜¯å¦ä½¿ç”¨ Qwen-VL æ ‡æ³¨
        use_heuristic: æ˜¯å¦ä½¿ç”¨å¯å‘å¼è§„åˆ™ï¼ˆå¤‡ç”¨ï¼‰
    """
    # 1. ç­›é€‰åŒ…å«ä½ç½®æè¿°çš„æ ·æœ¬
    filtered_tsv = input_tsv.replace('.tsv', '_filtered.tsv')
    filtered_df = filter_captions_with_position(input_tsv, filtered_tsv, num_samples)
    
    # 2. æ ‡æ³¨ bounding box
    print(f"\nğŸ“ å¼€å§‹æ ‡æ³¨ bounding box...")
    
    qwen_model = None
    processor = None
    if use_qwen_vl:
        try:
            from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
            import torch
            device = "cuda" if torch.cuda.is_available() else "cpu"
            processor = AutoProcessor.from_pretrained(
                "Qwen/Qwen-VL", trust_remote_code=True
            )
            qwen_model = Qwen2VLForConditionalGeneration.from_pretrained(
                "Qwen/Qwen-VL",
                torch_dtype=torch.bfloat16,
                device_map=device,
                trust_remote_code=True
            )
            print("âœ“ Qwen-VL åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ Qwen-VL åŠ è½½å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨å¯å‘å¼è§„åˆ™")
            use_qwen_vl = False
    
    results = []
    for idx, row in tqdm(filtered_df.iterrows(), total=len(filtered_df), desc="æ ‡æ³¨è¿›åº¦"):
        caption = row['caption']
        image_file = row['image']
        image_path = os.path.join(image_dir, image_file)
        
        # æ£€æŸ¥å›¾åƒæ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            continue
        
        # æ ‡æ³¨
        if use_qwen_vl and qwen_model is not None:
            objects = annotate_with_qwen_vl(image_path, caption, qwen_model, processor)
        else:
            objects = annotate_with_heuristic(caption)
        
        if len(objects) == 0:
            continue
        
        # ä¿å­˜ç»“æœ
        result = {
            "caption": caption,
            "image_path": image_path,
            "objects": objects
        }
        results.append(result)
    
    # 3. ä¿å­˜åˆ° JSONL
    print(f"\nğŸ’¾ ä¿å­˜ {len(results)} æ¡æ ‡æ³¨æ•°æ®åˆ°: {output_jsonl}")
    with open(output_jsonl, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    print(f"âœ“ å®Œæˆï¼å…±ç”Ÿæˆ {len(results)} æ¡å¸ƒå±€æ•°æ®")


def main():
    parser = argparse.ArgumentParser(description="æ„å»ºå¸¦ç©ºé—´æ ‡æ³¨çš„ä¸­æ–‡æ•°æ®é›†")
    parser.add_argument('--input-tsv', type=str, required=True,
                       help='è¾“å…¥ TSV æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-jsonl', type=str, required=True,
                       help='è¾“å‡º JSONL æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--image-dir', type=str, required=True,
                       help='å›¾åƒç›®å½•è·¯å¾„')
    parser.add_argument('--num-samples', type=int, default=10000,
                       help='éœ€è¦æ ‡æ³¨çš„æ ·æœ¬æ•°é‡')
    parser.add_argument('--use-qwen-vl', action='store_true',
                       help='ä½¿ç”¨ Qwen-VL è‡ªåŠ¨æ ‡æ³¨ï¼ˆéœ€è¦ GPUï¼‰')
    parser.add_argument('--use-heuristic', action='store_true', default=True,
                       help='ä½¿ç”¨å¯å‘å¼è§„åˆ™æ ‡æ³¨ï¼ˆå¤‡ç”¨ï¼‰')
    
    args = parser.parse_args()
    
    create_layout_dataset(
        args.input_tsv,
        args.output_jsonl,
        args.image_dir,
        args.num_samples,
        args.use_qwen_vl,
        args.use_heuristic
    )


if __name__ == '__main__':
    main()

