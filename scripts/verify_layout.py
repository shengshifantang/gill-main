#!/usr/bin/env python3
"""
Layout Planner éªŒè¯è„šæœ¬ (Fixed Tokenizer Loading)

ä¿®æ­£ç‚¹ï¼šå¼ºåˆ¶ä» adapter_path åŠ è½½è®­ç»ƒå¥½çš„ Tokenizerï¼Œç¡®ä¿ç‰¹æ®Š Token å’Œ Embeddings å¯¹é½ã€‚
"""

import os
import sys
import argparse
import torch
import re
from typing import List, Dict

# ä¿è¯å¯ä»¥ import gill
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸ç›´æ¥ç”¨ LayoutPlanner ç±»ï¼Œè€Œæ˜¯æ‰‹åŠ¨åŠ è½½ä»¥ç¡®ä¿æ§åˆ¶æƒ
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Layout Planner éªŒè¯è„šæœ¬")
    parser.add_argument("--base-model", type=str, default="./model/qwen2.5-7B-Instruct")
    parser.add_argument("--adapter-path", type=str, default="./checkpoints/layout_planner/final")
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--disable-refinement", action="store_true")
    return parser.parse_args()

def parse_layout_output(text: str) -> List[Dict]:
    """è§£æå¸ƒå±€è¾“å‡º (<obj>...</obj><box>...</box>)"""
    objects = []
    # åŒ¹é… <obj>...</obj><box>...</box>
    pattern = r'<obj>(.*?)</obj><box>\[(.*?)\]</box>'
    matches = re.findall(pattern, text)
    
    for name, bbox_str in matches:
        try:
            bbox = [float(x.strip()) for x in bbox_str.split(',')]
            if len(bbox) == 4:
                # å…¼å®¹ 0-1000 æ ¼å¼
                if max(bbox) > 1.5:
                    bbox = [b/1000.0 for b in bbox]
                objects.append({"name": name.strip(), "bbox": bbox})
        except:
            continue
    return objects

def load_model_and_tokenizer(base_path, adapter_path, device):
    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    print(f"  - åŸºåº§: {base_path}")
    print(f"  - Adapter: {adapter_path}")

    # 1. ä¼˜å…ˆä» Adapter è·¯å¾„åŠ è½½ Tokenizer (è¿™æ˜¯å…³é”®ï¼ç¡®ä¿ ID ä¸€è‡´)
    try:
        print("ğŸ“¦ å°è¯•ä» Adapter åŠ è½½è®­ç»ƒå¥½çš„ Tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)
        print("âœ“ æˆåŠŸä» Adapter åŠ è½½ Tokenizer")
    except Exception as e:
        print(f"âš ï¸ Adapter ä¸­æ—  Tokenizerï¼Œå›é€€åˆ°åŸºåº§ (å¯èƒ½å¯¼è‡´ä¹±ç ): {e}")
        tokenizer = AutoTokenizer.from_pretrained(base_path, trust_remote_code=True)
        tokenizer.add_special_tokens({"additional_special_tokens": ["<obj>", "</obj>", "<box>", "</box>"]})

    # 2. åŠ è½½åŸºåº§æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        base_path,
        torch_dtype=torch.float16,
        device_map=device,
        trust_remote_code=True
    )
    
    # 3. è°ƒæ•´ Embedding å¤§å° (å¿…é¡»ä¸ Tokenizer ä¸€è‡´)
    model.resize_token_embeddings(len(tokenizer))

    # 4. åŠ è½½ LoRA
    # æ³¨æ„ï¼šå¦‚æœè®­ç»ƒæ—¶ä¿å­˜äº† embedding layerï¼ŒPeftModel ä¼šè‡ªåŠ¨åŠ è½½å®ƒ
    print("ğŸ“¦ æ­£åœ¨åŠ è½½ LoRA Adapter...")
    model = PeftModel.from_pretrained(model, adapter_path)
    model.eval()
    print("âœ“ LoRA Adapter åŠ è½½æˆåŠŸï¼")
    
    return model, tokenizer

def format_prompt(tokenizer, prompt):
    # ä½¿ç”¨ Chat Template æ„å»ºè¾“å…¥
    messages = [{"role": "user", "content": prompt}]
    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

def main():
    args = parse_args()
    
    # è®¾ç½®è®¾å¤‡
    if "cuda" in args.device and torch.cuda.is_available():
        device = args.device
    else:
        device = "cpu"

    # åŠ è½½
    model, tokenizer = load_model_and_tokenizer(args.base_model, args.adapter_path, device)

    # æµ‹è¯•ç”¨ä¾‹
    test_prompts = [
        "ç”»ä¸€åªåœ¨æ¡Œå­å·¦è¾¹çš„çŒ«",
        "å¤©ç©ºä¸­æœ‰é£é¸Ÿï¼Œä¸‹é¢æ˜¯å¹¿é˜”çš„è‰åœ°",
        "å·¦è¾¹æ˜¯ä¸€ä¸ªçº¢è‰²çš„è‹¹æœï¼Œå³è¾¹æ˜¯ä¸€ä¸ªé»„è‰²çš„é¦™è•‰",
        "ä¸€ä¸ªåœ¨ç©é£ç›˜çš„ç‹—ï¼ŒèƒŒæ™¯æ˜¯æµ·æ»©",
        "ä¸Šæ–¹æ˜¯è“å¤©ï¼Œä¸‹æ–¹æ˜¯è‰åœ°",
        "ä¸­é—´æœ‰ä¸€æœµèŠ±ï¼Œå·¦è¾¹æ˜¯æ ‘ï¼Œå³è¾¹æ˜¯æˆ¿å­",
    ]
    
    print("\n" + "=" * 60)
    print("ğŸ§ å¼€å§‹éªŒè¯æ¨ç†æ•ˆæœ (Fixed Version)")
    print("=" * 60)
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}: {prompt}")
        
        # æ„é€ è¾“å…¥
        input_text = format_prompt(tokenizer, prompt)
        inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
        
        try:
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=256,
                    do_sample=False, # éªŒè¯æ—¶å»ºè®®è´ªå©ªè§£ç ï¼Œçœ‹æœ€ç¨³çš„ç»“æœ
                    temperature=0.1,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            # è§£ç 
            generated_ids = outputs[0][len(inputs.input_ids[0]):]
            output_text = tokenizer.decode(generated_ids, skip_special_tokens=False) # ä¸è·³è¿‡ç‰¹æ®Š tokenï¼Œæˆ‘ä»¬è¦çœ‹ <obj>
            
            # æ¸…ç† Qwen ç‰¹æ®Š token æ–¹ä¾¿æ˜¾ç¤º
            clean_text = output_text.replace("<|im_end|>", "").replace("<|endoftext|>", "").strip()
            print(f"ğŸ¤– è¾“å‡º: {clean_text}")
            
            # è§£æ
            objects = parse_layout_output(clean_text)
            if objects:
                for obj in objects:
                    bbox_str = ",".join([f"{x:.2f}" for x in obj['bbox']])
                    print(f"   - {obj['name']}: [{bbox_str}]")
            else:
                print("   âš ï¸ æœªè§£æåˆ°å¯¹è±¡ (å¯èƒ½æ˜¯æ ¼å¼ä»æœ‰é—®é¢˜)")
                
        except Exception as e:
            print(f"âŒ å‡ºé”™: {e}")

if __name__ == "__main__":
    main()
