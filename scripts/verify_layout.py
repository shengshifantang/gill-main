#!/usr/bin/env python3
"""
Layout Planner éªŒè¯è„šæœ¬ (Fixed Tokenizer Loading)

ä¿®æ­£ç‚¹ï¼šå¼ºåˆ¶ä» adapter_path åŠ è½½è®­ç»ƒå¥½çš„ Tokenizerï¼Œç¡®ä¿ç‰¹æ®Š Token å’Œ Embeddings å¯¹é½ã€‚
"""

import os
import sys
import argparse
import torch
import re
from typing import List, Dict

# ä¿è¯å¯ä»¥ import gill
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸ç›´æ¥ç”¨ LayoutPlanner ç±»ï¼Œè€Œæ˜¯æ‰‹åŠ¨åŠ è½½ä»¥ç¡®ä¿æ§åˆ¶æƒ
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from gill.layout_planner import parse_layout_output, format_layout_input

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Layout Planner éªŒè¯è„šæœ¬")
    parser.add_argument("--base-model", type=str, default="./model/qwen2.5-7B-Instruct")
    parser.add_argument("--adapter-path", type=str, default="./checkpoints/layout_planner/final")
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--disable-refinement", action="store_true")
    parser.add_argument("--input-jsonl", type=str, default=None, help="å¯é€‰ï¼šä» JSONL è¯»å–æµ‹è¯•æ ·æœ¬")
    parser.add_argument("--max-samples", type=int, default=6, help="æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°é‡")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­ï¼ˆç”¨äºé‡‡æ ·ï¼‰")
    parser.add_argument("--output-jsonl", type=str, default=None, help="å¯é€‰ï¼šä¿å­˜é€æ¡éªŒè¯ç»“æœ")
    return parser.parse_args()

def load_model_and_tokenizer(base_path, adapter_path, device):
    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    print(f"  - åŸºåº§: {base_path}")
    print(f"  - Adapter: {adapter_path}")

    # 1. ä¼˜å…ˆä» Adapter è·¯å¾„åŠ è½½ Tokenizer (è¿™æ˜¯å…³é”®ï¼ç¡®ä¿ ID ä¸€è‡´)
    try:
        print("ğŸ“¦ å°è¯•ä» Adapter åŠ è½½è®­ç»ƒå¥½çš„ Tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)
        print("âœ“ æˆåŠŸä» Adapter åŠ è½½ Tokenizer")
    except Exception as e:
        print(f"âš ï¸ Adapter ä¸­æ—  Tokenizerï¼Œå›é€€åˆ°åŸºåº§ (å¯èƒ½å¯¼è‡´ä¹±ç ): {e}")
        tokenizer = AutoTokenizer.from_pretrained(base_path, trust_remote_code=True)
        tokenizer.add_special_tokens({"additional_special_tokens": ["<obj>", "</obj>", "<box>", "</box>", "<no_layout>"]})

    # 2. åŠ è½½åŸºåº§æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        base_path,
        torch_dtype=torch.float16,
        device_map=device,
        trust_remote_code=True
    )
    
    # 3. è°ƒæ•´ Embedding å¤§å° (å¿…é¡»ä¸ Tokenizer ä¸€è‡´)
    model.resize_token_embeddings(len(tokenizer))

    # 4. åŠ è½½ LoRA
    # æ³¨æ„ï¼šå¦‚æœè®­ç»ƒæ—¶ä¿å­˜äº† embedding layerï¼ŒPeftModel ä¼šè‡ªåŠ¨åŠ è½½å®ƒ
    print("ğŸ“¦ æ­£åœ¨åŠ è½½ LoRA Adapter...")
    model = PeftModel.from_pretrained(model, adapter_path)
    model.eval()
    print("âœ“ LoRA Adapter åŠ è½½æˆåŠŸï¼")
    
    return model, tokenizer

def format_prompt(tokenizer, prompt):
    return format_layout_input(tokenizer, prompt, enable_cot=False, feedback=None)

def _get_prompt(item: Dict) -> str:
    for key in ("caption", "prompt", "text"):
        val = item.get(key, "")
        if isinstance(val, str) and val.strip():
            return val.strip()
    return ""

def _reservoir_sample_prompts(path: str, k: int, seed: int) -> List[str]:
    import json
    import random
    rng = random.Random(seed)
    reservoir = []
    seen = 0
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                item = json.loads(line)
            except Exception:
                continue
            prompt = _get_prompt(item)
            if not prompt:
                continue
            seen += 1
            if len(reservoir) < k:
                reservoir.append(prompt)
            else:
                j = rng.randint(1, seen)
                if j <= k:
                    reservoir[j - 1] = prompt
    return reservoir

def main():
    args = parse_args()
    
    # è®¾ç½®è®¾å¤‡
    if "cuda" in args.device and torch.cuda.is_available():
        device = args.device
    else:
        device = "cpu"

    # åŠ è½½
    model, tokenizer = load_model_and_tokenizer(args.base_model, args.adapter_path, device)

    # æµ‹è¯•ç”¨ä¾‹ï¼ˆå¯ä» JSONL é‡‡æ ·ï¼‰
    if args.input_jsonl and os.path.exists(args.input_jsonl):
        test_prompts = _reservoir_sample_prompts(args.input_jsonl, args.max_samples, args.seed)
        if not test_prompts:
            print("âš ï¸ ä» JSONL æœªé‡‡æ ·åˆ°æœ‰æ•ˆ promptï¼Œå›é€€åˆ°å†…ç½®æ ·ä¾‹")
    else:
        test_prompts = []
    if not test_prompts:
        test_prompts = [
            "ç”»ä¸€åªåœ¨æ¡Œå­å·¦è¾¹çš„çŒ«",
            "å¤©ç©ºä¸­æœ‰é£é¸Ÿï¼Œä¸‹é¢æ˜¯å¹¿é˜”çš„è‰åœ°",
            "å·¦è¾¹æ˜¯ä¸€ä¸ªçº¢è‰²çš„è‹¹æœï¼Œå³è¾¹æ˜¯ä¸€ä¸ªé»„è‰²çš„é¦™è•‰",
            "ä¸€ä¸ªåœ¨ç©é£ç›˜çš„ç‹—ï¼ŒèƒŒæ™¯æ˜¯æµ·æ»©",
            "ä¸Šæ–¹æ˜¯è“å¤©ï¼Œä¸‹æ–¹æ˜¯è‰åœ°",
            "ä¸­é—´æœ‰ä¸€æœµèŠ±ï¼Œå·¦è¾¹æ˜¯æ ‘ï¼Œå³è¾¹æ˜¯æˆ¿å­",
        ]
    if args.max_samples > 0:
        test_prompts = test_prompts[:args.max_samples]
    
    print("\n" + "=" * 60)
    print("ğŸ§ å¼€å§‹éªŒè¯æ¨ç†æ•ˆæœ (Fixed Version)")
    print("=" * 60)
    
    total = 0
    format_ok = 0
    parse_ok = 0  # includes <no_layout>
    parse_obj_ok = 0  # only when objects parsed
    no_layout_count = 0
    results = []

    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}: {prompt}")
        
        # æ„é€ è¾“å…¥
        input_text = format_prompt(tokenizer, prompt)
        inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
        
        try:
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=256,
                    do_sample=False, # éªŒè¯æ—¶å»ºè®®è´ªå©ªè§£ç ï¼Œçœ‹æœ€ç¨³çš„ç»“æœ
                    temperature=0.1,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            # è§£ç 
            generated_ids = outputs[0][len(inputs.input_ids[0]):]
            output_text = tokenizer.decode(generated_ids, skip_special_tokens=False) # ä¸è·³è¿‡ç‰¹æ®Š tokenï¼Œæˆ‘ä»¬è¦çœ‹ <obj>
            
            # æ¸…ç† Qwen ç‰¹æ®Š token æ–¹ä¾¿æ˜¾ç¤º
            clean_text = output_text.replace("<|im_end|>", "").replace("<|endoftext|>", "").strip()
            print(f"ğŸ¤– è¾“å‡º: {clean_text}")
            
            # è§£æ
            total += 1
            is_format_ok = ("<no_layout>" in clean_text) or ("<obj>" in clean_text and "<box>" in clean_text)
            if is_format_ok:
                format_ok += 1
            if "<no_layout>" in clean_text:
                print("   - <no_layout> (æ— å¸ƒå±€è¾“å‡º)")
                no_layout_count += 1
                parse_ok += 1
                results.append({
                    "prompt": prompt,
                    "layout_text": clean_text,
                    "format_ok": is_format_ok,
                    "parsed_ok": True,
                    "parsed_objects_ok": False,
                    "no_layout": True,
                    "parsed": []
                })
                continue
            objects = parse_layout_output(clean_text)
            if objects:
                parse_ok += 1
                parse_obj_ok += 1
                for obj in objects:
                    bbox_str = ",".join([f"{x:.2f}" for x in obj['bbox']])
                    print(f"   - {obj['name']}: [{bbox_str}]")
            else:
                print("   âš ï¸ æœªè§£æåˆ°å¯¹è±¡ (å¯èƒ½æ˜¯æ ¼å¼ä»æœ‰é—®é¢˜)")
            results.append({
                "prompt": prompt,
                "layout_text": clean_text,
                "format_ok": is_format_ok,
                "parsed_ok": bool(objects),
                "parsed_objects_ok": bool(objects),
                "no_layout": False,
                "parsed": objects
            })
                
        except Exception as e:
            print(f"âŒ å‡ºé”™: {e}")

    # Summary
    if total > 0:
        print("\n" + "=" * 60)
        print("ğŸ“Š æ ¼å¼ç»Ÿè®¡")
        print("=" * 60)
        print(f"Total: {total}")
        print(f"Format OK: {format_ok} ({format_ok/total*100:.1f}%)")
        print(f"Parsed OK (incl. <no_layout>): {parse_ok} ({parse_ok/total*100:.1f}%)")
        print(f"Parsed Objects OK: {parse_obj_ok} ({parse_obj_ok/total*100:.1f}%)")
        print(f"No-layout: {no_layout_count} ({no_layout_count/total*100:.1f}%)")

    if args.output_jsonl:
        os.makedirs(os.path.dirname(args.output_jsonl) or ".", exist_ok=True)
        import json
        with open(args.output_jsonl, "w", encoding="utf-8") as f:
            for r in results:
                f.write(json.dumps(r, ensure_ascii=False) + "\n")

if __name__ == "__main__":
    main()
