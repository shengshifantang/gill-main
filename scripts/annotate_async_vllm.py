#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŸºäº vLLM çš„å¼‚æ­¥é«˜å¹¶å‘æ ‡æ³¨è„šæœ¬ï¼ˆQwen2.5-VL-32Bï¼‰

æ¶æ„ï¼š
- æœåŠ¡ç«¯ï¼švLLM API Server (Tensor Parallelism=3)
- å®¢æˆ·ç«¯ï¼šAsyncIO å¼‚æ­¥å¹¶å‘è¯·æ±‚

ä¼˜åŠ¿ï¼š
1. æ¶æ„è§£è€¦ï¼šæœåŠ¡ç«¯å’Œå®¢æˆ·ç«¯åˆ†ç¦»ï¼Œæ›´ç¨³å®š
2. Continuous Batchingï¼švLLM è‡ªåŠ¨ä¼˜åŒ–æ‰¹å¤„ç†
3. é«˜å¹¶å‘ï¼šå•çº¿ç¨‹å¯å¤„ç†æ•°åƒä¸ªå¹¶å‘è¯·æ±‚
4. æ–­ç‚¹ç»­ä¼ ï¼šè‡ªåŠ¨è·³è¿‡å·²å¤„ç†çš„æ•°æ®

æœåŠ¡ç«¯å¯åŠ¨å‘½ä»¤ï¼ˆåœ¨ç‹¬ç«‹ç»ˆç«¯è¿è¡Œï¼‰ï¼š
export CUDA_VISIBLE_DEVICES=0,1,2
python -m vllm.entrypoints.openai.api_server \
    --model /root/models/Qwen2.5-VL-32B-Instruct-AWQ \
    --quantization awq \
    --tensor-parallel-size 3 \
    --trust-remote-code \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.95 \
    --port 8000 \
    --disable-log-requests

å®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹ï¼š
python scripts/annotate_async_vllm.py \
    --input /mnt/disk/lxh/gill_data/wukong_downloaded_500k.jsonl \
    --image-root /mnt/disk/lxh/gill_data/wukong_images \
    --output /mnt/disk/lxh/gill_data/wukong_labeled_vllm.jsonl \
    --api-base http://localhost:8000/v1 \
    --model-name /root/models/Qwen2.5-VL-32B-Instruct-AWQ \
    --max-concurrency 32
"""

import os
import json
import asyncio
import base64
import argparse
import re
from typing import Set, Dict, Any, Optional
from pathlib import Path

try:
    from tqdm.asyncio import tqdm as async_tqdm
    HAS_ASYNC_TQDM = True
except ImportError:
    # å¦‚æœ tqdm ç‰ˆæœ¬ä¸æ”¯æŒ asyncioï¼Œä½¿ç”¨æ™®é€š tqdm
    from tqdm import tqdm
    HAS_ASYNC_TQDM = False

try:
    from openai import AsyncOpenAI
except ImportError:
    print("âŒ éœ€è¦å®‰è£… openai åº“: pip install openai")
    exit(1)


# ================= é…ç½®åŒºåŸŸ =================
DEFAULT_MAX_CONCURRENCY = 32  # å»ºè®®æ ¹æ®æ˜¾å­˜è´Ÿè½½è°ƒæ•´ï¼š2x4090 TP=2 å»ºè®® 32-50ï¼ˆé™ä½å¹¶å‘æ•°å¯å‡å°‘è¶…æ—¶é”™è¯¯ï¼‰
DEFAULT_API_BASE = "http://localhost:8000/v1"
DEFAULT_API_KEY = "EMPTY"  # vLLM ä¸éœ€è¦çœŸå®çš„ API Key
# ===========================================


def encode_image_base64(image_path: str) -> str:
    """å¿«é€Ÿè¯»å–å›¾ç‰‡å¹¶è½¬ä¸ºBase64"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')


def build_prompt(caption: str) -> str:
    """
    æ„å»º Reasoning-Aware Promptï¼ˆFew-shot ä¼˜åŒ–ç‰ˆï¼‰
    
    åŠ å…¥å…·ä½“ç¤ºä¾‹å¯ä»¥æ˜¾è‘—é™ä½æ ¼å¼é”™è¯¯ç‡ï¼Œæå‡æ•°æ®è´¨é‡ã€‚
    """
    return f"""ä½ æ˜¯ä¸€ä¸ªç©ºé—´æ™ºèƒ½ä¸“å®¶ã€‚è¯·åˆ†æå›¾ç‰‡ä¸­ä¸æè¿°"{caption}"ç›¸å…³çš„å®ä½“ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ€è€ƒï¼š
1. **Rationale**: è¿ç”¨ç©ºé—´æ¨ç†ï¼Œè§£é‡Šä¸ºä»€ä¹ˆç‰©ä½“ä½äºè¯¥ä½ç½®ï¼ˆè€ƒè™‘é®æŒ¡ã€æ”¯æ’‘ã€é€è§†å…³ç³»ï¼‰ã€‚
2. **Detection**: è¾“å‡ºä¸¥æ ¼çš„JSONæ ¼å¼ã€‚

ç¤ºä¾‹è¾“å…¥ï¼šæè¿°"ä¸€åªçŒ«ååœ¨æ²™å‘ä¸Š"
ç¤ºä¾‹è¾“å‡ºï¼š
{{
    "rationale": "çŒ«æ˜¯ç”»é¢ä¸»ä½“ï¼Œä½äºå›¾åƒä¸­å¿ƒåä¸‹ï¼›æ²™å‘ä½œä¸ºæ”¯æ’‘ç‰©ä½“ä½äºåº•éƒ¨ã€‚",
    "objects": [
        {{"name": "çŒ«", "bbox": [200, 300, 600, 700]}},
        {{"name": "æ²™å‘", "bbox": [100, 800, 900, 1000]}}
    ]
}}

å½“å‰ä»»åŠ¡æè¿°ï¼š"{caption}"

æ³¨æ„ï¼š
- åæ ‡è¯·ä½¿ç”¨ 0-1000 çš„å½’ä¸€åŒ–æ•´æ•°ï¼ˆç›¸å¯¹äºå›¾ç‰‡å°ºå¯¸ï¼‰
- bbox æ ¼å¼ï¼š[x1, y1, x2, y2]ï¼Œå…¶ä¸­ (x1,y1) æ˜¯å·¦ä¸Šè§’ï¼Œ(x2,y2) æ˜¯å³ä¸‹è§’
- ç¡®ä¿ x1 < x2 ä¸” y1 < y2
- å¦‚æœå›¾ç‰‡ä¸­æ²¡æœ‰æè¿°ä¸­çš„ç‰©ä½“ï¼Œobjects åˆ—è¡¨ä¸ºç©º
- åªè¾“å‡º JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—æˆ– Markdown æ ‡è®°"""


def sanitize_bbox(bbox: list, width: int = 1000, height: int = 1000) -> Optional[list]:
    """
    [å…³é”®] åæ ‡æ¸…æ´—ä¸éªŒè¯
    
    1. ç¡®ä¿åæ ‡æ˜¯æ•°å­—
    2. è‡ªåŠ¨æ£€æµ‹ 0-1 èŒƒå›´çš„å½’ä¸€åŒ–åæ ‡å¹¶è½¬æ¢ä¸º 0-1000
    3. ç¡®ä¿ x1 < x2, y1 < y2
    4. è£å‰ªåˆ° [0, 1000] èŒƒå›´
    5. è¿‡æ»¤æ— æ•ˆæ¡†ï¼ˆé¢ç§¯è¿‡å°æˆ–åå‘æ¡†ï¼‰
    
    è¿™å¯¹äºè®­ç»ƒç¨³å®šæ€§è‡³å…³é‡è¦ï¼Œé˜²æ­¢ NaN/Inf å¯¼è‡´ Loss å¼‚å¸¸ã€‚
    """
    try:
        # å¼ºåˆ¶è½¬ floatï¼ˆå¤„ç†å­—ç¬¦ä¸²æ•°å­—ï¼‰
        b = [float(x) for x in bbox]
        if len(b) != 4:
            return None
        
        # [ä¼˜åŒ–] è‡ªåŠ¨æ£€æµ‹ 0-1 èŒƒå›´çš„å½’ä¸€åŒ–åæ ‡
        # å¦‚æœæ‰€æœ‰åæ ‡éƒ½åœ¨ 0.0-1.0 ä¹‹é—´ï¼Œåˆ™è‡ªåŠ¨ä¹˜ä»¥ 1000
        if all(0.0 <= x <= 1.0 for x in b):
            b = [x * 1000 for x in b]
        
        # è½¬æ¢ä¸ºæ•´æ•°
        b = [int(x) for x in b]
        x1, y1, x2, y2 = b[0], b[1], b[2], b[3]
        
        # ç¡®ä¿é¡ºåºæ­£ç¡®ï¼ˆå·¦ä¸Šè§’ < å³ä¸‹è§’ï¼‰
        if x1 > x2:
            x1, x2 = x2, x1
        if y1 > y2:
            y1, y2 = y2, y1
        
        # è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´
        x1 = max(0, min(width, x1))
        y1 = max(0, min(height, y1))
        x2 = max(0, min(width, x2))
        y2 = max(0, min(height, y2))
        
        # è¿‡æ»¤æ— æ•ˆæ¡†ï¼ˆé¢ç§¯è¿‡å°æˆ–åå‘æ¡†ï¼‰
        if x2 <= x1 + 10 or y2 <= y1 + 10:
            return None
        
        return [x1, y1, x2, y2]
    except (ValueError, TypeError, IndexError):
        return None


def robust_parse_json(content: str) -> Optional[Dict[str, Any]]:
    """
    [å…³é”®] é²æ£’çš„ JSON æå–å™¨ï¼Œåº”å¯¹ Markdown å’Œæ–‡æœ¬å™ªå£°
    
    æ”¹è¿›ç‚¹ï¼š
    1. ç§»é™¤ Markdown ä»£ç å—æ ‡è®°
    2. å¤„ç†æ³¨é‡Šå’Œå¤šä½™æ–‡æœ¬
    3. åæ ‡æ¸…æ´—å’ŒéªŒè¯
    4. ç»“æ„éªŒè¯
    
    è¿™èƒ½æ˜¾è‘—æå‡æ•°æ®åˆ©ç”¨ç‡ï¼Œå‡å°‘å› æ ¼å¼é—®é¢˜å¯¼è‡´çš„æœ‰æ•ˆæ•°æ®ä¸¢å¤±ã€‚
    """
    if not content:
        return None
    
    # 1. ç§»é™¤ Markdown ä»£ç å—æ ‡è®°
    content = re.sub(r'```json\s*', '', content, flags=re.IGNORECASE)
    content = re.sub(r'```', '', content)
    
    # 2. ç§»é™¤å¯èƒ½çš„æ³¨é‡Šï¼ˆ// æˆ– # å¼€å¤´çš„è¡Œï¼‰
    lines = content.split('\n')
    cleaned_lines = []
    for line in lines:
        stripped = line.strip()
        if not stripped.startswith('//') and not stripped.startswith('#'):
            cleaned_lines.append(line)
    content = '\n'.join(cleaned_lines)
    
    # 3. å¯»æ‰¾æœ€å¤–å±‚å¤§æ‹¬å·
    start = content.find('{')
    end = content.rfind('}')
    
    if start == -1 or end == -1 or start >= end:
        return None
    
    json_str = content[start:end+1]
    
    try:
        # 4. æ ‡å‡† JSON è§£æ
        data = json.loads(json_str)
        
        # 5. ç»“æ„éªŒè¯ä¸æ¸…æ´—
        if not isinstance(data, dict):
            return None
        
        if "objects" in data and isinstance(data["objects"], list):
            valid_objs = []
            for obj in data["objects"]:
                if isinstance(obj, dict) and "name" in obj and "bbox" in obj:
                    if isinstance(obj["bbox"], list):
                        clean_bbox = sanitize_bbox(obj["bbox"])
                        if clean_bbox:
                            obj["bbox"] = clean_bbox
                            valid_objs.append(obj)
            data["objects"] = valid_objs
        
        # éªŒè¯è‡³å°‘åŒ…å« objects å­—æ®µ
        if "objects" not in data:
            return None
        
        return data
        
    except json.JSONDecodeError as e:
        # JSON è§£æå¤±è´¥ï¼Œå¯èƒ½æ˜¯æ ¼å¼é—®é¢˜
        return None
    except Exception:
        return None


def parse_bboxes_from_content(content: str) -> Optional[Dict[str, Any]]:
    """ä»æ¨¡å‹è¾“å‡ºä¸­è§£æ JSONï¼ˆä½¿ç”¨é²æ£’è§£æå™¨ï¼‰"""
    return robust_parse_json(content)


class AnnotationWorker:
    def __init__(self, args):
        self.args = args
        self.client = AsyncOpenAI(
            api_key=args.api_key or DEFAULT_API_KEY,
            base_url=args.api_base or DEFAULT_API_BASE
        )
        # å…ˆåˆå§‹åŒ– statsï¼Œå› ä¸º _load_progress ä¼šä½¿ç”¨å®ƒ
        self.stats = {
            'total': 0,
            'processed': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'parse_error': 0,  # JSON è§£æå¤±è´¥
            'invalid_bbox': 0  # åæ ‡æ— æ•ˆ
        }
        self.processed_paths = self._load_progress()
        self.max_concurrency = args.max_concurrency
        # æ³¨æ„ï¼šsemaphore å’Œ lock åœ¨ run() æ–¹æ³•ä¸­åˆ›å»ºï¼Œç¡®ä¿åœ¨æ­£ç¡®çš„äº‹ä»¶å¾ªç¯ä¸­
        self.semaphore = None
        self.write_lock = None
        self.error_log_path = args.output.replace(".jsonl", "_errors.jsonl")  # é”™è¯¯æ—¥å¿—

    def _load_progress(self) -> Set[str]:
        """åŠ è½½å·²å¤„ç†çš„å›¾ç‰‡è·¯å¾„ï¼ˆæ ‡å‡†åŒ–è·¯å¾„ä»¥ç¡®ä¿ä¸€è‡´æ€§ï¼‰"""
        processed = set()
        if os.path.exists(self.args.output):
            print(f"ğŸ“– è¯»å–æ–­ç‚¹ç»­ä¼ æ–‡ä»¶: {self.args.output}")
            # ä¿®å¤æœ«å°¾å¯èƒ½ç¼ºå¤±çš„æ¢è¡Œç¬¦
            self._fix_newline(self.args.output)
            with open(self.args.output, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        image_path = data.get('image_path')
                        if image_path:
                            # æ ‡å‡†åŒ–è·¯å¾„ï¼ˆä¸ process_single_item ä¿æŒä¸€è‡´ï¼‰
                            if not os.path.isabs(image_path):
                                full_path = os.path.join(self.args.image_root, image_path)
                            else:
                                full_path = image_path
                            normalized_path = os.path.normpath(full_path)
                            processed.add(normalized_path)
                    except:
                        pass
        print(f"âœ… å·²å®Œæˆ: {len(processed)} æ¡")
        self.stats['skipped'] = len(processed)
        return processed

    def _fix_newline(self, filepath: str):
        """ç¡®ä¿æ–‡ä»¶ä»¥æ¢è¡Œç¬¦ç»“å°¾"""
        try:
            with open(filepath, 'rb+') as f:
                f.seek(0, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
                if f.tell() > 0:  # å¦‚æœæ–‡ä»¶ä¸ä¸ºç©º
                    f.seek(-1, 2)  # ç§»åŠ¨åˆ°å€’æ•°ç¬¬ä¸€ä¸ªå­—èŠ‚
                    if f.read(1) != b'\n':
                        f.write(b'\n')
        except Exception:
            pass

    async def process_single_item(self, item: Dict[str, Any], pbar) -> None:
        """å¤„ç†å•å¼ å›¾ç‰‡"""
        image_path = item.get('image_path', '')
        if not image_path:
            pbar.update(1)
            return

        # å¤„ç†ç›¸å¯¹è·¯å¾„
        if not os.path.isabs(image_path):
            image_path = os.path.join(self.args.image_root, image_path)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            self.stats['failed'] += 1
            pbar.update(1)
            return

        # é™åˆ¶å¹¶å‘
        async with self.semaphore:
            try:
                # å‡†å¤‡è¯·æ±‚
                b64_img = await asyncio.to_thread(encode_image_base64, image_path)
                prompt = build_prompt(item.get('caption', ''))

                # [ä¼˜åŒ–] æ·»åŠ è¶…æ—¶æ§åˆ¶ï¼ˆ120ç§’ï¼‰ï¼Œé˜²æ­¢è¯·æ±‚å¡æ­»
                try:
                    response = await asyncio.wait_for(
                        self.client.chat.completions.create(
                    model=self.args.model_name,
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}
                                },
                                {
                                    "type": "text",
                                    "text": prompt
                                },
                            ],
                        }
                    ],
                    max_tokens=512,
                    temperature=0.1,  # ä½æ¸©åº¦ä¿è¯æ ¼å¼ç¨³å®š
                    top_p=0.9,
                        ),
                        timeout=120.0  # 120ç§’è¶…æ—¶ï¼ˆä»60ç§’å¢åŠ åˆ°120ç§’ï¼Œå‡å°‘è¶…æ—¶é”™è¯¯ï¼‰
                )
                except asyncio.TimeoutError:
                    raise Exception("Request timeout after 120 seconds")

                content = response.choices[0].message.content

                # è§£æç»“æœ
                parsed_annotations = parse_bboxes_from_content(content)

                # æ„å»ºç»“æœé¡¹
                result_item = item.copy()
                result_item['vlm_output'] = content  # ä¿å­˜åŸå§‹è¾“å‡º
                
                if parsed_annotations:
                    objects = parsed_annotations.get('objects', [])
                    if objects:
                        result_item['annotations'] = parsed_annotations
                        result_item['objects'] = objects
                        self.stats['success'] += 1
                    else:
                        # è§£ææˆåŠŸä½†æ²¡æœ‰æœ‰æ•ˆå¯¹è±¡
                        result_item['annotations'] = parsed_annotations
                        result_item['objects'] = []
                        result_item['no_objects'] = True
                        self.stats['success'] += 1  # ä»ç„¶ç®—æˆåŠŸï¼ˆå¯èƒ½æ˜¯å›¾ç‰‡ä¸­ç¡®å®æ²¡æœ‰ç‰©ä½“ï¼‰
                else:
                    # JSON è§£æå¤±è´¥
                    result_item['annotations_error'] = True
                    result_item['error_type'] = 'parse_error'
                    self.stats['parse_error'] += 1
                    self.stats['failed'] += 1

                # å¼‚æ­¥å†™å…¥ç»“æœï¼ˆåŠ é”ä¿è¯çº¿ç¨‹å®‰å…¨ï¼‰
                async with self.write_lock:
                    with open(self.args.output, 'a', encoding='utf-8') as f:
                        f.write(json.dumps(result_item, ensure_ascii=False) + "\n")

            except Exception as e:
                # é”™è¯¯å¤„ç†ï¼šè®°å½•å¤±è´¥ä½†ä¸ä¸­æ–­æµç¨‹
                self.stats['failed'] += 1
                
                # [å…³é”®] è®°å½•é”™è¯¯æ—¥å¿—ï¼ˆç”¨äºè®ºæ–‡çš„ Failure Analysisï¼‰
                error_entry = {
                    "image_path": image_path,
                    "caption": item.get('caption', ''),
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "raw_output": content if 'content' in locals() else ""
                }
                
                async with self.write_lock:
                    try:
                        with open(self.error_log_path, 'a', encoding='utf-8') as f_err:
                            f_err.write(json.dumps(error_entry, ensure_ascii=False) + "\n")
                    except Exception:
                        pass  # é”™è¯¯æ—¥å¿—å†™å…¥å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            finally:
                self.stats['processed'] += 1
                pbar.update(1)

    async def run(self):
        """ä¸»è¿è¡Œå‡½æ•°"""
        # åœ¨äº‹ä»¶å¾ªç¯ä¸­åˆ›å»º Semaphore å’Œ Lockï¼ˆä¿®å¤äº‹ä»¶å¾ªç¯é—®é¢˜ï¼‰
        self.semaphore = asyncio.Semaphore(self.max_concurrency)
        self.write_lock = asyncio.Lock()
        
        # 1. è¯»å–è¾“å…¥æ•°æ®ï¼ˆè·³è¿‡å·²å¤„ç†çš„ï¼Œä½¿ç”¨æ ‡å‡†åŒ–è·¯å¾„ï¼‰
        tasks_data = []
        print(f"ğŸ“– è¯»å–è¾“å…¥æ–‡ä»¶: {self.args.input}")
        with open(self.args.input, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    item = json.loads(line.strip())
                    image_path = item.get('image_path', '')
                    if image_path:
                        # æ ‡å‡†åŒ–è·¯å¾„ï¼ˆä¸ _load_progress ä¿æŒä¸€è‡´ï¼‰
                        if not os.path.isabs(image_path):
                            full_path = os.path.join(self.args.image_root, image_path)
                        else:
                            full_path = image_path
                        normalized_path = os.path.normpath(full_path)
                        if normalized_path not in self.processed_paths:
                            tasks_data.append(item)
                except json.JSONDecodeError:
                    continue

        self.stats['total'] = len(tasks_data)
        print(f"ğŸš€ å¾…å¤„ç†ä»»åŠ¡æ•°: {len(tasks_data)}")

        if len(tasks_data) == 0:
            print("âœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œæ— éœ€å¤„ç†")
            return

        # 2. åˆ›å»ºè¿›åº¦æ¡
        if HAS_ASYNC_TQDM:
            progress_bar = async_tqdm(
                total=len(tasks_data),
                desc="æ ‡æ³¨è¿›åº¦",
                unit="img",
                ncols=100
            )
        else:
            from tqdm import tqdm
            progress_bar = tqdm(
                total=len(tasks_data),
                desc="æ ‡æ³¨è¿›åº¦",
                unit="img",
                ncols=100
            )

        # 3. åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [
            self.process_single_item(item, progress_bar)
            for item in tasks_data
        ]

        # 4. å¹¶å‘æ‰§è¡Œ
        start_time = asyncio.get_event_loop().time()
        await asyncio.gather(*tasks)
        end_time = asyncio.get_event_loop().time()

        progress_bar.close()

        # 5. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        elapsed = end_time - start_time
        print(f"\n{'='*60}")
        print(f"ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"æ€»ä»»åŠ¡æ•°: {self.stats['total']}")
        print(f"æˆåŠŸ: {self.stats['success']}")
        print(f"å¤±è´¥: {self.stats['failed']}")
        print(f"  - JSON è§£æå¤±è´¥: {self.stats['parse_error']}")
        print(f"  - å…¶ä»–é”™è¯¯: {self.stats['failed'] - self.stats['parse_error']}")
        print(f"è·³è¿‡: {self.stats['skipped']}")
        print(f"è€—æ—¶: {elapsed:.2f} ç§’")
        if self.stats['processed'] > 0:
            print(f"å¹³å‡é€Ÿåº¦: {self.stats['processed']/elapsed:.2f} å›¾ç‰‡/ç§’")
        if self.stats['total'] > 0:
            success_rate = (self.stats['success'] / self.stats['total']) * 100
            print(f"æˆåŠŸç‡: {success_rate:.2f}%")
        print(f"{'='*60}")
        print(f"ğŸ“ é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {self.error_log_path}")
        print(f"   å¯ç”¨äº Failure Analysis å’Œè®ºæ–‡çš„ Limitations ç« èŠ‚")
        print(f"{'='*60}")


def main():
    parser = argparse.ArgumentParser(
        description="åŸºäº vLLM çš„å¼‚æ­¥é«˜å¹¶å‘æ ‡æ³¨è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹ï¼š
1. å¯åŠ¨ vLLM æœåŠ¡ç«¯ï¼ˆåœ¨ç‹¬ç«‹ç»ˆç«¯ï¼‰ï¼š
   export CUDA_VISIBLE_DEVICES=0,1,2
   python -m vllm.entrypoints.openai.api_server \\
       --model /root/models/Qwen2.5-VL-32B-Instruct-AWQ \\
       --quantization awq \\
       --tensor-parallel-size 3 \\
       --trust-remote-code \\
       --max-model-len 8192 \\
       --gpu-memory-utilization 0.95 \\
       --port 8000

2. è¿è¡Œå®¢æˆ·ç«¯è„šæœ¬ï¼š
   python scripts/annotate_async_vllm.py \\
       --input wukong_downloaded.jsonl \\
       --image-root ./images \\
       --output wukong_labeled.jsonl \\
       --api-base http://localhost:8000/v1 \\
       --model-name /root/models/Qwen2.5-VL-32B-Instruct-AWQ \\
       --max-concurrency 32
        """
    )
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="è¾“å…¥ JSONL æ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å« image_path å’Œ captionï¼‰"
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="è¾“å‡º JSONL æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--image-root",
        type=str,
        required=True,
        help="å›¾ç‰‡æ ¹ç›®å½•ï¼ˆç”¨äºè§£æç›¸å¯¹è·¯å¾„ï¼‰"
    )
    parser.add_argument(
        "--api-base",
        type=str,
        default=DEFAULT_API_BASE,
        help=f"vLLM API æœåŠ¡åœ°å€ï¼ˆé»˜è®¤: {DEFAULT_API_BASE}ï¼‰"
    )
    parser.add_argument(
        "--api-key",
        type=str,
        default=DEFAULT_API_KEY,
        help=f"API Keyï¼ˆvLLM ä¸éœ€è¦çœŸå® keyï¼Œé»˜è®¤: {DEFAULT_API_KEY}ï¼‰"
    )
    parser.add_argument(
        "--model-name",
        type=str,
        required=True,
        help="æ¨¡å‹åç§°ï¼ˆå¿…é¡»ä¸ vLLM å¯åŠ¨å‚æ•°ä¸­çš„ --model ä¸€è‡´ï¼‰"
    )
    parser.add_argument(
        "--max-concurrency",
        type=int,
        default=DEFAULT_MAX_CONCURRENCY,
        help=f"æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤: {DEFAULT_MAX_CONCURRENCY}ï¼Œå»ºè®®æ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰"
    )

    args = parser.parse_args()

    # éªŒè¯è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)

    # è¿è¡Œæ ‡æ³¨ä»»åŠ¡
    worker = AnnotationWorker(args)
    asyncio.run(worker.run())


if __name__ == "__main__":
    main()

